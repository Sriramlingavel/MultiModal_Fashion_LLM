{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOAUDAPBCfWhyE0Pf9O2sd7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sriramlingavel/MultiModal_Fashion_LLM/blob/main/MultiModal_FashionLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t7ybd_89Tcu",
        "outputId": "f18a2186-d643-416f-95a9-c832ad93998d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m131.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken tensorflow bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNFuLerR-CSZ",
        "outputId": "4eb86fd3-c656-43f4-b986-cbb2e1515dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UbqiKf8H-Eol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Replace 'your_file.zip' with the name of the uploaded zip file\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/gpt2_model.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/Files')"
      ],
      "metadata": {
        "id": "Ez0imj8d-k3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------------\n",
        "# Custom Model Architecture\n",
        "# ------------------------------\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=True):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "# ------------------------------\n",
        "# Data Loading and Preprocessing\n",
        "# ------------------------------\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "def create_dataloader(txt, batch_size=4, max_length=1024,\n",
        "                      stride=512, shuffle=True, drop_last=True,\n",
        "                      num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "# ------------------------------\n",
        "# GPT-2 Model Setup\n",
        "# ------------------------------\n",
        "\n",
        "# Configuration for GPT-2 large (774M)\n",
        "GPT_CONFIG_774M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 1280,\n",
        "    \"n_heads\": 20,\n",
        "    \"n_layers\": 36,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": True\n",
        "}\n",
        "\n",
        "# ------------------------------\n",
        "# GPT-2 Model Weight Loading (from local path)\n",
        "# ------------------------------\n",
        "\n",
        "def load_gpt2_params_from_local_path(model_path, model_size=\"774M\"):\n",
        "    \"\"\"Load parameters from local GPT-2 model path.\"\"\"\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "    except ImportError:\n",
        "        raise ImportError(\"TensorFlow is required to load GPT-2 weights.\")\n",
        "\n",
        "    # Define the checkpoint path\n",
        "    model_dir = os.path.join(model_path, model_size)\n",
        "    ckpt_path = os.path.join(model_dir, \"model.ckpt\")\n",
        "    hparams_path = os.path.join(model_dir, \"hparams.json\")\n",
        "\n",
        "    # Check if files exist\n",
        "    if not os.path.exists(hparams_path):\n",
        "        raise FileNotFoundError(f\"hparams.json not found at {hparams_path}\")\n",
        "\n",
        "    # Load settings\n",
        "    with open(hparams_path, 'r') as f:\n",
        "        settings = json.load(f)\n",
        "\n",
        "    # Initialize params dict\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "\n",
        "    # Load each variable from the checkpoint\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
        "\n",
        "        # Map TensorFlow variables to our PyTorch model structure\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "\n",
        "        # Build nested dictionary structure\n",
        "        for key in variable_name_parts[1:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "\n",
        "        # Store variable in dictionary\n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "\n",
        "    # Add wte (word token embeddings) and wpe (position embeddings) at top level for convenience\n",
        "    for name in [\"wte\", \"wpe\"]:\n",
        "        if name in params:\n",
        "            continue  # Skip if already at top level\n",
        "        for block in params[\"blocks\"]:\n",
        "            if name in block:\n",
        "                params[name] = block[name]\n",
        "                break\n",
        "\n",
        "    return settings, params\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    \"\"\"Load GPT-2 weights from params dict into PyTorch model.\"\"\"\n",
        "    # Load embeddings\n",
        "    gpt.pos_emb.weight = nn.Parameter(torch.tensor(params['wpe']))\n",
        "    gpt.tok_emb.weight = nn.Parameter(torch.tensor(params['wte']))\n",
        "\n",
        "    # Load transformer blocks\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        # Split QKV weights and biases\n",
        "        block = params[\"blocks\"][b]\n",
        "\n",
        "        # Handle attention layers\n",
        "        if \"attn\" in block:\n",
        "            # QKV weights\n",
        "            q_w, k_w, v_w = np.split(block[\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1)\n",
        "            gpt.trf_blocks[b].att.W_query.weight = nn.Parameter(torch.tensor(q_w.T))\n",
        "            gpt.trf_blocks[b].att.W_key.weight = nn.Parameter(torch.tensor(k_w.T))\n",
        "            gpt.trf_blocks[b].att.W_value.weight = nn.Parameter(torch.tensor(v_w.T))\n",
        "\n",
        "            # QKV biases\n",
        "            q_b, k_b, v_b = np.split(block[\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1)\n",
        "            gpt.trf_blocks[b].att.W_query.bias = nn.Parameter(torch.tensor(q_b))\n",
        "            gpt.trf_blocks[b].att.W_key.bias = nn.Parameter(torch.tensor(k_b))\n",
        "            gpt.trf_blocks[b].att.W_value.bias = nn.Parameter(torch.tensor(v_b))\n",
        "\n",
        "            # Output projection\n",
        "            gpt.trf_blocks[b].att.out_proj.weight = nn.Parameter(torch.tensor(block[\"attn\"][\"c_proj\"][\"w\"].T))\n",
        "            gpt.trf_blocks[b].att.out_proj.bias = nn.Parameter(torch.tensor(block[\"attn\"][\"c_proj\"][\"b\"]))\n",
        "\n",
        "        # Handle feed-forward layers\n",
        "        if \"mlp\" in block:\n",
        "            # First linear layer\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight = nn.Parameter(torch.tensor(block[\"mlp\"][\"c_fc\"][\"w\"].T))\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias = nn.Parameter(torch.tensor(block[\"mlp\"][\"c_fc\"][\"b\"]))\n",
        "\n",
        "            # Second linear layer (after activation)\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight = nn.Parameter(torch.tensor(block[\"mlp\"][\"c_proj\"][\"w\"].T))\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias = nn.Parameter(torch.tensor(block[\"mlp\"][\"c_proj\"][\"b\"]))\n",
        "\n",
        "        # Handle layer norms\n",
        "        if \"ln_1\" in block:\n",
        "            gpt.trf_blocks[b].norm1.scale = nn.Parameter(torch.tensor(block[\"ln_1\"][\"g\"]))\n",
        "            gpt.trf_blocks[b].norm1.shift = nn.Parameter(torch.tensor(block[\"ln_1\"][\"b\"]))\n",
        "\n",
        "        if \"ln_2\" in block:\n",
        "            gpt.trf_blocks[b].norm2.scale = nn.Parameter(torch.tensor(block[\"ln_2\"][\"g\"]))\n",
        "            gpt.trf_blocks[b].norm2.shift = nn.Parameter(torch.tensor(block[\"ln_2\"][\"b\"]))\n",
        "\n",
        "    # Load final layer norm\n",
        "    if \"ln_f\" in params:\n",
        "        gpt.final_norm.scale = nn.Parameter(torch.tensor(params[\"ln_f\"][\"g\"]))\n",
        "        gpt.final_norm.shift = nn.Parameter(torch.tensor(params[\"ln_f\"][\"b\"]))\n",
        "    elif \"g\" in params and \"b\" in params:\n",
        "        gpt.final_norm.scale = nn.Parameter(torch.tensor(params[\"g\"]))\n",
        "        gpt.final_norm.shift = nn.Parameter(torch.tensor(params[\"b\"]))\n",
        "\n",
        "    # Output head (reuse token embedding weights)\n",
        "    gpt.out_head.weight = nn.Parameter(torch.tensor(params[\"wte\"]))\n",
        "\n",
        "# ------------------------------\n",
        "# Text Generation Functions\n",
        "# ------------------------------\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.7, top_k=40, eos_id=None):\n",
        "    \"\"\"Generate text with temperature and top-k sampling.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Get context for current generation step\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus on the last token\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply top-k filtering\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "        # Apply temperature\n",
        "        if temperature > 0:\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            # Greedy sampling\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        # Stop if EOS token is generated\n",
        "        if eos_id is not None and (idx_next == eos_id).any():\n",
        "            stop_indices = (idx_next == eos_id).nonzero(as_tuple=True)[0]\n",
        "            if len(stop_indices) > 0:\n",
        "                break\n",
        "\n",
        "        # Append new token to sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def text_to_token_ids(text, tokenizer=None):\n",
        "    \"\"\"Convert text to token IDs.\"\"\"\n",
        "    if tokenizer is None:\n",
        "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    return torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer=None):\n",
        "    \"\"\"Convert token IDs to text.\"\"\"\n",
        "    if tokenizer is None:\n",
        "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    flat = token_ids.squeeze(0)\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "# ------------------------------\n",
        "# Training Functions\n",
        "# ------------------------------\n",
        "\n",
        "def train_step(model, optimizer, batch, device, criterion=nn.CrossEntropyLoss()):\n",
        "    \"\"\"Perform one training step.\"\"\"\n",
        "    inputs, targets = batch\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    logits = model(inputs)\n",
        "\n",
        "    # Reshape for loss calculation\n",
        "    b, t, c = logits.shape\n",
        "    logits = logits.view(b * t, c)\n",
        "    targets = targets.view(b * t)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(logits, targets)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(model, val_loader, device, criterion=nn.CrossEntropyLoss()):\n",
        "    \"\"\"Evaluate model on validation data.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            logits = model(inputs)\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            b, t, c = logits.shape\n",
        "            logits = logits.view(b * t, c)\n",
        "            targets = targets.view(b * t)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(logits, targets)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    model.train()\n",
        "    return avg_loss\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, device,\n",
        "                num_epochs=3, save_path=\"gpt2_774m_model.pt\"):\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch in progress_bar:\n",
        "            loss = train_step(model, optimizer, batch, device, criterion)\n",
        "            train_losses.append(loss)\n",
        "            progress_bar.set_postfix({\"train_loss\": f\"{loss:.4f}\"})\n",
        "\n",
        "        # Validation phase\n",
        "        val_loss = evaluate(model, val_loader, device, criterion)\n",
        "\n",
        "        # Print epoch results\n",
        "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"epoch\": epoch,\n",
        "                \"val_loss\": val_loss\n",
        "            }, save_path)\n",
        "            print(f\"Saved best model to {save_path}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ------------------------------\n",
        "# Main Functions\n",
        "# ------------------------------\n",
        "\n",
        "def load_pretrained_model_from_path(model_path, model_size=\"774M\", device=None):\n",
        "    \"\"\"Load the GPT-2 model from a local path.\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load GPT-2 weights from local path\n",
        "    print(f\"Loading GPT-2 {model_size} model weights from {model_path}...\")\n",
        "    settings, params = load_gpt2_params_from_local_path(model_path, model_size)\n",
        "\n",
        "    # Initialize our model\n",
        "    print(\"Initializing model...\")\n",
        "    model = GPTModel(GPT_CONFIG_774M)\n",
        "\n",
        "    # Load weights into our model\n",
        "    print(\"Loading weights into model...\")\n",
        "    load_weights_into_gpt(model, params)\n",
        "\n",
        "    # Move model to device\n",
        "    model.to(device)\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def fine_tune_on_text(model, text_file_path, batch_size=2, num_epochs=3, learning_rate=1e-5):\n",
        "    \"\"\"Fine-tune the model on a text file.\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Load text data\n",
        "    print(f\"Loading text data from {text_file_path}...\")\n",
        "    with open(text_file_path, 'r', encoding='utf-8') as f:\n",
        "        text_data = f.read()\n",
        "\n",
        "    # Split into train/val\n",
        "    train_ratio = 0.9\n",
        "    split_idx = int(train_ratio * len(text_data))\n",
        "    train_data = text_data[:split_idx]\n",
        "    val_data = text_data[split_idx:]\n",
        "\n",
        "    # Create dataloaders\n",
        "    print(\"Creating dataloaders...\")\n",
        "    train_loader = create_dataloader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        max_length=GPT_CONFIG_774M[\"context_length\"],\n",
        "        stride=512,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = create_dataloader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        max_length=GPT_CONFIG_774M[\"context_length\"],\n",
        "        stride=512,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.1)\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"Starting training for {num_epochs} epochs...\")\n",
        "    model = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        optimizer,\n",
        "        device,\n",
        "        num_epochs=num_epochs\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def generate_text(model, prompt, max_length=50, temperature=0.7, top_k=40):\n",
        "    \"\"\"Generate text from a prompt.\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Convert prompt to token ids\n",
        "    input_ids = text_to_token_ids(prompt, tokenizer).to(device)\n",
        "\n",
        "    # Generate text\n",
        "    output_ids = generate(\n",
        "        model,\n",
        "        input_ids,\n",
        "        max_new_tokens=max_length,\n",
        "        context_size=GPT_CONFIG_774M[\"context_length\"],\n",
        "        temperature=temperature,\n",
        "        top_k=top_k\n",
        "    )\n",
        "\n",
        "    # Convert back to text\n",
        "    generated_text = token_ids_to_text(output_ids, tokenizer)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "def save_model(model, path=\"gpt2_774m_model.pt\"):\n",
        "    \"\"\"Save the model to disk.\"\"\"\n",
        "    torch.save({\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"config\": GPT_CONFIG_774M\n",
        "    }, path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "def load_model(path=\"gpt2_774m_model.pt\", device=None):\n",
        "    \"\"\"Load a saved model from disk.\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    model = GPTModel(GPT_CONFIG_774M)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    model.to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "# ------------------------------\n",
        "# Example Usage in Google Colab with Drive\n",
        "# ------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage in Google Colab\n",
        "    import sys\n",
        "    is_colab = 'google.colab' in sys.modules\n",
        "\n",
        "    if is_colab:\n",
        "        #from google.colab import drive\n",
        "        #print(\"Running in Google Colab environment\")\n",
        "\n",
        "        # Mount Google Drive\n",
        "        #print(\"Mounting Google Drive...\")\n",
        "        #drive.mount('/content/drive')\n",
        "\n",
        "        # Specify the path to your uploaded model\n",
        "        model_path = '/content/Files/gpt2_model'  # Adjust path as needed\n",
        "\n",
        "        # Check if model exists in the specified path\n",
        "        if not os.path.exists(os.path.join(model_path, '774M')):\n",
        "            print(f\"GPT-2 model not found at {model_path}/774M\")\n",
        "            print(\"Please make sure you've uploaded the model to this location\")\n",
        "            exit()\n",
        "    else:\n",
        "        # Running locally\n",
        "        print(\"Running in local environment\")\n",
        "        model_path = './gpt2_model'  # Default local path\n",
        "\n",
        "    # Load model from the specified path\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load model\n",
        "    model = load_pretrained_model_from_path(model_path, device=device)\n",
        "\n",
        "    # Generate sample text\n",
        "    sample_text = generate_text(model, \"The dress which suits you is\")\n",
        "    print(\"\\nGenerated Text:\")\n",
        "    print(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7oJzofW_VEh",
        "outputId": "64bf02b7-d98d-416d-9090-4eeb1c6bd8eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Using device: cpu\n",
            "Loading GPT-2 774M model weights from /content/Files/gpt2_model...\n",
            "Initializing model...\n",
            "Loading weights into model...\n",
            "Model loaded successfully!\n",
            "\n",
            "Generated Text:\n",
            "The dress which suits you is not the dress which suits me. You may want to wear the dress to a formal event, but it is not necessary for formal occasions.\n",
            "\n",
            "\n",
            "Some people don't care for formal attire and will wear more casual attire - even to a formal event\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# Import bitsandbytes for 8-bit quantization\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# Load the fashion dataset\n",
        "def load_fashion_dataset(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"Loaded {len(data)} entries from {file_path}\")\n",
        "    return data\n",
        "\n",
        "# Format input as per your requirements\n",
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry.get('input', '') else \"\"\n",
        "\n",
        "    return instruction_text + input_text\n",
        "\n",
        "# Custom Dataset class for instruction tuning\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text, allowed_special={\"<|endoftext|>\"})\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# Collate function for padding sequences in a batch\n",
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,  # <|endoftext|> token id for GPT-2\n",
        "    ignore_index=-100,\n",
        "    max_length=None,\n",
        "    device=\"cuda\"\n",
        "):\n",
        "    # Find the max length in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Limit to max_length if specified\n",
        "    if max_length is not None and batch_max_length > max_length:\n",
        "        batch_max_length = max_length\n",
        "\n",
        "    # Prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "\n",
        "        # Truncate if longer than max_length\n",
        "        if max_length is not None:\n",
        "            new_item = new_item[:max_length+1]  # +1 for the added token\n",
        "\n",
        "        # Pad sequences to batch_max_length\n",
        "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
        "\n",
        "        inputs = torch.tensor(padded[:-1])  # Inputs: all tokens except the last\n",
        "        targets = torch.tensor(padded[1:])  # Targets: all tokens shifted by 1\n",
        "\n",
        "        # Replace padding tokens in targets with ignore_index for loss calculation\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:  # If there's more than one padding token\n",
        "            if isinstance(indices, torch.Tensor) and indices.dim() > 0:\n",
        "                targets[indices[1:]] = ignore_index  # Keep only the first padding token\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Stack tensors\n",
        "    inputs_tensor = torch.stack(inputs_lst)\n",
        "    targets_tensor = torch.stack(targets_lst)\n",
        "\n",
        "    # Use cpu for collation then move to device later - better memory management\n",
        "    return inputs_tensor.to(device), targets_tensor.to(device)\n",
        "\n",
        "def quantize_model(model, use_8bit=True):\n",
        "    \"\"\"\n",
        "    Convert the model to 8-bit quantization for both inference and training if GPU is available\n",
        "    \"\"\"\n",
        "    if not use_8bit:\n",
        "        print(\"Skipping 8-bit quantization as running on CPU\")\n",
        "        return model\n",
        "\n",
        "    print(\"Quantizing model to 8-bit...\")\n",
        "\n",
        "    # Replace linear layers with 8-bit quantized linear layers\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            parent_name = '.'.join(name.split('.')[:-1])\n",
        "            child_name = name.split('.')[-1]\n",
        "            parent = model\n",
        "\n",
        "            if parent_name:\n",
        "                for part in parent_name.split('.'):\n",
        "                    parent = getattr(parent, part)\n",
        "\n",
        "            # Replace with 8-bit quantized linear layer\n",
        "            setattr(parent, child_name, bnb.nn.Linear8bitLt(\n",
        "                module.in_features,\n",
        "                module.out_features,\n",
        "                module.bias is not None\n",
        "            ))\n",
        "\n",
        "            # Copy weights and bias\n",
        "            if module.bias is not None:\n",
        "                getattr(parent, child_name).bias = nn.Parameter(module.bias.data)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                getattr(parent, child_name).weight.copy_(module.weight)\n",
        "\n",
        "    print(\"Model quantized to 8-bit successfully!\")\n",
        "    return model\n",
        "\n",
        "def setup_model(device, load_from_checkpoint=None, use_8bit=True):\n",
        "    \"\"\"Set up the GPT-2 774M model and quantize it to 8-bit if use_8bit is True.\"\"\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Download and load GPT-2 774M weights\n",
        "    print(\"Downloading/loading GPT-2 774M model weights...\")\n",
        "    settings, params = load_gpt2_params_from_local_path(model_size=\"774M\", model_path='/content/Files/gpt2_model')\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"Initializing model...\")\n",
        "    model = GPTModel(GPT_CONFIG_774M)\n",
        "\n",
        "    # Load weights\n",
        "    print(\"Loading weights into model...\")\n",
        "    load_weights_into_gpt(model, params)\n",
        "\n",
        "    # Load from checkpoint if provided\n",
        "    if load_from_checkpoint and os.path.exists(load_from_checkpoint):\n",
        "        print(f\"Loading from checkpoint: {load_from_checkpoint}\")\n",
        "        checkpoint = torch.load(load_from_checkpoint, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        print(f\"Loaded checkpoint successfully!\")\n",
        "\n",
        "    # Move model to device first (important for CPU mode)\n",
        "    model.to(device)\n",
        "\n",
        "    # Quantize model to 8-bit if use_8bit is True (requires GPU)\n",
        "    if use_8bit:\n",
        "        model = quantize_model(model, use_8bit)\n",
        "        print(\"Model quantized successfully!\")\n",
        "    else:\n",
        "        print(\"Using full precision model (no quantization)\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    num_epochs=3,\n",
        "    eval_freq=100,  # Evaluate every n steps\n",
        "    save_dir=\"checkpoints\",\n",
        "    output_dir=\"fashion_files\"  # Changed path\n",
        "):\n",
        "    \"\"\"Train the model on fashion dataset.\"\"\"\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Create dirs if they don't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # For tracking\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch = -1\n",
        "    global_step = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Path to save the best model\n",
        "    best_model_path = os.path.join(save_dir, \"best_model.pt\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
        "            # Forward pass\n",
        "            logits = model(inputs)\n",
        "\n",
        "            # Calculate loss (reshape for cross entropy)\n",
        "            b, t, c = logits.shape\n",
        "            loss = torch.nn.functional.cross_entropy(\n",
        "                logits.view(b*t, c),\n",
        "                targets.view(b*t)\n",
        "            )\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Record loss\n",
        "            loss_value = loss.item()\n",
        "            epoch_losses.append(loss_value)\n",
        "            train_losses.append(loss_value)\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\"train_loss\": f\"{loss_value:.4f}\"})\n",
        "\n",
        "            # Evaluate periodically\n",
        "            global_step += 1\n",
        "            if global_step % eval_freq == 0:\n",
        "                val_loss = evaluate_model(model, val_loader, device)\n",
        "                val_losses.append(val_loss)\n",
        "                print(f\"Step {global_step}: Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "                # Save if best\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    best_epoch = epoch\n",
        "                    torch.save({\n",
        "                        \"model_state_dict\": model.state_dict(),\n",
        "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                        \"epoch\": epoch,\n",
        "                        \"global_step\": global_step,\n",
        "                        \"val_loss\": val_loss\n",
        "                    }, best_model_path)\n",
        "                    print(f\"Saved best model (val_loss: {val_loss:.4f}) to {best_model_path}\")\n",
        "\n",
        "                # Switch back to training mode\n",
        "                model.train()\n",
        "\n",
        "        # End of epoch\n",
        "        avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} completed. Avg train loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "        # Final evaluation for this epoch\n",
        "        val_loss = evaluate_model(model, val_loader, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint at end of epoch\n",
        "        epoch_save_path = os.path.join(save_dir, f\"fashion_gpt2_epoch{epoch+1}.pt\")\n",
        "        torch.save({\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"epoch\": epoch,\n",
        "            \"global_step\": global_step,\n",
        "            \"val_loss\": val_loss\n",
        "        }, epoch_save_path)\n",
        "\n",
        "        # Update best model if needed\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            shutil.copy(epoch_save_path, best_model_path)\n",
        "            print(f\"Updated best model with val_loss: {val_loss:.4f}\")\n",
        "\n",
        "    # After all epochs, zip and save the best model\n",
        "    print(f\"Best model was from epoch {best_epoch+1} with val_loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    best_model_zip = os.path.join(output_dir, \"best_model.zip\")\n",
        "    with zipfile.ZipFile(best_model_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        zipf.write(best_model_path, arcname=os.path.basename(best_model_path))\n",
        "\n",
        "    print(f\"Best model zipped and saved to {best_model_zip}\")\n",
        "\n",
        "    return model, train_losses, val_losses, best_epoch+1\n",
        "\n",
        "def evaluate_model(model, data_loader, device, num_batches=None):\n",
        "    \"\"\"Evaluate the model on validation data.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    # Determine how many batches to evaluate\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(data_loader):\n",
        "            if i >= num_batches:\n",
        "                break\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            b, t, c = logits.shape\n",
        "            loss = torch.nn.functional.cross_entropy(\n",
        "                logits.view(b*t, c),\n",
        "                targets.view(b*t)\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "    return total_loss / batch_count if batch_count > 0 else float('inf')\n",
        "\n",
        "def generate_samples(model, tokenizer, val_data, device, num_samples=3):\n",
        "    \"\"\"Generate text samples using the model.\"\"\"\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "\n",
        "    for i in range(min(num_samples, len(val_data))):\n",
        "        entry = val_data[i]\n",
        "        prompt = format_input(entry)\n",
        "\n",
        "        # Tokenize the prompt\n",
        "        encoded = torch.tensor(tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})).unsqueeze(0).to(device)\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            token_ids = generate_text(\n",
        "                model=model,\n",
        "                idx=encoded,\n",
        "                max_new_tokens=150,\n",
        "                context_size=context_size,\n",
        "                temperature=0.7,\n",
        "                top_k=40\n",
        "            )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_text = tokenizer.decode(token_ids.squeeze().tolist())\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\n--- Sample {i+1} ---\")\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Generated: {generated_text[len(prompt):]}\")\n",
        "        print(f\"Expected: {entry['output']}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "def generate_text(model, idx, max_new_tokens, context_size, temperature=0.7, top_k=40, eos_id=50256):\n",
        "    \"\"\"Generate text with the model.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Get context for the current step\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Get logits for the last token\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply temperature and top-k filtering\n",
        "        if temperature > 0:\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "            probs = torch.softmax(logits / temperature, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            # Greedy sampling\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        # Append new token\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        # Stop if EOS token is generated\n",
        "        if idx_next.item() == eos_id:\n",
        "            break\n",
        "\n",
        "    return idx\n",
        "\n",
        "def main():\n",
        "    # Check if GPU is available - bitsandbytes requires GPU\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"WARNING: GPU not available. bitsandbytes requires GPU.\")\n",
        "        print(\"Switching to standard training without 8-bit quantization.\")\n",
        "        use_8bit = False\n",
        "        device = torch.device(\"cpu\")\n",
        "    else:\n",
        "        use_8bit = True\n",
        "        device = torch.device(\"cuda\")\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Using 8-bit quantization: {use_8bit}\")\n",
        "\n",
        "    # Mount Google Drive if running in Colab\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_mounted = True\n",
        "        print(\"Google Drive mounted successfully\")\n",
        "    except ImportError:\n",
        "        drive_mounted = False\n",
        "        print(\"Not running in Colab or Drive already mounted\")\n",
        "\n",
        "    # 1. Load dataset - update path if using Google Drive\n",
        "    dataset_path = \"/content/drive/MyDrive/recommendation_fashion_dataset.json\" if drive_mounted else \"men_fashion_dataset.json\"\n",
        "    data = load_fashion_dataset(dataset_path)\n",
        "\n",
        "    # 2. Split data\n",
        "    train_ratio = 0.85\n",
        "    val_ratio = 0.15\n",
        "\n",
        "    total_size = len(data)\n",
        "    train_size = int(total_size * train_ratio)\n",
        "\n",
        "    # Shuffle data (optional)\n",
        "    import random\n",
        "    random.seed(42)\n",
        "    random.shuffle(data)\n",
        "\n",
        "    train_data = data[:train_size]\n",
        "    val_data = data[train_size:]\n",
        "\n",
        "    print(f\"Training set size: {len(train_data)}\")\n",
        "    print(f\"Validation set size: {len(val_data)}\")\n",
        "\n",
        "    # 3. Setup tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # 4. Create datasets and dataloaders\n",
        "    train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "    val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "\n",
        "    batch_size = 2  # Adjust based on your GPU memory\n",
        "    max_length = 1024  # Context length\n",
        "\n",
        "    # Customize collate function with your device\n",
        "    # If on CPU, keep tensors on CPU during collation for memory efficiency\n",
        "    collate_device = \"cpu\" if device.type == \"cpu\" else device\n",
        "    collate_fn = partial(\n",
        "        custom_collate_fn,\n",
        "        device=collate_device,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=0  # Adjust based on your environment\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    # 5. Check for existing best model - fix paths\n",
        "    base_dir = \"/content/drive/MyDrive/fashion_files\" if drive_mounted else \"fashion_files\"\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    best_model_path = os.path.join(base_dir, \"best_model.zip\")\n",
        "    checkpoint_path = None\n",
        "\n",
        "    if os.path.exists(best_model_path):\n",
        "        print(f\"Found existing best model: {best_model_path}\")\n",
        "        # Extract best model for continued training\n",
        "        with zipfile.ZipFile(best_model_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"temp_model\")\n",
        "\n",
        "        checkpoint_path = \"temp_model/best_model.pt\"\n",
        "        print(f\"Extracted checkpoint to: {checkpoint_path}\")\n",
        "\n",
        "    # 6. Setup model with optional 8-bit quantization\n",
        "    model = setup_model(device, load_from_checkpoint=checkpoint_path, use_8bit=use_8bit)\n",
        "\n",
        "    # 7. Setup optimizer (with 8-bit optimization if using GPU)\n",
        "    if use_8bit:\n",
        "        optimizer = bnb.optim.AdamW8bit(\n",
        "            model.parameters(),\n",
        "            lr=5e-5,  # Learning rate\n",
        "            weight_decay=0.1  # L2 regularization\n",
        "        )\n",
        "    else:\n",
        "        # Use standard AdamW for CPU\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=5e-5,  # Learning rate\n",
        "            weight_decay=0.1  # L2 regularization\n",
        "        )\n",
        "\n",
        "    # 8. Fine-tune the model\n",
        "    print(f\"Starting fine-tuning {'with 8-bit quantization' if use_8bit else 'in full precision mode'}...\")\n",
        "    model, train_losses, val_losses, best_epoch = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        num_epochs=3,  # Adjust based on your needs\n",
        "        eval_freq=50,  # Evaluate every 50 steps\n",
        "        save_dir=\"checkpoints\",\n",
        "        output_dir=base_dir\n",
        "    )\n",
        "\n",
        "    # 9. Generate some samples to evaluate the model\n",
        "    print(f\"\\nGenerating samples using the best model (Epoch {best_epoch})...\")\n",
        "    # Load the best model for generation\n",
        "    best_model = setup_model(device, use_8bit=use_8bit)\n",
        "    best_model_checkpoint = torch.load(\"checkpoints/best_model.pt\", map_location=device)\n",
        "    best_model.load_state_dict(best_model_checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    generate_samples(best_model, tokenizer, val_data, device, num_samples=3)\n",
        "\n",
        "    print(f\"Fine-tuning completed! Best model from epoch {best_epoch} saved to {os.path.join(base_dir, 'best_model.zip')}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "u-CnqSFxEcj8",
        "outputId": "b7fa14ae-e4fb-49ae-ffa5-fde519c472b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: GPU not available. bitsandbytes requires GPU.\n",
            "Switching to standard training without 8-bit quantization.\n",
            "Using device: cpu\n",
            "Using 8-bit quantization: False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ba4413591d94>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-ba4413591d94>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mdrive_mounted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Google Drive mounted successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define source and destination paths\n",
        "source_model_path = '/content/checkpoints/best_model.pt'\n",
        "destination_folder = '/content/drive/MyDrive/fashion_files/'\n",
        "zip_filename = 'best_model.zip'\n",
        "zip_filepath = os.path.join(destination_folder, zip_filename)\n",
        "\n",
        "# Ensure the destination directory exists\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Get file size for tqdm\n",
        "file_size = os.path.getsize(source_model_path)\n",
        "\n",
        "# Custom class to track progress with tqdm\n",
        "class TqdmZipFile(zipfile.ZipFile):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def write(self, filename, arcname=None, compress_type=None):\n",
        "        if arcname is None:\n",
        "            arcname = os.path.basename(filename)\n",
        "\n",
        "        with open(filename, 'rb') as f:\n",
        "            # Set up tqdm progress bar\n",
        "            with tqdm(total=os.path.getsize(filename),\n",
        "                      unit='B',\n",
        "                      unit_scale=True,\n",
        "                      desc=f\"Zipping {os.path.basename(filename)}\") as pbar:\n",
        "                # Create a custom file-like object to update the progress bar\n",
        "                class TqdmReadFile:\n",
        "                    def __init__(self, file, pbar):\n",
        "                        self.file = file\n",
        "                        self.pbar = pbar\n",
        "\n",
        "                    def read(self, size):\n",
        "                        data = self.file.read(size)\n",
        "                        self.pbar.update(len(data))\n",
        "                        return data\n",
        "\n",
        "                zinfo = zipfile.ZipInfo.from_file(filename, arcname)\n",
        "                if compress_type is not None:\n",
        "                    zinfo.compress_type = compress_type\n",
        "                else:\n",
        "                    zinfo.compress_type = self.compression\n",
        "\n",
        "                with self.open(zinfo, 'w') as dest:\n",
        "                    shutil.copyfileobj(TqdmReadFile(f, pbar), dest)\n",
        "\n",
        "# Create zip file with progress tracking\n",
        "try:\n",
        "    print(f\"Creating zip file: {zip_filepath}\")\n",
        "    with TqdmZipFile(zip_filepath, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n",
        "        zipf.write(source_model_path, arcname=os.path.basename(source_model_path))\n",
        "    print(f\"Successfully created zip at: {zip_filepath}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during zipping: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqguau4UHHjj",
        "outputId": "0212400e-560f-410d-e6ac-5214952fcb71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating zip file: /content/drive/MyDrive/fashion_files/best_model.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Zipping best_model.pt: 100%|██████████| 10.2G/10.2G [08:42<00:00, 19.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created zip at: /content/drive/MyDrive/fashion_files/best_model.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import tiktoken\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define paths\n",
        "zip_filepath = '/content/drive/MyDrive/fashion_files/best_model.zip'\n",
        "unzip_destination = '/content/extracted_model/'\n",
        "\n",
        "# Ensure directory exists\n",
        "os.makedirs(unzip_destination, exist_ok=True)\n",
        "\n",
        "# Function to extract zip with progress tracking\n",
        "def extract_with_progress(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # Get file info\n",
        "        file_info_list = zip_ref.infolist()\n",
        "\n",
        "        # Set up tqdm progress bar\n",
        "        with tqdm(total=len(file_info_list), desc=\"Extracting files\") as pbar:\n",
        "            for file_info in file_info_list:\n",
        "                zip_ref.extract(file_info, extract_to)\n",
        "                pbar.update(1)\n",
        "\n",
        "    print(f\"Extraction complete to {extract_to}\")\n",
        "    return os.path.join(extract_to, os.path.basename(zip_path).replace('.zip', '.pt'))\n",
        "\n",
        "# Model loading and interaction functions\n",
        "def format_input(instruction, input_text=\"\"):\n",
        "    \"\"\"Format the instruction and input according to the training format.\"\"\"\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{instruction}\"\n",
        "    )\n",
        "    input_part = f\"\\n\\n### Input:\\n{input_text}\" if input_text else \"\"\n",
        "    return instruction_text + input_part\n",
        "\n",
        "def load_finetuned_model(checkpoint_path, device):\n",
        "    \"\"\"Load the fine-tuned model from a checkpoint.\"\"\"\n",
        "    print(f\"Loading model from {checkpoint_path}\")\n",
        "\n",
        "    # Import your model definitions here\n",
        "    #from gpt_model import GPTModel, GPT_CONFIG_774M\n",
        "\n",
        "    # Initialize model with configuration\n",
        "    model = GPTModel(GPT_CONFIG_774M)\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # Move model to device\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Model loaded successfully! Validation loss: {checkpoint.get('val_loss', 'N/A')}\")\n",
        "    return model\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, device, max_new_tokens=500, temperature=0.7, top_k=40):\n",
        "    \"\"\"Generate text based on a prompt using the fine-tuned model.\"\"\"\n",
        "    # Tokenize the prompt\n",
        "    encoded = torch.tensor(tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})).unsqueeze(0).to(device)\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "\n",
        "    # Generate response\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Get context for the current step\n",
        "            idx_cond = encoded[:, -context_size:]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "            # Get logits for the last token\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Apply temperature and top-k filtering\n",
        "            if temperature > 0:\n",
        "                if top_k is not None:\n",
        "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                    logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "                probs = torch.softmax(logits / temperature, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                # Greedy sampling\n",
        "                idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "            # Append new token\n",
        "            encoded = torch.cat((encoded, idx_next), dim=1)\n",
        "\n",
        "            # Stop if EOS token is generated (50256 is <|endoftext|>)\n",
        "            if idx_next.item() == 50256:\n",
        "                break\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(encoded.squeeze().tolist())\n",
        "\n",
        "    # Return only the response part (after the prompt)\n",
        "    return generated_text[len(prompt):]\n",
        "\n",
        "def test_model_interactive(model, tokenizer, device):\n",
        "    \"\"\"Interactive testing of the model with custom prompts.\"\"\"\n",
        "    print(\"\\n===== Interactive Testing Mode =====\")\n",
        "    print(\"Enter your queries to test the model. Type 'exit' to quit.\")\n",
        "    print(\"Format: instruction | input (optional)\")\n",
        "    print(\"\\nSuggested fashion-related queries:\")\n",
        "    print(\"1. Describe this outfit: | A black leather jacket with white t-shirt and blue jeans\")\n",
        "    print(\"2. Suggest accessories for | a summer wedding guest outfit with a floral dress\")\n",
        "    print(\"3. Compare these styles: | minimalist vs maximalist fashion\")\n",
        "    print(\"4. Create a shopping list for | building a capsule wardrobe for fall\")\n",
        "    print(\"5. What are current trends in | sustainable fashion for 2024?\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYour query: \")\n",
        "\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Parse instruction and optional input\n",
        "        parts = user_input.split('|', 1)\n",
        "        instruction = parts[0].strip()\n",
        "        input_text = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "\n",
        "        # Format the prompt\n",
        "        prompt = format_input(instruction, input_text)\n",
        "\n",
        "        print(\"\\nGenerating response...\")\n",
        "        response = generate_text(model, tokenizer, prompt, device)\n",
        "\n",
        "        print(\"\\n--- Result ---\")\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Generated response: {response}\")\n",
        "        print(\"---------------\")\n",
        "\n",
        "def main():\n",
        "    # Unzip the model file\n",
        "    print(\"Unzipping model file...\")\n",
        "    extracted_model_path = extract_with_progress(zip_filepath, unzip_destination)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Load the fine-tuned model\n",
        "    model = load_finetuned_model(extracted_model_path, device)\n",
        "\n",
        "    # Test model interactively\n",
        "    test_model_interactive(model, tokenizer, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P-IRry2XHHef",
        "outputId": "c86e8b8f-59ad-417f-b2d7-3fb20edf9891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping model file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting files: 100%|██████████| 1/1 [01:34<00:00, 94.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete to /content/extracted_model/\n",
            "Using device: cpu\n",
            "Loading model from /content/extracted_model/best_model.pt\n",
            "Model loaded successfully! Validation loss: 1.2289036760727565\n",
            "\n",
            "===== Interactive Testing Mode =====\n",
            "Enter your queries to test the model. Type 'exit' to quit.\n",
            "Format: instruction | input (optional)\n",
            "\n",
            "Suggested fashion-related queries:\n",
            "1. Describe this outfit: | A black leather jacket with white t-shirt and blue jeans\n",
            "2. Suggest accessories for | a summer wedding guest outfit with a floral dress\n",
            "3. Compare these styles: | minimalist vs maximalist fashion\n",
            "4. Create a shopping list for | building a capsule wardrobe for fall\n",
            "5. What are current trends in | sustainable fashion for 2024?\n",
            "\n",
            "Your query: suggest a pants for pink polo t shirt\n",
            "\n",
            "Generating response...\n",
            "\n",
            "--- Result ---\n",
            "Prompt: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "suggest a pants for pink polo t shirt\n",
            "Generated response: .  I'm thinking a relaxed, slightly worn-in pair with a slightly textured fabric like rayon.  I'm open to suggestions on colors and styles, but nothing too formal or sporty.\n",
            "\n",
            "### Input:\n",
            "{'profile': {'gender': 'male', 'height': '5\\'7\"', 'weight': '175lbs', 'skin_tone': 'tan', 'hair_color': 'auburn', 'eye_color': 'amber', 'body_shape': 'stocky', 'face_shape': 'diamond'}, 'outfit_type': 'pants', 'pattern': 'paisley', 'base_color': 'purple', 'size': '4XL', 'accessories': ['loafers', 'ring'], 'context': 'for a formal event'}\n",
            "\n",
            "### Response:\n",
            "Okay, so a pants for a pink polo t-shirt!  I understand that even stocky builds like yours can pull off a relaxed vibe.  While you like paisley and purple, we'll have to work around your existing wardrobe limitations and build.  We'll need some shopping! \n",
            "\n",
            "I recommend starting with your existing 4XL paisley and purple wardrobe.  Pair it with a lightweight, breathable linen shirt in a neutral color like gray or black.  Linen breathes well and is comfortable.  Look for a paisley paisley print in a relaxed fit that can be easily rolled up or down for a more casual feel.  \n",
            "\n",
            "Since your existing paisley pattern isn't quite right for this occasion, let’s create a new one.  Buy a paisley paisley scarf in shades of purple and paisley, focusing on the paisley to tie in with your new paisley.  Since it's a paisley paisley, the scarf's design will complement your coloring beautifully.  Since it’s a paisley, it can incorporate your preferred paisley pattern in a subtle way.  \n",
            "\n",
            "Since you’re stocky and prefer 4XL, it’ll be important to choose a shirt length that accentuates your shoulders without clinging.  A slightly longer, relaxed fit will look more put-together.  A button-down will do this nicely.  If you can't find a paisley shirt\n",
            "---------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e6ec42559f74>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-e6ec42559f74>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# Test model interactively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mtest_model_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e6ec42559f74>\u001b[0m in \u001b[0;36mtest_model_interactive\u001b[0;34m(model, tokenizer, device)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYour query: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXNzNhcPuyly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Z_sBMIBuy4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import tiktoken\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define paths\n",
        "zip_filepath = '/content/drive/MyDrive/fashion_files/best_model.zip'\n",
        "unzip_destination = '/content/extracted_model/'\n",
        "user_text_path = '/content/drive/MyDrive/fashion_files/user.txt'\n",
        "config_path = '/content/drive/MyDrive/fashion_files/generation_config.json'\n",
        "\n",
        "# Default generation parameters (will be overridden by config file if present)\n",
        "DEFAULT_CONFIG = {\n",
        "    \"max_new_tokens\": 150,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 40,\n",
        "    \"instruction\": \"\",\n",
        "    \"input_text\": \"\"\n",
        "}\n",
        "\n",
        "# Ensure directory exists\n",
        "os.makedirs(unzip_destination, exist_ok=True)\n",
        "\n",
        "# Function to extract zip with progress tracking\n",
        "def extract_with_progress(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # Get file info\n",
        "        file_info_list = zip_ref.infolist()\n",
        "\n",
        "        # Set up tqdm progress bar\n",
        "        with tqdm(total=len(file_info_list), desc=\"Extracting files\") as pbar:\n",
        "            for file_info in file_info_list:\n",
        "                zip_ref.extract(file_info, extract_to)\n",
        "                pbar.update(1)\n",
        "\n",
        "    print(f\"Extraction complete to {extract_to}\")\n",
        "    return os.path.join(extract_to, os.path.basename(zip_path).replace('.zip', '.pt'))\n",
        "\n",
        "# Model loading and interaction functions\n",
        "def format_input(instruction, input_text=\"\"):\n",
        "    \"\"\"Format the instruction and input according to the training format.\"\"\"\n",
        "    # Modified to explicitly tell the model not to generate additional inputs\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request. \"\n",
        "        f\"Do not generate additional inputs or profile information.\"\n",
        "        f\"\\n\\n### Instruction:\\n{instruction}\"\n",
        "    )\n",
        "    input_part = f\"\\n\\n### Input:\\n{input_text}\" if input_text else \"\"\n",
        "    return instruction_text + input_part + \"\\n\\n### Response:\"\n",
        "\n",
        "def load_finetuned_model(checkpoint_path, device):\n",
        "    \"\"\"Load the fine-tuned model from a checkpoint.\"\"\"\n",
        "    print(f\"Loading model from {checkpoint_path}\")\n",
        "\n",
        "    # Import your model definitions here\n",
        "    #from gpt_model import GPTModel, GPT_CONFIG_774M\n",
        "\n",
        "    # Initialize model with configuration\n",
        "    model = GPTModel(GPT_CONFIG_774M)\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # Move model to device\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Model loaded successfully! Validation loss: {checkpoint.get('val_loss', 'N/A')}\")\n",
        "    return model\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, device, max_new_tokens=150, temperature=0.7, top_k=40):\n",
        "    \"\"\"Generate text based on a prompt using the fine-tuned model.\"\"\"\n",
        "    # Tokenize the prompt\n",
        "    encoded = torch.tensor(tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})).unsqueeze(0).to(device)\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "\n",
        "    # Generate response\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Generation parameters: max_tokens={max_new_tokens}, temp={temperature}, top_k={top_k}\")\n",
        "    generated_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(max_new_tokens), desc=\"Generating tokens\"):\n",
        "            # Get context for the current step\n",
        "            idx_cond = encoded[:, -context_size:]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "            # Get logits for the last token\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Apply temperature and top-k filtering\n",
        "            if temperature > 0:\n",
        "                if top_k is not None:\n",
        "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                    logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "                probs = torch.softmax(logits / temperature, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                # Greedy sampling\n",
        "                idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "            # Append new token\n",
        "            encoded = torch.cat((encoded, idx_next), dim=1)\n",
        "            generated_tokens.append(idx_next.item())\n",
        "\n",
        "            # Stop if EOS token is generated (50256 is <|endoftext|>)\n",
        "            if idx_next.item() == 50256:\n",
        "                break\n",
        "\n",
        "            # Also stop if we see \"### Input:\" - this is a sign the model is hallucinating additional input\n",
        "            current_text = tokenizer.decode(encoded.squeeze().tolist()[len(tokenizer.encode(prompt)):])\n",
        "            if \"### Input:\" in current_text:\n",
        "                break\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(encoded.squeeze().tolist())\n",
        "\n",
        "    # Return only the response part (after the prompt)\n",
        "    response = generated_text[len(prompt):]\n",
        "\n",
        "    # Remove any hallucinated input sections\n",
        "    if \"### Input:\" in response:\n",
        "        response = response.split(\"### Input:\")[0].strip()\n",
        "\n",
        "    # Remove any hallucinated response markers\n",
        "    if \"### Response:\" in response:\n",
        "        response = response.replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    return response, generated_tokens\n",
        "\n",
        "def load_user_config():\n",
        "    \"\"\"Load user configuration from config file if it exists, otherwise use defaults.\"\"\"\n",
        "    config = DEFAULT_CONFIG.copy()\n",
        "\n",
        "    # Check if config file exists\n",
        "    if os.path.exists(config_path):\n",
        "        try:\n",
        "            with open(config_path, 'r') as f:\n",
        "                user_config = json.load(f)\n",
        "                config.update(user_config)\n",
        "                print(f\"Loaded configuration from {config_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading config file: {e}\")\n",
        "            print(\"Using default configuration\")\n",
        "    else:\n",
        "        print(f\"Config file not found at {config_path}. Using default configuration.\")\n",
        "        # Create a template config file for future use\n",
        "        try:\n",
        "            with open(config_path, 'w') as f:\n",
        "                json.dump(DEFAULT_CONFIG, f, indent=4)\n",
        "                print(f\"Created template config file at {config_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create template config file: {e}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "def load_user_text():\n",
        "    \"\"\"Load user text from file if it exists.\"\"\"\n",
        "    if os.path.exists(user_text_path):\n",
        "        try:\n",
        "            with open(user_text_path, 'r') as f:\n",
        "                content = f.read().strip()\n",
        "                print(f\"Loaded user text from {user_text_path} ({len(content)} characters)\")\n",
        "                return content\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading user text file: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"User text file not found at {user_text_path}\")\n",
        "        return None\n",
        "\n",
        "def update_config_interactive(config):\n",
        "    \"\"\"Allow user to interactively update configuration parameters.\"\"\"\n",
        "    print(\"\\n===== Generation Configuration =====\")\n",
        "    print(\"Current settings:\")\n",
        "    for key, value in config.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    print(\"\\nEnter new values (press Enter to keep current value):\")\n",
        "\n",
        "    # Update numerical parameters\n",
        "    for param in [\"max_new_tokens\", \"temperature\", \"top_k\"]:\n",
        "        new_value = input(f\"{param} [{config[param]}]: \")\n",
        "        if new_value.strip():\n",
        "            try:\n",
        "                if param == \"max_new_tokens\" or param == \"top_k\":\n",
        "                    config[param] = int(new_value)\n",
        "                else:\n",
        "                    config[param] = float(new_value)\n",
        "            except ValueError:\n",
        "                print(f\"Invalid value for {param}, keeping current value.\")\n",
        "\n",
        "    # Save updated config\n",
        "    try:\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(config, f, indent=4)\n",
        "            print(f\"Configuration saved to {config_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save configuration: {e}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "def save_user_text(text):\n",
        "    \"\"\"Save user text to file.\"\"\"\n",
        "    try:\n",
        "        with open(user_text_path, 'w') as f:\n",
        "            f.write(text)\n",
        "        print(f\"Text saved to {user_text_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save text: {e}\")\n",
        "\n",
        "def process_user_input_file(model, tokenizer, device):\n",
        "    \"\"\"Process user input from file and generate response.\"\"\"\n",
        "    # Load configuration\n",
        "    config = load_user_config()\n",
        "\n",
        "    while True:\n",
        "        # Get user text\n",
        "        user_text = load_user_text()\n",
        "        if user_text is None:\n",
        "            print(\"Would you like to create a user.txt file? (y/n)\")\n",
        "            if input().lower() == 'y':\n",
        "                print(\"Enter your text (end with a newline and Ctrl+D):\")\n",
        "                text = []\n",
        "                try:\n",
        "                    while True:\n",
        "                        line = input()\n",
        "                        text.append(line)\n",
        "                except EOFError:\n",
        "                    user_text = '\\n'.join(text)\n",
        "                    save_user_text(user_text)\n",
        "            else:\n",
        "                print(\"Switching to interactive mode...\")\n",
        "                return test_model_interactive(model, tokenizer, device)\n",
        "\n",
        "        # Parse instruction and input from user text\n",
        "        parts = user_text.split('|', 1)\n",
        "        config[\"instruction\"] = parts[0].strip()\n",
        "        config[\"input_text\"] = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "\n",
        "        # Allow user to adjust generation parameters\n",
        "        print(\"\\nWould you like to adjust generation parameters? (y/n)\")\n",
        "        if input().lower() == 'y':\n",
        "            config = update_config_interactive(config)\n",
        "\n",
        "        # Format the prompt\n",
        "        prompt = format_input(config[\"instruction\"], config[\"input_text\"])\n",
        "\n",
        "        # Generate response\n",
        "        print(\"\\nGenerating response...\")\n",
        "        response, tokens = generate_text(\n",
        "            model, tokenizer, prompt, device,\n",
        "            max_new_tokens=config[\"max_new_tokens\"],\n",
        "            temperature=config[\"temperature\"],\n",
        "            top_k=config[\"top_k\"]\n",
        "        )\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\n=== Generated Response ===\")\n",
        "        print(response)\n",
        "        print(\"\\n=== Token Stats ===\")\n",
        "        print(f\"Generated {len(tokens)} tokens\")\n",
        "        print(f\"Input tokens: {len(tokenizer.encode(prompt))}\")\n",
        "        print(f\"Total tokens: {len(tokenizer.encode(prompt)) + len(tokens)}\")\n",
        "\n",
        "        # Save response\n",
        "        response_path = os.path.join(os.path.dirname(user_text_path), \"response.txt\")\n",
        "        try:\n",
        "            with open(response_path, 'w') as f:\n",
        "                f.write(response)\n",
        "            print(f\"Response saved to {response_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save response: {e}\")\n",
        "\n",
        "        # Ask to continue\n",
        "        print(\"\\nOptions:\")\n",
        "        print(\"1. Process the same text with different parameters\")\n",
        "        print(\"2. Edit the user.txt file\")\n",
        "        print(\"3. Switch to interactive mode\")\n",
        "        print(\"4. Exit\")\n",
        "        choice = input(\"Choose an option (1-4): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            continue\n",
        "        elif choice == '2':\n",
        "            print(\"Enter new text (end with a newline and Ctrl+D):\")\n",
        "            text = []\n",
        "            try:\n",
        "                while True:\n",
        "                    line = input()\n",
        "                    text.append(line)\n",
        "            except EOFError:\n",
        "                user_text = '\\n'.join(text)\n",
        "                save_user_text(user_text)\n",
        "        elif choice == '3':\n",
        "            return test_model_interactive(model, tokenizer, device)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "def test_model_interactive(model, tokenizer, device):\n",
        "    \"\"\"Interactive testing of the model with custom prompts.\"\"\"\n",
        "    print(\"\\n===== Interactive Testing Mode =====\")\n",
        "    print(\"Enter your queries to test the model. Type 'exit' to quit.\")\n",
        "    print(\"Format: instruction | input (optional)\")\n",
        "\n",
        "    # Provide helpful suggestions\n",
        "    print(\"\\nSuggested fashion-related queries:\")\n",
        "    print(\"1. Describe this outfit: | A black leather jacket with white t-shirt and blue jeans\")\n",
        "    print(\"2. Suggest accessories for | a summer wedding guest outfit with a floral dress\")\n",
        "    print(\"3. Compare these styles: | minimalist vs maximalist fashion\")\n",
        "    print(\"4. Create a shopping list for | building a capsule wardrobe for fall\")\n",
        "    print(\"5. What are current trends in | sustainable fashion for 2024?\")\n",
        "\n",
        "    # Load default config for generation parameters\n",
        "    config = load_user_config()\n",
        "\n",
        "    while True:\n",
        "        # Ask to adjust generation parameters\n",
        "        print(\"\\nWould you like to adjust generation parameters? (y/n)\")\n",
        "        if input().lower() == 'y':\n",
        "            config = update_config_interactive(config)\n",
        "\n",
        "        user_input = input(\"\\nYour query: \")\n",
        "\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "        elif user_input.lower() == 'file':\n",
        "            return process_user_input_file(model, tokenizer, device)\n",
        "\n",
        "        # Parse instruction and optional input\n",
        "        parts = user_input.split('|', 1)\n",
        "        instruction = parts[0].strip()\n",
        "        input_text = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "\n",
        "        # Format the prompt\n",
        "        prompt = format_input(instruction, input_text)\n",
        "\n",
        "        print(\"\\nGenerating response...\")\n",
        "        response, tokens = generate_text(\n",
        "            model, tokenizer, prompt, device,\n",
        "            max_new_tokens=config[\"max_new_tokens\"],\n",
        "            temperature=config[\"temperature\"],\n",
        "            top_k=config[\"top_k\"]\n",
        "        )\n",
        "\n",
        "        print(\"\\n=== Generated Response ===\")\n",
        "        print(response)\n",
        "        print(\"\\n=== Token Stats ===\")\n",
        "        print(f\"Generated {len(tokens)} tokens\")\n",
        "\n",
        "def main():\n",
        "    # Unzip the model file\n",
        "    print(\"Unzipping model file...\")\n",
        "    extracted_model_path = extract_with_progress(zip_filepath, unzip_destination)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Load the fine-tuned model\n",
        "    model = load_finetuned_model(extracted_model_path, device)\n",
        "\n",
        "    # Check if user.txt file exists\n",
        "    if os.path.exists(user_text_path):\n",
        "        print(f\"Found user text file at {user_text_path}\")\n",
        "        process_user_input_file(model, tokenizer, device)\n",
        "    else:\n",
        "        print(\"No user text file found. Starting in interactive mode.\")\n",
        "        # Test model interactively\n",
        "        test_model_interactive(model, tokenizer, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "AWrUv2sFHHV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41708806-0594-4855-9d2a-41a63a2cc184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping model file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting files: 100%|██████████| 1/1 [01:26<00:00, 86.94s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete to /content/extracted_model/\n",
            "Using device: cpu\n",
            "Loading model from /content/extracted_model/best_model.pt\n",
            "Model loaded successfully! Validation loss: 1.2289036760727565\n",
            "No user text file found. Starting in interactive mode.\n",
            "\n",
            "===== Interactive Testing Mode =====\n",
            "Enter your queries to test the model. Type 'exit' to quit.\n",
            "Format: instruction | input (optional)\n",
            "\n",
            "Suggested fashion-related queries:\n",
            "1. Describe this outfit: | A black leather jacket with white t-shirt and blue jeans\n",
            "2. Suggest accessories for | a summer wedding guest outfit with a floral dress\n",
            "3. Compare these styles: | minimalist vs maximalist fashion\n",
            "4. Create a shopping list for | building a capsule wardrobe for fall\n",
            "5. What are current trends in | sustainable fashion for 2024?\n",
            "Config file not found at /content/drive/MyDrive/fashion_files/generation_config.json. Using default configuration.\n",
            "Created template config file at /content/drive/MyDrive/fashion_files/generation_config.json\n",
            "\n",
            "Would you like to adjust generation parameters? (y/n)\n",
            "y\n",
            "\n",
            "===== Generation Configuration =====\n",
            "Current settings:\n",
            "  max_new_tokens: 150\n",
            "  temperature: 0.7\n",
            "  top_k: 40\n",
            "  instruction: \n",
            "  input_text: \n",
            "\n",
            "Enter new values (press Enter to keep current value):\n",
            "max_new_tokens [150]: 300\n",
            "temperature [0.7]: 0.7\n",
            "top_k [40]: 40\n",
            "Configuration saved to /content/drive/MyDrive/fashion_files/generation_config.json\n",
            "\n",
            "Your query: suggest some pants for pink polo tshirt\n",
            "\n",
            "Generating response...\n",
            "Generation parameters: max_tokens=300, temp=0.7, top_k=40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating tokens: 100%|██████████| 300/300 [07:50<00:00,  1.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generated Response ===\n",
            "\n",
            "Hey there! Pants for a Pink Polo Tshirt are perfect!  While your existing t-shirt is a bit too casual, we can definitely elevate it with some fresh pants.\n",
            "\n",
            "I recommend a pair of well-fitting dark wash jeans or chinos.  Dark wash denim will create a sleek, polished look, ideal for balancing a bold polo.  Make sure the length hits just above the knee, avoiding anything too baggy.  For a subtle pop of pink, consider a subtle pink bandana tied loosely around the neck or a pocket square with a pink pendant.  This adds a feminine touch without being overwhelming.\n",
            "\n",
            "For a slightly more relaxed vibe, consider a pair of dark wash denim shorts or cargo shorts that hit just above the knee.  Cargo shorts offer a slightly more structured feel than denim shorts but are still comfortable.  \n",
            "\n",
            "Accessorize with a simple silver chain necklace or a sleek, textured pocket square.  A brown leather belt will complement the tan pants nicely.  Since you're tall, a good knee-length would be ideal for a perfect fit.  For those with larger sizes, consider finding a pair of well-fitting chinos in a darker shade in the same color family or opting for a lighter fabric like cotton or linen blend for a breathable and balanced look.  \n",
            "\n",
            "This combination is flattering on your hips and shoulders, while the darker denim/chinos create a balanced silhouette. The\n",
            "\n",
            "=== Token Stats ===\n",
            "Generated 300 tokens\n",
            "\n",
            "Would you like to adjust generation parameters? (y/n)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import tiktoken\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define paths\n",
        "zip_filepath = '/content/drive/MyDrive/fashion_files/best_model.zip'\n",
        "unzip_destination = '/content/extracted_model/'\n",
        "user_text_path = '/content/drive/MyDrive/fashion_files/user.txt'\n",
        "config_path = '/content/drive/MyDrive/fashion_files/generation_config.json'\n",
        "\n",
        "# Default generation parameters (will be overridden by config file if present)\n",
        "DEFAULT_CONFIG = {\n",
        "    \"max_new_tokens\": 150,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 40,\n",
        "    \"instruction\": \"\",\n",
        "    \"input_text\": \"\",\n",
        "    \"consider_wardrobe\": \"no\",  # New parameter for personalization\n",
        "    \"consider_user_details\": \"no\"  # New parameter for personalization\n",
        "}\n",
        "\n",
        "# Ensure directory exists\n",
        "os.makedirs(unzip_destination, exist_ok=True)\n",
        "\n",
        "# Function to extract zip with progress tracking\n",
        "def extract_with_progress(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # Get file info\n",
        "        file_info_list = zip_ref.infolist()\n",
        "\n",
        "        # Set up tqdm progress bar\n",
        "        with tqdm(total=len(file_info_list), desc=\"Extracting files\") as pbar:\n",
        "            for file_info in file_info_list:\n",
        "                zip_ref.extract(file_info, extract_to)\n",
        "                pbar.update(1)\n",
        "\n",
        "    print(f\"Extraction complete to {extract_to}\")\n",
        "    return os.path.join(extract_to, os.path.basename(zip_path).replace('.zip', '.pt'))\n",
        "\n",
        "# New class for parsing and handling user profile data\n",
        "class UserProfileManager:\n",
        "    def __init__(self, user_text_path):\n",
        "        self.user_text_path = user_text_path\n",
        "        self.user_data = {}\n",
        "        self.user_wardrobe = []\n",
        "        self.load_user_data()\n",
        "\n",
        "    def load_user_data(self):\n",
        "        \"\"\"Load and parse user data from user.txt\"\"\"\n",
        "        if not os.path.exists(self.user_text_path):\n",
        "            print(f\"User profile not found at {self.user_text_path}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            with open(self.user_text_path, 'r') as f:\n",
        "                content = f.read().strip()\n",
        "                # Parse the user data\n",
        "                self._parse_user_data(content)\n",
        "                print(f\"Loaded user profile from {self.user_text_path}\")\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading user profile: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _parse_user_data(self, content):\n",
        "        \"\"\"Parse user data content into structured data\"\"\"\n",
        "        lines = content.split('\\n')\n",
        "        for line in lines:\n",
        "            # Skip empty lines and lines without a proper key:value format\n",
        "            if not line.strip() or ':' not in line:\n",
        "                continue\n",
        "\n",
        "            parts = line.split(':', 1)\n",
        "            key = parts[0].strip().lower()\n",
        "            value = parts[1].strip()\n",
        "\n",
        "            # Special handling for wardrobe items\n",
        "            if key == 'wardrobe':\n",
        "                self.user_wardrobe = [item.strip() for item in value.split(',')]\n",
        "                self.user_data[key] = self.user_wardrobe\n",
        "            else:\n",
        "                self.user_data[key] = value\n",
        "\n",
        "    def get_user_profile_summary(self):\n",
        "        \"\"\"Get a formatted summary of the user profile for the model\"\"\"\n",
        "        if not self.user_data:\n",
        "            return \"No user profile data available.\"\n",
        "\n",
        "        summary = []\n",
        "        # Add basic user information\n",
        "        for key, value in self.user_data.items():\n",
        "            if key != 'wardrobe':  # Handle wardrobe separately\n",
        "                summary.append(f\"{key}: {value}\")\n",
        "\n",
        "        return \"\\n\".join(summary)\n",
        "\n",
        "    def get_wardrobe_summary(self):\n",
        "        \"\"\"Get a formatted summary of the user's wardrobe\"\"\"\n",
        "        if not self.user_wardrobe:\n",
        "            return \"No wardrobe information available.\"\n",
        "\n",
        "        return \"Wardrobe items: \" + \", \".join(self.user_wardrobe)\n",
        "\n",
        "# Model loading and interaction functions\n",
        "def format_input(instruction, input_text=\"\", consider_wardrobe=\"no\", consider_user_details=\"no\", user_profile=None):\n",
        "    \"\"\"Format the instruction and input according to the training format, including personalization options.\"\"\"\n",
        "    # Base instruction\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request. \"\n",
        "        f\"Do not generate additional inputs or profile information.\"\n",
        "        f\"\\n\\n### Instruction:\\n{instruction}\"\n",
        "    )\n",
        "\n",
        "    # Add personalization details if requested\n",
        "    if user_profile and consider_user_details.lower() == \"yes\":\n",
        "        instruction_text += f\"\\n\\nConsider the following user details for personalization:\\n{user_profile.get_user_profile_summary()}\"\n",
        "\n",
        "    # Add wardrobe information if requested\n",
        "    if user_profile and consider_wardrobe.lower() == \"yes\":\n",
        "        instruction_text += f\"\\n\\nConsider these items from the user's wardrobe for your suggestions:\\n{user_profile.get_wardrobe_summary()}\"\n",
        "\n",
        "    # Add input text if provided\n",
        "    input_part = f\"\\n\\n### Input:\\n{input_text}\" if input_text else \"\"\n",
        "\n",
        "    return instruction_text + input_part + \"\\n\\n### Response:\"\n",
        "\n",
        "def load_finetuned_model(checkpoint_path, device):\n",
        "    \"\"\"Load the fine-tuned model from a checkpoint.\"\"\"\n",
        "    print(f\"Loading model from {checkpoint_path}\")\n",
        "\n",
        "    # Import your model definitions here\n",
        "    #from gpt_model import GPTModel, GPT_CONFIG_774M\n",
        "\n",
        "    # Initialize model with configuration\n",
        "    model = GPTModel(GPT_CONFIG_774M)\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # Move model to device\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Model loaded successfully! Validation loss: {checkpoint.get('val_loss', 'N/A')}\")\n",
        "    return model\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, device, max_new_tokens=150, temperature=0.7, top_k=40):\n",
        "    \"\"\"Generate text based on a prompt using the fine-tuned model.\"\"\"\n",
        "    # Tokenize the prompt\n",
        "    encoded = torch.tensor(tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})).unsqueeze(0).to(device)\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "\n",
        "    # Generate response\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Generation parameters: max_tokens={max_new_tokens}, temp={temperature}, top_k={top_k}\")\n",
        "    generated_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(max_new_tokens), desc=\"Generating tokens\"):\n",
        "            # Get context for the current step\n",
        "            idx_cond = encoded[:, -context_size:]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "            # Get logits for the last token\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Apply temperature and top-k filtering\n",
        "            if temperature > 0:\n",
        "                if top_k is not None:\n",
        "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                    logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "                probs = torch.softmax(logits / temperature, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                # Greedy sampling\n",
        "                idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "            # Append new token\n",
        "            encoded = torch.cat((encoded, idx_next), dim=1)\n",
        "            generated_tokens.append(idx_next.item())\n",
        "\n",
        "            # Stop if EOS token is generated (50256 is <|endoftext|>)\n",
        "            if idx_next.item() == 50256:\n",
        "                break\n",
        "\n",
        "            # Also stop if we see \"### Input:\" - this is a sign the model is hallucinating additional input\n",
        "            current_text = tokenizer.decode(encoded.squeeze().tolist()[len(tokenizer.encode(prompt)):])\n",
        "            if \"### Input:\" in current_text:\n",
        "                break\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(encoded.squeeze().tolist())\n",
        "\n",
        "    # Return only the response part (after the prompt)\n",
        "    response = generated_text[len(prompt):]\n",
        "\n",
        "    # Remove any hallucinated input sections\n",
        "    if \"### Input:\" in response:\n",
        "        response = response.split(\"### Input:\")[0].strip()\n",
        "\n",
        "    # Remove any hallucinated response markers\n",
        "    if \"### Response:\" in response:\n",
        "        response = response.replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    return response, generated_tokens\n",
        "\n",
        "def load_user_config():\n",
        "    \"\"\"Load user configuration from config file if it exists, otherwise use defaults.\"\"\"\n",
        "    config = DEFAULT_CONFIG.copy()\n",
        "\n",
        "    # Check if config file exists\n",
        "    if os.path.exists(config_path):\n",
        "        try:\n",
        "            with open(config_path, 'r') as f:\n",
        "                user_config = json.load(f)\n",
        "                config.update(user_config)\n",
        "                print(f\"Loaded configuration from {config_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading config file: {e}\")\n",
        "            print(\"Using default configuration\")\n",
        "    else:\n",
        "        print(f\"Config file not found at {config_path}. Using default configuration.\")\n",
        "        # Create a template config file for future use\n",
        "        try:\n",
        "            with open(config_path, 'w') as f:\n",
        "                json.dump(DEFAULT_CONFIG, f, indent=4)\n",
        "                print(f\"Created template config file at {config_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create template config file: {e}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "def load_user_text():\n",
        "    \"\"\"Load user text from file if it exists.\"\"\"\n",
        "    if os.path.exists(user_text_path):\n",
        "        try:\n",
        "            with open(user_text_path, 'r') as f:\n",
        "                content = f.read().strip()\n",
        "                print(f\"Loaded user text from {user_text_path} ({len(content)} characters)\")\n",
        "                return content\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading user text file: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"User text file not found at {user_text_path}\")\n",
        "        return None\n",
        "\n",
        "def update_config_interactive(config):\n",
        "    \"\"\"Allow user to interactively update configuration parameters.\"\"\"\n",
        "    print(\"\\n===== Generation Configuration =====\")\n",
        "    print(\"Current settings:\")\n",
        "    for key, value in config.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    print(\"\\nEnter new values (press Enter to keep current value):\")\n",
        "\n",
        "    # Update numerical parameters\n",
        "    for param in [\"max_new_tokens\", \"temperature\", \"top_k\"]:\n",
        "        new_value = input(f\"{param} [{config[param]}]: \")\n",
        "        if new_value.strip():\n",
        "            try:\n",
        "                if param == \"max_new_tokens\" or param == \"top_k\":\n",
        "                    config[param] = int(new_value)\n",
        "                else:\n",
        "                    config[param] = float(new_value)\n",
        "            except ValueError:\n",
        "                print(f\"Invalid value for {param}, keeping current value.\")\n",
        "\n",
        "    # Update personalization parameters\n",
        "    for param in [\"consider_wardrobe\", \"consider_user_details\"]:\n",
        "        new_value = input(f\"{param} [{config[param]}] (yes/no): \")\n",
        "        if new_value.strip().lower() in ['yes', 'no']:\n",
        "            config[param] = new_value.lower()\n",
        "\n",
        "    # Save updated config\n",
        "    try:\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(config, f, indent=4)\n",
        "            print(f\"Configuration saved to {config_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save configuration: {e}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "def save_user_text(text):\n",
        "    \"\"\"Save user text to file.\"\"\"\n",
        "    try:\n",
        "        with open(user_text_path, 'w') as f:\n",
        "            f.write(text)\n",
        "        print(f\"Text saved to {user_text_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save text: {e}\")\n",
        "\n",
        "def process_user_input_file(model, tokenizer, device, user_profile):\n",
        "    \"\"\"Process user input from file and generate response.\"\"\"\n",
        "    # Load configuration\n",
        "    config = load_user_config()\n",
        "\n",
        "    while True:\n",
        "        # Get user text\n",
        "        user_text = load_user_text()\n",
        "        if user_text is None:\n",
        "            print(\"Would you like to create a user.txt file? (y/n)\")\n",
        "            if input().lower() == 'y':\n",
        "                print(\"Enter your text (end with a newline and Ctrl+D):\")\n",
        "                text = []\n",
        "                try:\n",
        "                    while True:\n",
        "                        line = input()\n",
        "                        text.append(line)\n",
        "                except EOFError:\n",
        "                    user_text = '\\n'.join(text)\n",
        "                    save_user_text(user_text)\n",
        "            else:\n",
        "                print(\"Switching to interactive mode...\")\n",
        "                return test_model_interactive(model, tokenizer, device, user_profile)\n",
        "\n",
        "        # Parse instruction and input from user text\n",
        "        parts = user_text.split('|', 1)\n",
        "        config[\"instruction\"] = parts[0].strip()\n",
        "        config[\"input_text\"] = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "\n",
        "        # Allow user to adjust generation parameters\n",
        "        print(\"\\nWould you like to adjust generation parameters? (y/n)\")\n",
        "        if input().lower() == 'y':\n",
        "            config = update_config_interactive(config)\n",
        "\n",
        "        # Format the prompt with personalization options\n",
        "        prompt = format_input(\n",
        "            config[\"instruction\"],\n",
        "            config[\"input_text\"],\n",
        "            config[\"consider_wardrobe\"],\n",
        "            config[\"consider_user_details\"],\n",
        "            user_profile\n",
        "        )\n",
        "\n",
        "        # Generate response\n",
        "        print(\"\\nGenerating response...\")\n",
        "        response, tokens = generate_text(\n",
        "            model, tokenizer, prompt, device,\n",
        "            max_new_tokens=config[\"max_new_tokens\"],\n",
        "            temperature=config[\"temperature\"],\n",
        "            top_k=config[\"top_k\"]\n",
        "        )\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\n=== Generated Response ===\")\n",
        "        print(response)\n",
        "        print(\"\\n=== Token Stats ===\")\n",
        "        print(f\"Generated {len(tokens)} tokens\")\n",
        "        print(f\"Input tokens: {len(tokenizer.encode(prompt))}\")\n",
        "        print(f\"Total tokens: {len(tokenizer.encode(prompt)) + len(tokens)}\")\n",
        "        print(f\"Personalization: Wardrobe = {config['consider_wardrobe']}, User Details = {config['consider_user_details']}\")\n",
        "\n",
        "        # Save response\n",
        "        response_path = os.path.join(os.path.dirname(user_text_path), \"response.txt\")\n",
        "        try:\n",
        "            with open(response_path, 'w') as f:\n",
        "                f.write(response)\n",
        "            print(f\"Response saved to {response_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save response: {e}\")\n",
        "\n",
        "        # Ask to continue\n",
        "        print(\"\\nOptions:\")\n",
        "        print(\"1. Process the same text with different parameters\")\n",
        "        print(\"2. Edit the user.txt file\")\n",
        "        print(\"3. Switch to interactive mode\")\n",
        "        print(\"4. Exit\")\n",
        "        choice = input(\"Choose an option (1-4): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            continue\n",
        "        elif choice == '2':\n",
        "            print(\"Enter new text (end with a newline and Ctrl+D):\")\n",
        "            text = []\n",
        "            try:\n",
        "                while True:\n",
        "                    line = input()\n",
        "                    text.append(line)\n",
        "            except EOFError:\n",
        "                user_text = '\\n'.join(text)\n",
        "                save_user_text(user_text)\n",
        "        elif choice == '3':\n",
        "            return test_model_interactive(model, tokenizer, device, user_profile)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "def test_model_interactive(model, tokenizer, device, user_profile):\n",
        "    \"\"\"Interactive testing of the model with custom prompts.\"\"\"\n",
        "    print(\"\\n===== Interactive Testing Mode =====\")\n",
        "    print(\"Enter your queries to test the model. Type 'exit' to quit.\")\n",
        "    print(\"Format: instruction | input (optional)\")\n",
        "\n",
        "    # Provide helpful suggestions\n",
        "    print(\"\\nSuggested fashion-related queries:\")\n",
        "    print(\"1. Describe this outfit: | A black leather jacket with white t-shirt and blue jeans\")\n",
        "    print(\"2. Suggest accessories for | a summer wedding guest outfit with a floral dress\")\n",
        "    print(\"3. Compare these styles: | minimalist vs maximalist fashion\")\n",
        "    print(\"4. Create a shopping list for | building a capsule wardrobe for fall\")\n",
        "    print(\"5. What are current trends in | sustainable fashion for 2024?\")\n",
        "\n",
        "    # Load default config for generation parameters\n",
        "    config = load_user_config()\n",
        "\n",
        "    while True:\n",
        "        # Show current personalization settings\n",
        "        print(f\"\\nCurrent personalization: Consider Wardrobe = {config['consider_wardrobe']}, Consider User Details = {config['consider_user_details']}\")\n",
        "\n",
        "        # Ask to adjust generation parameters\n",
        "        print(\"\\nWould you like to adjust generation parameters? (y/n)\")\n",
        "        if input().lower() == 'y':\n",
        "            config = update_config_interactive(config)\n",
        "\n",
        "        user_input = input(\"\\nYour query: \")\n",
        "\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "        elif user_input.lower() == 'file':\n",
        "            return process_user_input_file(model, tokenizer, device, user_profile)\n",
        "\n",
        "        # Parse instruction and optional input\n",
        "        parts = user_input.split('|', 1)\n",
        "        instruction = parts[0].strip()\n",
        "        input_text = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "\n",
        "        # Format the prompt with personalization options\n",
        "        prompt = format_input(\n",
        "            instruction,\n",
        "            input_text,\n",
        "            config[\"consider_wardrobe\"],\n",
        "            config[\"consider_user_details\"],\n",
        "            user_profile\n",
        "        )\n",
        "\n",
        "        print(\"\\nGenerating response...\")\n",
        "        response, tokens = generate_text(\n",
        "            model, tokenizer, prompt, device,\n",
        "            max_new_tokens=config[\"max_new_tokens\"],\n",
        "            temperature=config[\"temperature\"],\n",
        "            top_k=config[\"top_k\"]\n",
        "        )\n",
        "\n",
        "        print(\"\\n=== Generated Response ===\")\n",
        "        print(response)\n",
        "        print(\"\\n=== Token Stats ===\")\n",
        "        print(f\"Generated {len(tokens)} tokens\")\n",
        "        print(f\"Personalization: Wardrobe = {config['consider_wardrobe']}, User Details = {config['consider_user_details']}\")\n",
        "\n",
        "def update_user_profile(user_profile):\n",
        "    \"\"\"Interactive function to update user profile data\"\"\"\n",
        "    print(\"\\n===== Update User Profile =====\")\n",
        "\n",
        "    if user_profile.user_data:\n",
        "        print(\"Current profile:\")\n",
        "        for key, value in user_profile.user_data.items():\n",
        "            if key != 'wardrobe':  # Display wardrobe separately\n",
        "                print(f\"  {key}: {value}\")\n",
        "\n",
        "        # Display wardrobe items separately\n",
        "        if user_profile.user_wardrobe:\n",
        "            print(\"Wardrobe items:\")\n",
        "            for i, item in enumerate(user_profile.user_wardrobe, 1):\n",
        "                print(f\"  {i}. {item}\")\n",
        "    else:\n",
        "        print(\"No profile data found. Creating new profile.\")\n",
        "\n",
        "    print(\"\\nOptions:\")\n",
        "    print(\"1. Edit basic profile information\")\n",
        "    print(\"2. Edit wardrobe items\")\n",
        "    print(\"3. Cancel\")\n",
        "\n",
        "    choice = input(\"Choose an option (1-3): \")\n",
        "\n",
        "    if choice == '1':\n",
        "        # Edit basic profile info\n",
        "        print(\"\\nEnter profile information (press Enter to keep current values, 'delete' to remove):\")\n",
        "        fields = ['name', 'age', 'gender', 'location', 'style', 'occupation', 'preferences', 'favorite_colors', 'size']\n",
        "\n",
        "        updated_data = {}\n",
        "        for field in fields:\n",
        "            current = user_profile.user_data.get(field, '')\n",
        "            new_value = input(f\"{field} [{current}]: \")\n",
        "\n",
        "            if new_value.lower() == 'delete':\n",
        "                # Don't include this field\n",
        "                pass\n",
        "            elif new_value.strip():\n",
        "                # Update with new value\n",
        "                updated_data[field] = new_value\n",
        "            elif field in user_profile.user_data:\n",
        "                # Keep current value\n",
        "                updated_data[field] = current\n",
        "\n",
        "        # Preserve wardrobe data\n",
        "        if 'wardrobe' in user_profile.user_data:\n",
        "            updated_data['wardrobe'] = user_profile.user_data['wardrobe']\n",
        "\n",
        "        # Update user data\n",
        "        user_profile.user_data = updated_data\n",
        "\n",
        "        # Save to file\n",
        "        save_updated_profile(user_profile)\n",
        "\n",
        "    elif choice == '2':\n",
        "        # Edit wardrobe items\n",
        "        print(\"\\nOptions for wardrobe:\")\n",
        "        print(\"1. Add items\")\n",
        "        print(\"2. Remove items\")\n",
        "        print(\"3. Replace all items\")\n",
        "        print(\"4. Cancel\")\n",
        "\n",
        "        wardrobe_choice = input(\"Choose an option (1-4): \")\n",
        "\n",
        "        if wardrobe_choice == '1':\n",
        "            # Add items\n",
        "            new_items = input(\"Enter items to add (comma separated): \")\n",
        "            items_to_add = [item.strip() for item in new_items.split(',') if item.strip()]\n",
        "\n",
        "            if 'wardrobe' not in user_profile.user_data:\n",
        "                user_profile.user_data['wardrobe'] = []\n",
        "                user_profile.user_wardrobe = []\n",
        "\n",
        "            user_profile.user_wardrobe.extend(items_to_add)\n",
        "            user_profile.user_data['wardrobe'] = user_profile.user_wardrobe\n",
        "\n",
        "            save_updated_profile(user_profile)\n",
        "\n",
        "        elif wardrobe_choice == '2':\n",
        "            # Remove items\n",
        "            if not user_profile.user_wardrobe:\n",
        "                print(\"No wardrobe items to remove.\")\n",
        "                return\n",
        "\n",
        "            print(\"Current items:\")\n",
        "            for i, item in enumerate(user_profile.user_wardrobe, 1):\n",
        "                print(f\"  {i}. {item}\")\n",
        "\n",
        "            to_remove = input(\"Enter numbers of items to remove (comma separated): \")\n",
        "            try:\n",
        "                indices = [int(idx.strip()) for idx in to_remove.split(',') if idx.strip()]\n",
        "                # Sort in reverse to avoid index shifting during removal\n",
        "                indices.sort(reverse=True)\n",
        "\n",
        "                for idx in indices:\n",
        "                    if 1 <= idx <= len(user_profile.user_wardrobe):\n",
        "                        del user_profile.user_wardrobe[idx-1]\n",
        "\n",
        "                user_profile.user_data['wardrobe'] = user_profile.user_wardrobe\n",
        "                save_updated_profile(user_profile)\n",
        "\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please enter numbers separated by commas.\")\n",
        "\n",
        "        elif wardrobe_choice == '3':\n",
        "            # Replace all\n",
        "            new_wardrobe = input(\"Enter new wardrobe items (comma separated): \")\n",
        "            new_items = [item.strip() for item in new_wardrobe.split(',') if item.strip()]\n",
        "\n",
        "            user_profile.user_wardrobe = new_items\n",
        "            user_profile.user_data['wardrobe'] = new_items\n",
        "\n",
        "            save_updated_profile(user_profile)\n",
        "\n",
        "def save_updated_profile(user_profile):\n",
        "    \"\"\"Save updated profile to user.txt file\"\"\"\n",
        "    try:\n",
        "        # Format profile data for saving\n",
        "        lines = []\n",
        "\n",
        "        # Add basic profile fields first\n",
        "        for key, value in user_profile.user_data.items():\n",
        "            if key != 'wardrobe':  # Handle wardrobe separately\n",
        "                lines.append(f\"{key}: {value}\")\n",
        "\n",
        "        # Add wardrobe at the end\n",
        "        if user_profile.user_wardrobe:\n",
        "            lines.append(f\"wardrobe: {', '.join(user_profile.user_wardrobe)}\")\n",
        "\n",
        "        # Write to file\n",
        "        with open(user_profile.user_text_path, 'w') as f:\n",
        "            f.write('\\n'.join(lines))\n",
        "\n",
        "        print(f\"Profile updated and saved to {user_profile.user_text_path}\")\n",
        "        # Reload the profile to ensure everything is up to date\n",
        "        user_profile.load_user_data()\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving profile: {e}\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    # Unzip the model file\n",
        "    print(\"Unzipping model file...\")\n",
        "    extracted_model_path = extract_with_progress(zip_filepath, unzip_destination)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Initialize user profile manager\n",
        "    user_profile = UserProfileManager(user_text_path)\n",
        "\n",
        "    # Check if user profile needs to be created or updated\n",
        "    if not user_profile.user_data:\n",
        "        print(\"No user profile found. Would you like to create one? (y/n)\")\n",
        "        if input().lower() == 'y':\n",
        "            update_user_profile(user_profile)\n",
        "    else:\n",
        "        print(\"User profile loaded. Would you like to update it? (y/n)\")\n",
        "        if input().lower() == 'y':\n",
        "            update_user_profile(user_profile)\n",
        "\n",
        "    # Load the fine-tuned model\n",
        "    model = load_finetuned_model(extracted_model_path, device)\n",
        "\n",
        "    # Main menu\n",
        "    while True:\n",
        "        print(\"\\n===== Fashion Recommendation System =====\")\n",
        "        print(\"1. Interactive mode\")\n",
        "        print(\"2. Process input from file\")\n",
        "        print(\"3. Update user profile\")\n",
        "        print(\"4. Exit\")\n",
        "\n",
        "        choice = input(\"Choose an option (1-4): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            test_model_interactive(model, tokenizer, device, user_profile)\n",
        "        elif choice == '2':\n",
        "            process_user_ninput_file(model, tokenizer, device, user_profile)\n",
        "        elif choice == '3':\n",
        "            update_user_profile(user_profile)\n",
        "        else:\n",
        "            print(\"Exiting. Thank you!\")\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UhHWmxcn9x8s",
        "outputId": "06c9a37e-2104-45d9-9df9-4b767b0b89ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping model file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting files: 100%|██████████| 1/1 [01:39<00:00, 99.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete to /content/extracted_model/\n",
            "Using device: cpu\n",
            "Loaded user profile from /content/drive/MyDrive/fashion_files/user.txt\n",
            "User profile loaded. Would you like to update it? (y/n)\n",
            "n\n",
            "Loading model from /content/extracted_model/best_model.pt\n",
            "Model loaded successfully! Validation loss: 1.2289036760727565\n",
            "\n",
            "===== Fashion Recommendation System =====\n",
            "1. Interactive mode\n",
            "2. Process input from file\n",
            "3. Update user profile\n",
            "4. Exit\n",
            "Choose an option (1-4): 3\n",
            "\n",
            "===== Update User Profile =====\n",
            "Current profile:\n",
            "  name: sriram\n",
            "  age: 21\n",
            "  gender: male\n",
            "  location: indi\n",
            "  style: minimalist\n",
            "  occupation: student\n",
            "  preferences: minimalistic, stylish, classy\n",
            "  favorite_colors: navy, beige, white, black\n",
            "  size: m\n",
            "\n",
            "Options:\n",
            "1. Edit basic profile information\n",
            "2. Edit wardrobe items\n",
            "3. Cancel\n",
            "Choose an option (1-3): 2\n",
            "\n",
            "Options for wardrobe:\n",
            "1. Add items\n",
            "2. Remove items\n",
            "3. Replace all items\n",
            "4. Cancel\n",
            "Choose an option (1-4): 1\n",
            "Enter items to add (comma separated): white button-up shirt, navy trousers, black coat, beige cashmere sweater, white sneakers, black ankle boots, denim jeans, white t-shirts,pink polo tshirt, little black dress, brown watch, cotton black tshirt, brown trousers\n",
            "Profile updated and saved to /content/drive/MyDrive/fashion_files/user.txt\n",
            "Loaded user profile from /content/drive/MyDrive/fashion_files/user.txt\n",
            "\n",
            "===== Fashion Recommendation System =====\n",
            "1. Interactive mode\n",
            "2. Process input from file\n",
            "3. Update user profile\n",
            "4. Exit\n",
            "Choose an option (1-4): 2\n",
            "Loaded configuration from /content/drive/MyDrive/fashion_files/generation_config.json\n",
            "Loaded user text from /content/drive/MyDrive/fashion_files/user.txt (415 characters)\n",
            "\n",
            "Would you like to adjust generation parameters? (y/n)\n",
            "y\n",
            "\n",
            "===== Generation Configuration =====\n",
            "Current settings:\n",
            "  max_new_tokens: 300\n",
            "  temperature: 0.7\n",
            "  top_k: 40\n",
            "  instruction: name: sriram\n",
            "age: 21\n",
            "gender: male\n",
            "location: indi\n",
            "style: minimalist\n",
            "occupation: student\n",
            "preferences: minimalistic, stylish, classy\n",
            "favorite_colors: navy, beige, white, black\n",
            "size: m\n",
            "wardrobe: white button-up shirt, navy trousers, black coat, beige cashmere sweater, white sneakers, black ankle boots, denim jeans, white t-shirts, pink polo tshirt, little black dress, brown watch, cotton black tshirt, brown trousers\n",
            "  input_text: \n",
            "  consider_wardrobe: no\n",
            "  consider_user_details: no\n",
            "\n",
            "Enter new values (press Enter to keep current value):\n",
            "max_new_tokens [300]: 200\n",
            "temperature [0.7]: 0.7\n",
            "top_k [40]: 40\n",
            "consider_wardrobe [no] (yes/no): yes\n",
            "consider_user_details [no] (yes/no): yes\n",
            "Configuration saved to /content/drive/MyDrive/fashion_files/generation_config.json\n",
            "\n",
            "Generating response...\n",
            "Generation parameters: max_tokens=200, temp=0.7, top_k=40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating tokens: 100%|██████████| 200/200 [12:31<00:00,  3.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generated Response ===\n",
            "\n",
            "Okay, let's get you looking sharp and feeling comfortable while maintaining a minimalist aesthetic.  While your current wardrobe items are a bit too casual, I can definitely help you put together a sharp and minimalist look.\n",
            "\n",
            "Since you like navy and beige, we can definitely work with that.  White is a fantastic base.  You can incorporate that with a navy blazer, which is perfect for a casual, summery vibe.  You can also roll up the sleeves for a relaxed touch if the weather's cool.  Make sure the blazer isn’t too long, ensuring it’s proportionally flattering.  Since you’re a minimalist man, leaving the top button undone will create a clean, streamlined silhouette, especially with the t-shirt.\n",
            "\n",
            "For the shoes, black leather loafers or brogues would look sharp.  Avoid overly chunky sneakers.   Since you like polos, think classic white or black chinos.\n",
            "\n",
            "=== Token Stats ===\n",
            "Generated 200 tokens\n",
            "Input tokens: 291\n",
            "Total tokens: 491\n",
            "Personalization: Wardrobe = yes, User Details = yes\n",
            "Response saved to /content/drive/MyDrive/fashion_files/response.txt\n",
            "\n",
            "Options:\n",
            "1. Process the same text with different parameters\n",
            "2. Edit the user.txt file\n",
            "3. Switch to interactive mode\n",
            "4. Exit\n",
            "Choose an option (1-4): 3\n",
            "\n",
            "===== Interactive Testing Mode =====\n",
            "Enter your queries to test the model. Type 'exit' to quit.\n",
            "Format: instruction | input (optional)\n",
            "\n",
            "Suggested fashion-related queries:\n",
            "1. Describe this outfit: | A black leather jacket with white t-shirt and blue jeans\n",
            "2. Suggest accessories for | a summer wedding guest outfit with a floral dress\n",
            "3. Compare these styles: | minimalist vs maximalist fashion\n",
            "4. Create a shopping list for | building a capsule wardrobe for fall\n",
            "5. What are current trends in | sustainable fashion for 2024?\n",
            "Loaded configuration from /content/drive/MyDrive/fashion_files/generation_config.json\n",
            "\n",
            "Current personalization: Consider Wardrobe = yes, Consider User Details = yes\n",
            "\n",
            "Would you like to adjust generation parameters? (y/n)\n",
            "y\n",
            "\n",
            "===== Generation Configuration =====\n",
            "Current settings:\n",
            "  max_new_tokens: 200\n",
            "  temperature: 0.7\n",
            "  top_k: 40\n",
            "  instruction: name: sriram\n",
            "age: 21\n",
            "gender: male\n",
            "location: indi\n",
            "style: minimalist\n",
            "occupation: student\n",
            "preferences: minimalistic, stylish, classy\n",
            "favorite_colors: navy, beige, white, black\n",
            "size: m\n",
            "wardrobe: white button-up shirt, navy trousers, black coat, beige cashmere sweater, white sneakers, black ankle boots, denim jeans, white t-shirts, pink polo tshirt, little black dress, brown watch, cotton black tshirt, brown trousers\n",
            "  input_text: \n",
            "  consider_wardrobe: yes\n",
            "  consider_user_details: yes\n",
            "\n",
            "Enter new values (press Enter to keep current value):\n",
            "max_new_tokens [200]: \n",
            "temperature [0.7]: \n",
            "top_k [40]: \n",
            "consider_wardrobe [yes] (yes/no): \n",
            "consider_user_details [yes] (yes/no): \n",
            "Configuration saved to /content/drive/MyDrive/fashion_files/generation_config.json\n",
            "\n",
            "Your query: suggest some oufit for dinner outing\n",
            "\n",
            "Generating response...\n",
            "Generation parameters: max_tokens=200, temp=0.7, top_k=40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating tokens: 100%|██████████| 200/200 [09:15<00:00,  2.78s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Generated Response ===\n",
            "\n",
            "Okay, so, here's an evening outing where you want to make a statement dressed down.  Given your minimalist aesthetic, minimalist color palette, and preferences, I've got some ideas.  Since you like navy and beige, and you're a student, we can definitely work with that.\n",
            "\n",
            "For dinner, I recommend starting with the simple white button-up shirt.  It's the foundation.  Underneath, go for a navy slim-fit chino pant or a navy blazer.  This provides a streamlined, polished look.  Choose a slim fit, not too baggy, which will flatter your build.  Ensure the trousers are well-fitting, not too tight, for a relaxed vibe.  For footwear, black or dark brown leather loafers or brogues would work well.  Again, black or dark brown is versatile, and loafers elevate the look.  Finish with a simple white polo t-shirt and black denim jeans\n",
            "\n",
            "=== Token Stats ===\n",
            "Generated 200 tokens\n",
            "Personalization: Wardrobe = yes, User Details = yes\n",
            "\n",
            "Current personalization: Consider Wardrobe = yes, Consider User Details = yes\n",
            "\n",
            "Would you like to adjust generation parameters? (y/n)\n",
            "n\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9524a975bfde>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-9524a975bfde>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0mtest_model_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_profile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0mprocess_user_input_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_profile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'3'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0mupdate_user_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_profile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9524a975bfde>\u001b[0m in \u001b[0;36mprocess_user_input_file\u001b[0;34m(model, tokenizer, device, user_profile)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0msave_user_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'3'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtest_model_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_profile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9524a975bfde>\u001b[0m in \u001b[0;36mtest_model_interactive\u001b[0;34m(model, tokenizer, device, user_profile)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_config_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYour query: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference based on user details"
      ],
      "metadata": {
        "id": "_XfWfVJmDiqF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vYalJkjlDmq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GETwwFHADnMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VLM PART"
      ],
      "metadata": {
        "id": "yxp7Y44ZZLNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Replace 'your_file.zip' with the name of the uploaded zip file\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/paligemma_model.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/Files')"
      ],
      "metadata": {
        "id": "StjHprbRZNFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sympy==1.12.0 fire"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY9BOdOeZRD-",
        "outputId": "6bbb31e2-ffda-497d-f19e-38f551c10223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sympy==1.12.0\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting fire\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy==1.12.0) (1.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire) (3.0.1)\n",
            "Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=1abcecc59248637a17979b13a8b2bbe0e6631b8e292eec16bc49840060560d2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: sympy, fire\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fire-0.7.0 sympy-1.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === modeling_siglip.py ===\n",
        "from typing import Optional, Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SiglipVisionConfig:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_size=768,\n",
        "        intermediate_size=3072,\n",
        "        num_hidden_layers=12,\n",
        "        num_attention_heads=12,\n",
        "        num_channels=3,\n",
        "        image_size=224,\n",
        "        patch_size=16,\n",
        "        layer_norm_eps=1e-6,\n",
        "        attention_dropout=0.0,\n",
        "        num_image_tokens: int = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.num_channels = num_channels\n",
        "        self.patch_size = patch_size\n",
        "        self.image_size = image_size\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "        self.num_image_tokens = num_image_tokens\n",
        "\n",
        "\n",
        "class SiglipVisionEmbeddings(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embed_dim = config.hidden_size\n",
        "        self.image_size = config.image_size\n",
        "        self.patch_size = config.patch_size\n",
        "\n",
        "        self.patch_embedding = nn.Conv2d(\n",
        "            in_channels=config.num_channels,\n",
        "            out_channels=self.embed_dim,\n",
        "            kernel_size=self.patch_size,\n",
        "            stride=self.patch_size,\n",
        "            padding=\"valid\", # This indicates no padding is added\n",
        "        )\n",
        "\n",
        "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
        "        self.num_positions = self.num_patches\n",
        "        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
        "        self.register_buffer(\n",
        "            \"position_ids\",\n",
        "            torch.arange(self.num_positions).expand((1, -1)),\n",
        "            persistent=False,\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
        "        _, _, height, width = pixel_values.shape # [Batch_Size, Channels, Height, Width]\n",
        "        # Convolve the `patch_size` kernel over the image, with no overlapping patches since the stride is equal to the kernel size\n",
        "        # The output of the convolution will have shape [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W]\n",
        "        # where Num_Patches_H = height // patch_size and Num_Patches_W = width // patch_size\n",
        "        patch_embeds = self.patch_embedding(pixel_values)\n",
        "        # [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W] -> [Batch_Size, Embed_Dim, Num_Patches]\n",
        "        # where Num_Patches = Num_Patches_H * Num_Patches_W\n",
        "        embeddings = patch_embeds.flatten(2)\n",
        "        # [Batch_Size, Embed_Dim, Num_Patches] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        embeddings = embeddings.transpose(1, 2)\n",
        "        # Add position embeddings to each patch. Each positional encoding is a vector of size [Embed_Dim]\n",
        "        embeddings = embeddings + self.position_embedding(self.position_ids)\n",
        "        # [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class SiglipAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embed_dim = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.embed_dim // self.num_heads\n",
        "        self.scale = self.head_dim**-0.5 # Equivalent to 1 / sqrt(self.head_dim)\n",
        "        self.dropout = config.attention_dropout\n",
        "\n",
        "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "\n",
        "        # hidden_states: [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        batch_size, seq_len, _ = hidden_states.size()\n",
        "        # query_states: [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        query_states = self.q_proj(hidden_states)\n",
        "        # key_states: [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        key_states = self.k_proj(hidden_states)\n",
        "        # value_states: [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        value_states = self.v_proj(hidden_states)\n",
        "        # query_states: [Batch_Size, Num_Heads, Num_Patches, Head_Dim]\n",
        "        query_states = query_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        key_states = key_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        value_states = value_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        # Calculate the attention using the formula Q * K^T / sqrt(d_k). attn_weights: [Batch_Size, Num_Heads, Num_Patches, Num_Patches]\n",
        "        attn_weights = (torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale)\n",
        "\n",
        "        if attn_weights.size() != (batch_size, self.num_heads, seq_len, seq_len):\n",
        "            raise ValueError(\n",
        "                f\"Attention weights should be of size {(batch_size, self.num_heads, seq_len, seq_len)}, but is\"\n",
        "                f\" {attn_weights.size()}\"\n",
        "            )\n",
        "\n",
        "        # Apply the softmax row-wise. attn_weights: [Batch_Size, Num_Heads, Num_Patches, Num_Patches]\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "        # Apply dropout only during training\n",
        "        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "        # Multiply the attention weights by the value states. attn_output: [Batch_Size, Num_Heads, Num_Patches, Head_Dim]\n",
        "        attn_output = torch.matmul(attn_weights, value_states)\n",
        "\n",
        "        if attn_output.size() != (batch_size, self.num_heads, seq_len, self.head_dim):\n",
        "            raise ValueError(\n",
        "                f\"`attn_output` should be of size {(batch_size, self.num_heads, seq_len, self.head_dim)}, but is\"\n",
        "                f\" {attn_output.size()}\"\n",
        "            )\n",
        "        # [Batch_Size, Num_Heads, Num_Patches, Head_Dim] -> [Batch_Size, Num_Patches, Num_Heads, Head_Dim]\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        # [Batch_Size, Num_Patches, Num_Heads, Head_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)\n",
        "        # [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "\n",
        "class SiglipMLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Intermediate_Size]\n",
        "        hidden_states = self.fc1(hidden_states)\n",
        "        # hidden_states: [Batch_Size, Num_Patches, Intermediate_Size]\n",
        "        hidden_states = nn.functional.gelu(hidden_states, approximate=\"tanh\")\n",
        "        # [Batch_Size, Num_Patches, Intermediate_Size] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class SiglipEncoderLayer(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.hidden_size\n",
        "        self.self_attn = SiglipAttention(config)\n",
        "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
        "        self.mlp = SiglipMLP(config)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
        "\n",
        "    # Ignore copy\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        # residual: [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        residual = hidden_states\n",
        "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        hidden_states = self.layer_norm1(hidden_states)\n",
        "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        hidden_states, _ = self.self_attn(hidden_states=hidden_states)\n",
        "        # [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        hidden_states = residual + hidden_states\n",
        "        # residual: [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        residual = hidden_states\n",
        "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        hidden_states = self.layer_norm2(hidden_states)\n",
        "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        hidden_states = self.mlp(hidden_states)\n",
        "        # [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class SiglipEncoder(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layers = nn.ModuleList(\n",
        "            [SiglipEncoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
        "        )\n",
        "\n",
        "    # Ignore copy\n",
        "    def forward(\n",
        "        self,\n",
        "        inputs_embeds: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        # inputs_embeds: [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        hidden_states = inputs_embeds\n",
        "\n",
        "        for encoder_layer in self.layers:\n",
        "            # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
        "            hidden_states = encoder_layer(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class SiglipVisionTransformer(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        embed_dim = config.hidden_size\n",
        "\n",
        "        self.embeddings = SiglipVisionEmbeddings(config)\n",
        "        self.encoder = SiglipEncoder(config)\n",
        "        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "        # pixel_values: [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        hidden_states = self.embeddings(pixel_values)\n",
        "\n",
        "        last_hidden_state = self.encoder(inputs_embeds=hidden_states)\n",
        "\n",
        "        last_hidden_state = self.post_layernorm(last_hidden_state)\n",
        "\n",
        "        return last_hidden_state\n",
        "\n",
        "\n",
        "class SiglipVisionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.vision_model = SiglipVisionTransformer(config)\n",
        "\n",
        "    def forward(self, pixel_values) -> Tuple:\n",
        "        # [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        return self.vision_model(pixel_values=pixel_values)\n",
        "\n",
        "\n",
        "# === modeling_gemma.py ===\n",
        "import torch\n",
        "from torch import nn\n",
        "from typing import Optional, Tuple, List\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import math\n",
        "\n",
        "\n",
        "class KVCache():\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.key_cache: List[torch.Tensor] = []\n",
        "        self.value_cache: List[torch.Tensor] = []\n",
        "\n",
        "    def num_items(self) -> int:\n",
        "        if len(self.key_cache) == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            # The shape of the key_cache is [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
        "            return self.key_cache[0].shape[-2]\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        key_states: torch.Tensor,\n",
        "        value_states: torch.Tensor,\n",
        "        layer_idx: int,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        if len(self.key_cache) <= layer_idx:\n",
        "            # If we never added anything to the KV-Cache of this layer, let's create it.\n",
        "            self.key_cache.append(key_states)\n",
        "            self.value_cache.append(value_states)\n",
        "        else:\n",
        "            # ... otherwise we concatenate the new keys with the existing ones.\n",
        "            # each tensor has shape: [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
        "            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
        "            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
        "\n",
        "        # ... and then we return all the existing keys + the new ones.\n",
        "        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n",
        "\n",
        "class GemmaConfig():\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        hidden_size,\n",
        "        intermediate_size,\n",
        "        num_hidden_layers,\n",
        "        num_attention_heads,\n",
        "        num_key_value_heads,\n",
        "        head_dim=256,\n",
        "        max_position_embeddings=8192,\n",
        "        rms_norm_eps=1e-6,\n",
        "        rope_theta=10000.0,\n",
        "        attention_bias=False,\n",
        "        attention_dropout=0.0,\n",
        "        pad_token_id=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.hidden_size = hidden_size\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.num_key_value_heads = num_key_value_heads\n",
        "        self.rms_norm_eps = rms_norm_eps\n",
        "        self.rope_theta = rope_theta\n",
        "        self.attention_bias = attention_bias\n",
        "        self.attention_dropout = attention_dropout\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "class PaliGemmaConfig():\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vision_config=None,\n",
        "        text_config=None,\n",
        "        ignore_index=-100,\n",
        "        image_token_index=256000,\n",
        "        vocab_size=257152,\n",
        "        projection_dim=2048,\n",
        "        hidden_size=2048,\n",
        "        pad_token_id=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.image_token_index = image_token_index\n",
        "        self.vocab_size = vocab_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vision_config = vision_config\n",
        "        self.is_encoder_decoder = False\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "        self.vision_config = SiglipVisionConfig(**vision_config)\n",
        "        self.text_config = text_config\n",
        "\n",
        "        self.text_config = GemmaConfig(**text_config, pad_token_id=pad_token_id)\n",
        "        self.vocab_size = self.text_config.vocab_size\n",
        "\n",
        "        self.text_config.num_image_tokens = (self.vision_config.image_size // self.vision_config.patch_size) ** 2\n",
        "        self.vision_config.projection_dim = projection_dim\n",
        "\n",
        "\n",
        "class GemmaRMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.zeros(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float())\n",
        "        # Llama does x.to(float16) * w whilst Gemma is (x * w).to(float16)\n",
        "        # See https://github.com/huggingface/transformers/pull/29402\n",
        "        output = output * (1.0 + self.weight.float())\n",
        "        return output.type_as(x)\n",
        "\n",
        "class GemmaRotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim # it is set to the head_dim\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.base = base\n",
        "\n",
        "        # Calculate the theta according to the formula theta_i = base^(-2i/dim) where i = 0, 1, 2, ..., dim // 2\n",
        "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n",
        "        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x, position_ids, seq_len=None):\n",
        "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
        "        self.inv_freq.to(x.device)\n",
        "        # Copy the inv_freq tensor for batch in the sequence\n",
        "        # inv_freq_expanded: [Batch_Size, Head_Dim // 2, 1]\n",
        "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
        "        # position_ids_expanded: [Batch_Size, 1, Seq_Len]\n",
        "        position_ids_expanded = position_ids[:, None, :].float()\n",
        "        device_type = x.device.type\n",
        "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
        "        with torch.autocast(device_type=device_type, enabled=False):\n",
        "            # Multiply each theta by the position (which is the argument of the sin and cos functions)\n",
        "            # freqs: [Batch_Size, Head_Dim // 2, 1] @ [Batch_Size, 1, Seq_Len] --> [Batch_Size, Seq_Len, Head_Dim // 2]\n",
        "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
        "            # emb: [Batch_Size, Seq_Len, Head_Dim]\n",
        "            emb = torch.cat((freqs, freqs), dim=-1)\n",
        "            # cos, sin: [Batch_Size, Seq_Len, Head_Dim]\n",
        "            cos = emb.cos()\n",
        "            sin = emb.sin()\n",
        "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
        "\n",
        "\n",
        "def rotate_half(x):\n",
        "    # Build the [-x2, x1, -x4, x3, ...] tensor for the sin part of the positional encoding.\n",
        "    x1 = x[..., : x.shape[-1] // 2] # Takes the first half of the last dimension\n",
        "    x2 = x[..., x.shape[-1] // 2 :] # Takes the second half of the last dimension\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
        "    cos = cos.unsqueeze(unsqueeze_dim) # Add the head dimension\n",
        "    sin = sin.unsqueeze(unsqueeze_dim) # Add the head dimension\n",
        "    # Apply the formula (34) of the Rotary Positional Encoding paper.\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "\n",
        "class GemmaMLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Equivalent to:\n",
        "        # y = self.gate_proj(x) # [Batch_Size, Seq_Len, Hidden_Size] -> [Batch_Size, Seq_Len, Intermediate_Size]\n",
        "        # y = torch.gelu(y, approximate=\"tanh\") # [Batch_Size, Seq_Len, Intermediate_Size]\n",
        "        # j = self.up_proj(x) # [Batch_Size, Seq_Len, Hidden_Size] -> [Batch_Size, Seq_Len, Intermediate_Size]\n",
        "        # z = y * j # [Batch_Size, Seq_Len, Intermediate_Size]\n",
        "        # z = self.down_proj(z) # [Batch_Size, Seq_Len, Intermediate_Size] -> [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        return self.down_proj(nn.functional.gelu(self.gate_proj(x), approximate=\"tanh\") * self.up_proj(x))\n",
        "\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "\n",
        "class GemmaAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config: GemmaConfig, layer_idx: Optional[int] = None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        self.attention_dropout = config.attention_dropout\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.num_key_value_heads = config.num_key_value_heads\n",
        "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
        "        self.max_position_embeddings = config.max_position_embeddings\n",
        "        self.rope_theta = config.rope_theta\n",
        "        self.is_causal = True\n",
        "\n",
        "        assert self.hidden_size % self.num_heads == 0\n",
        "\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
        "        self.rotary_emb = GemmaRotaryEmbedding(\n",
        "            self.head_dim,\n",
        "            max_position_embeddings=self.max_position_embeddings,\n",
        "            base=self.rope_theta,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        kv_cache: Optional[KVCache] = None,\n",
        "        **kwargs,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        bsz, q_len, _ = hidden_states.size() # [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        # [Batch_Size, Seq_Len, Num_Heads_Q * Head_Dim]\n",
        "        query_states = self.q_proj(hidden_states)\n",
        "        # [Batch_Size, Seq_Len, Num_Heads_KV * Head_Dim]\n",
        "        key_states = self.k_proj(hidden_states)\n",
        "        # [Batch_Size, Seq_Len, Num_Heads_KV * Head_Dim]\n",
        "        value_states = self.v_proj(hidden_states)\n",
        "        # [Batch_Size, Num_Heads_Q, Seq_Len, Head_Dim]\n",
        "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        # [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
        "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "        # [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
        "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # [Batch_Size, Seq_Len, Head_Dim], [Batch_Size, Seq_Len, Head_Dim]\n",
        "        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=None)\n",
        "        # [Batch_Size, Num_Heads_Q, Seq_Len, Head_Dim], [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "        if kv_cache is not None:\n",
        "            key_states, value_states = kv_cache.update(key_states, value_states, self.layer_idx)\n",
        "\n",
        "        # Repeat the key and values to match the number of heads of the query\n",
        "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "        # Perform the calculation as usual, Q * K^T / sqrt(head_dim). Shape: [Batch_Size, Num_Heads_Q, Seq_Len_Q, Seq_Len_KV]\n",
        "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        assert attention_mask is not None\n",
        "        attn_weights = attn_weights + attention_mask\n",
        "\n",
        "        # Apply the softmax\n",
        "        # [Batch_Size, Num_Heads_Q, Seq_Len_Q, Seq_Len_KV]\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "        # Apply the dropout\n",
        "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
        "        # Multiply by the values. [Batch_Size, Num_Heads_Q, Seq_Len_Q, Seq_Len_KV] x [Batch_Size, Num_Heads_KV, Seq_Len_KV, Head_Dim] -> [Batch_Size, Num_Heads_Q, Seq_Len_Q, Head_Dim]\n",
        "        attn_output = torch.matmul(attn_weights, value_states)\n",
        "\n",
        "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
        "            raise ValueError(\n",
        "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
        "                f\" {attn_output.size()}\"\n",
        "            )\n",
        "        # Make sure the sequence length is the second dimension. # [Batch_Size, Num_Heads_Q, Seq_Len_Q, Head_Dim] -> [Batch_Size, Seq_Len_Q, Num_Heads_Q, Head_Dim]\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        # Concatenate all the heads together. [Batch_Size, Seq_Len_Q, Num_Heads_Q, Head_Dim] -> [Batch_Size, Seq_Len_Q, Num_Heads_Q * Head_Dim]\n",
        "        attn_output = attn_output.view(bsz, q_len, -1)\n",
        "        # Multiply by W_o. [Batch_Size, Seq_Len_Q, Hidden_Size]\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "class GemmaDecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, config: GemmaConfig, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "\n",
        "        self.self_attn = GemmaAttention(config=config, layer_idx=layer_idx)\n",
        "\n",
        "        self.mlp = GemmaMLP(config)\n",
        "        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        kv_cache: Optional[KVCache] = None,\n",
        "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "        residual = hidden_states\n",
        "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        hidden_states = self.input_layernorm(hidden_states)\n",
        "\n",
        "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        hidden_states, _, = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            kv_cache=kv_cache,\n",
        "        )\n",
        "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        residual = hidden_states\n",
        "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
        "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        hidden_states = self.mlp(hidden_states)\n",
        "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        hidden_states = residual + hidden_states\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "class GemmaModel(nn.Module):\n",
        "\n",
        "    def __init__(self, config: GemmaConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
        "        self.layers = nn.ModuleList(\n",
        "            [GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
        "        )\n",
        "        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    # Ignore copy\n",
        "    def forward(\n",
        "        self,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        kv_cache: Optional[KVCache] = None,\n",
        "    ) -> torch.FloatTensor:\n",
        "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        hidden_states = inputs_embeds\n",
        "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n",
        "        hidden_states = hidden_states * normalizer\n",
        "\n",
        "        for decoder_layer in self.layers:\n",
        "            # [Batch_Size, Seq_Len, Hidden_Size]\n",
        "            hidden_states = decoder_layer(\n",
        "                hidden_states,\n",
        "                attention_mask=attention_mask,\n",
        "                position_ids=position_ids,\n",
        "                kv_cache=kv_cache,\n",
        "            )\n",
        "\n",
        "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        hidden_states = self.norm(hidden_states)\n",
        "\n",
        "        # [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        return hidden_states\n",
        "\n",
        "class GemmaForCausalLM(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.model = GemmaModel(config)\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.embed_tokens\n",
        "\n",
        "    def tie_weights(self):\n",
        "        self.lm_head.weight = self.model.embed_tokens.weight\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        kv_cache: Optional[KVCache] = None,\n",
        "    ) -> Tuple:\n",
        "\n",
        "        # input_embeds: [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        # outputs: [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        outputs = self.model(\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            kv_cache=kv_cache,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs\n",
        "        logits = self.lm_head(hidden_states)\n",
        "        logits = logits.float()\n",
        "\n",
        "        return_data = {\n",
        "            \"logits\": logits,\n",
        "        }\n",
        "\n",
        "        if kv_cache is not None:\n",
        "            # Return the updated cache\n",
        "            return_data[\"kv_cache\"] = kv_cache\n",
        "\n",
        "        return return_data\n",
        "\n",
        "class PaliGemmaMultiModalProjector(nn.Module):\n",
        "    def __init__(self, config: PaliGemmaConfig):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(config.vision_config.hidden_size, config.vision_config.projection_dim, bias=True)\n",
        "\n",
        "    def forward(self, image_features):\n",
        "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Projection_Dim]\n",
        "        hidden_states = self.linear(image_features)\n",
        "        return hidden_states\n",
        "\n",
        "class PaliGemmaForConditionalGeneration(nn.Module):\n",
        "    def __init__(self, config: PaliGemmaConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.vision_tower = SiglipVisionModel(config.vision_config)\n",
        "        self.multi_modal_projector = PaliGemmaMultiModalProjector(config)\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "        language_model = GemmaForCausalLM(config.text_config)\n",
        "        self.language_model = language_model\n",
        "\n",
        "        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n",
        "\n",
        "    def tie_weights(self):\n",
        "        return self.language_model.tie_weights()\n",
        "\n",
        "    def _merge_input_ids_with_image_features(self, image_features: torch.Tensor, inputs_embeds: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor, kv_cache: Optional[KVCache] = None):\n",
        "        batch_size, num_patches, embed_dim = image_features.shape\n",
        "        batch_size_input, sequence_length = input_ids.shape\n",
        "        dtype, device = inputs_embeds.dtype, inputs_embeds.device\n",
        "\n",
        "        # Shape: [Batch_Size, Seq_Len, Hidden_Size]\n",
        "        scaled_image_features = image_features / (self.config.hidden_size**0.5)\n",
        "\n",
        "        # Combine the embeddings of the image tokens, the text tokens and mask out all the padding tokens.\n",
        "        final_embedding = torch.zeros(batch_size, sequence_length, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n",
        "\n",
        "        # Shape: [Batch_Size, Seq_Len]. True for text tokens\n",
        "        text_mask = (input_ids != self.config.image_token_index) & (input_ids != self.pad_token_id)\n",
        "\n",
        "        # Shape: [Batch_Size, Seq_Len]. True for image tokens\n",
        "        image_mask = input_ids == self.config.image_token_index\n",
        "\n",
        "        # Shape: [Batch_Size, Seq_Len]. True for padding tokens\n",
        "        pad_mask = input_ids == self.pad_token_id\n",
        "\n",
        "        # We need to expand the masks to the embedding dimension otherwise we can't use them in torch.where\n",
        "        text_mask_expanded = text_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
        "        pad_mask_expanded = pad_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
        "        image_mask_expanded = image_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
        "\n",
        "        # Add the text embeddings\n",
        "        final_embedding = torch.where(text_mask_expanded, inputs_embeds, final_embedding)\n",
        "\n",
        "        # FIX: Count the number of image tokens per batch item and ensure we have enough features\n",
        "        for batch_idx in range(batch_size):\n",
        "            image_token_positions = torch.where(image_mask[batch_idx])[0]\n",
        "            num_image_tokens = len(image_token_positions)\n",
        "\n",
        "            # Make sure we don't try to insert more features than we have\n",
        "            if num_image_tokens > 0:\n",
        "                # If we have more image tokens than features, we need to repeat the features\n",
        "                # or use a subset of the tokens depending on requirements\n",
        "                features_to_use = scaled_image_features[batch_idx, :num_image_tokens]\n",
        "\n",
        "                # Insert image embeddings for each image token position\n",
        "                for token_idx, pos in enumerate(image_token_positions):\n",
        "                    if token_idx < len(features_to_use):\n",
        "                        final_embedding[batch_idx, pos] = features_to_use[token_idx]\n",
        "\n",
        "        # Zero out padding tokens\n",
        "        final_embedding = torch.where(pad_mask_expanded, torch.zeros_like(final_embedding), final_embedding)\n",
        "\n",
        "        #### CREATE THE ATTENTION MASK ####\n",
        "\n",
        "        dtype, device = inputs_embeds.dtype, inputs_embeds.device\n",
        "        min_dtype = torch.finfo(dtype).min\n",
        "        q_len = inputs_embeds.shape[1]\n",
        "\n",
        "        if kv_cache is None or kv_cache.num_items() == 0:\n",
        "            # Do not mask any token, because we're in the prefill phase\n",
        "            # This only works when we have no padding\n",
        "            causal_mask = torch.full(\n",
        "                (batch_size, q_len, q_len), fill_value=0, dtype=dtype, device=device\n",
        "            )\n",
        "        else:\n",
        "            # Since we are generating tokens, the query must be one single token\n",
        "            assert q_len == 1\n",
        "            kv_len = kv_cache.num_items() + q_len\n",
        "            # Also in this case we don't need to mask anything, since each query should be able to attend all previous tokens.\n",
        "            # This only works when we have no padding\n",
        "            causal_mask = torch.full(\n",
        "                (batch_size, q_len, kv_len), fill_value=0, dtype=dtype, device=device\n",
        "            )\n",
        "\n",
        "        # Add the head dimension\n",
        "        # [Batch_Size, Q_Len, KV_Len] -> [Batch_Size, Num_Heads_Q, Q_Len, KV_Len]\n",
        "        causal_mask = causal_mask.unsqueeze(1)\n",
        "\n",
        "        if kv_cache is not None and kv_cache.num_items() > 0:\n",
        "            # The position of the query is just the last position\n",
        "            position_ids = attention_mask.cumsum(-1)[:, -1]\n",
        "            if position_ids.dim() == 1:\n",
        "                position_ids = position_ids.unsqueeze(0)\n",
        "        else:\n",
        "            # Create a position_ids based on the size of the attention_mask\n",
        "            # For masked tokens, use the number 1 as position.\n",
        "            position_ids = (attention_mask.cumsum(-1)).masked_fill_((attention_mask == 0), 1).to(device)\n",
        "\n",
        "        return final_embedding, causal_mask, position_ids\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        pixel_values: torch.FloatTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        kv_cache: Optional[KVCache] = None,\n",
        "    ) -> Tuple:\n",
        "\n",
        "        # Make sure the input is right-padded\n",
        "        #assert torch.all(attention_mask == 1), \"The input cannot be padded\"\n",
        "\n",
        "        # 1. Extra the input embeddings\n",
        "        # shape: (Batch_Size, Seq_Len, Hidden_Size)\n",
        "        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n",
        "\n",
        "        # 2. Merge text and images\n",
        "        # [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
        "        selected_image_feature = self.vision_tower(pixel_values.to(inputs_embeds.dtype))\n",
        "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Hidden_Size]\n",
        "        image_features = self.multi_modal_projector(selected_image_feature)\n",
        "\n",
        "        # Merge the embeddings of the text tokens and the image tokens\n",
        "        inputs_embeds, attention_mask, position_ids = self._merge_input_ids_with_image_features(image_features, inputs_embeds, input_ids, attention_mask, kv_cache)\n",
        "\n",
        "        outputs = self.language_model(\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            kv_cache=kv_cache,\n",
        "        )\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# === utils.py ===\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "import glob\n",
        "from safetensors import safe_open\n",
        "from typing import Tuple\n",
        "import os\n",
        "\n",
        "def load_hf_model(model_path: str, device: str) -> Tuple[PaliGemmaForConditionalGeneration, AutoTokenizer]:\n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"right\")\n",
        "    assert tokenizer.padding_side == \"right\"\n",
        "\n",
        "    # Find all the *.safetensors files\n",
        "    safetensors_files = glob.glob(os.path.join(model_path, \"*.safetensors\"))\n",
        "\n",
        "    # ... and load them one by one in the tensors dictionary\n",
        "    tensors = {}\n",
        "    for safetensors_file in safetensors_files:\n",
        "        with safe_open(safetensors_file, framework=\"pt\", device=\"cpu\") as f:\n",
        "            for key in f.keys():\n",
        "                tensors[key] = f.get_tensor(key)\n",
        "\n",
        "    # Load the model's config\n",
        "    with open(os.path.join(model_path, \"config.json\"), \"r\") as f:\n",
        "        model_config_file = json.load(f)\n",
        "        config = PaliGemmaConfig(**model_config_file)\n",
        "\n",
        "    # Create the model using the configuration\n",
        "    model = PaliGemmaForConditionalGeneration(config).to(device)\n",
        "\n",
        "    # Load the state dict of the model\n",
        "    model.load_state_dict(tensors, strict=False)\n",
        "\n",
        "    # Tie weights\n",
        "    model.tie_weights()\n",
        "\n",
        "    return (model, tokenizer)\n",
        "\n",
        "# === processing_paligemma.py ===\n",
        "from typing import Dict, List, Optional, Union, Tuple, Iterable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "IMAGENET_STANDARD_MEAN = [0.5, 0.5, 0.5]\n",
        "IMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]\n",
        "\n",
        "\n",
        "def add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token):\n",
        "    # Quoting from the blog (https://huggingface.co/blog/paligemma#detailed-inference-process):\n",
        "    #   The input text is tokenized normally.\n",
        "    #   A <bos> token is added at the beginning, and an additional newline token (\\n) is appended.\n",
        "    #   This newline token is an essential part of the input prompt the model was trained with, so adding it explicitly ensures it's always there.\n",
        "    #   The tokenized text is also prefixed with a fixed number of <image> tokens.\n",
        "    # NOTE: from the paper it looks like the `\\n` should be tokenized separately, but in the HF implementation this is not done.\n",
        "    #       ref to HF implementation: https://github.com/huggingface/transformers/blob/7f79a97399bb52aad8460e1da2f36577d5dccfed/src/transformers/models/paligemma/processing_paligemma.py#L55-L73\n",
        "    return f\"{image_token * image_seq_len}{bos_token}{prefix_prompt}\\n\"\n",
        "\n",
        "\n",
        "def rescale(\n",
        "    image: np.ndarray, scale: float, dtype: np.dtype = np.float32\n",
        ") -> np.ndarray:\n",
        "    rescaled_image = image * scale\n",
        "    rescaled_image = rescaled_image.astype(dtype)\n",
        "    return rescaled_image\n",
        "\n",
        "\n",
        "def resize(\n",
        "    image: Image,\n",
        "    size: Tuple[int, int],\n",
        "    resample: Image.Resampling = None,\n",
        "    reducing_gap: Optional[int] = None,\n",
        ") -> np.ndarray:\n",
        "    height, width = size\n",
        "    resized_image = image.resize(\n",
        "        (width, height), resample=resample, reducing_gap=reducing_gap\n",
        "    )\n",
        "    return resized_image\n",
        "\n",
        "\n",
        "def normalize(\n",
        "    image: np.ndarray,\n",
        "    mean: Union[float, Iterable[float]],\n",
        "    std: Union[float, Iterable[float]],\n",
        ") -> np.ndarray:\n",
        "    mean = np.array(mean, dtype=image.dtype)\n",
        "    std = np.array(std, dtype=image.dtype)\n",
        "    image = (image - mean) / std\n",
        "    return image\n",
        "\n",
        "\n",
        "def process_images(\n",
        "    images: List[Image.Image],\n",
        "    size: Dict[str, int] = None,\n",
        "    resample: Image.Resampling = None,\n",
        "    rescale_factor: float = None,\n",
        "    image_mean: Optional[Union[float, List[float]]] = None,\n",
        "    image_std: Optional[Union[float, List[float]]] = None,\n",
        ") -> List[np.ndarray]:\n",
        "    height, width = size[0], size[1]\n",
        "    images = [\n",
        "        resize(image=image, size=(height, width), resample=resample) for image in images\n",
        "    ]\n",
        "    # Convert each image to a numpy array\n",
        "    images = [np.array(image) for image in images]\n",
        "    # Rescale the pixel values to be in the range [0, 1]\n",
        "    images = [rescale(image, scale=rescale_factor) for image in images]\n",
        "    # Normalize the images to have mean 0 and standard deviation 1\n",
        "    images = [normalize(image, mean=image_mean, std=image_std) for image in images]\n",
        "    # Move the channel dimension to the first dimension. The model expects images in the format [Channel, Height, Width]\n",
        "    images = [image.transpose(2, 0, 1) for image in images]\n",
        "    return images\n",
        "\n",
        "\n",
        "class PaliGemmaProcessor:\n",
        "    IMAGE_TOKEN = \"<image>\"\n",
        "\n",
        "    def __init__(self, tokenizer, num_image_tokens: int, image_size: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_seq_length = num_image_tokens\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # Tokenizer described here: https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md#tokenizer\n",
        "        tokens_to_add = {\"additional_special_tokens\": [self.IMAGE_TOKEN]}\n",
        "        tokenizer.add_special_tokens(tokens_to_add)\n",
        "        EXTRA_TOKENS = [\n",
        "            f\"<loc{i:04d}>\" for i in range(1024)\n",
        "        ]  # These tokens are used for object detection (bounding boxes)\n",
        "        EXTRA_TOKENS += [\n",
        "            f\"<seg{i:03d}>\" for i in range(128)\n",
        "        ]  # These tokens are used for object segmentation\n",
        "        tokenizer.add_tokens(EXTRA_TOKENS)\n",
        "        self.image_token_id = tokenizer.convert_tokens_to_ids(self.IMAGE_TOKEN)\n",
        "        # We will add the BOS and EOS tokens ourselves\n",
        "        tokenizer.add_bos_token = False\n",
        "        tokenizer.add_eos_token = False\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        text: List[str],\n",
        "        images: List[Image.Image],\n",
        "        suffix: Optional[List[str]] = None,\n",
        "        padding: str = \"longest\",\n",
        "        truncation: bool = True,\n",
        "        return_tensors: str = \"pt\",\n",
        "    ) -> dict:\n",
        "        assert len(images) == 1 and len(text) == 1, f\"Received {len(images)} images for {len(text)} prompts.\"\n",
        "\n",
        "        # Process images\n",
        "        pixel_values = process_images(\n",
        "            images,\n",
        "            size=(self.image_size, self.image_size),\n",
        "            resample=Image.Resampling.BICUBIC,\n",
        "            rescale_factor=1 / 255.0,\n",
        "            image_mean=IMAGENET_STANDARD_MEAN,\n",
        "            image_std=IMAGENET_STANDARD_STD,\n",
        "        )\n",
        "        # Convert the list of numpy arrays to a single numpy array with shape [Batch_Size, Channel, Height, Width]\n",
        "        pixel_values = np.stack(pixel_values, axis=0)\n",
        "        # Convert the numpy array to a PyTorch tensor\n",
        "        pixel_values = torch.tensor(pixel_values)\n",
        "\n",
        "        # Prepend a `self.image_seq_length` number of image tokens to the prompt\n",
        "        input_strings = [\n",
        "            add_image_tokens_to_prompt(\n",
        "                prefix_prompt=prompt,\n",
        "                bos_token=self.tokenizer.bos_token,\n",
        "                image_seq_len=self.image_seq_length,\n",
        "                image_token=self.IMAGE_TOKEN,\n",
        "            )\n",
        "            for prompt in text\n",
        "        ]\n",
        "\n",
        "        # Process inputs based on whether suffix is provided\n",
        "        if suffix is not None:\n",
        "            assert len(suffix) == len(text), \"Number of suffixes must match number of prompts\"\n",
        "\n",
        "            # Tokenize the input prompts\n",
        "            inputs = self.tokenizer(\n",
        "                input_strings,\n",
        "                return_tensors=return_tensors,\n",
        "                padding=padding,\n",
        "                truncation=truncation,\n",
        "            )\n",
        "\n",
        "            # Tokenize the suffixes\n",
        "            suffix_encodings = self.tokenizer(\n",
        "                suffix,\n",
        "                return_tensors=return_tensors,\n",
        "                padding=padding,\n",
        "                truncation=truncation,\n",
        "            )\n",
        "\n",
        "            # Create token_type_ids: 0 for prompt, 1 for suffix\n",
        "            batch_size, seq_len = inputs[\"input_ids\"].shape\n",
        "            token_type_ids = torch.zeros_like(inputs[\"input_ids\"])\n",
        "\n",
        "            # Create labels for training\n",
        "            # Start with -100 (ignored in loss calculation)\n",
        "            labels = torch.full_like(inputs[\"input_ids\"], fill_value=-100)\n",
        "\n",
        "            # Concatenate suffix tokens to input_ids for the complete sequence\n",
        "            full_input_ids = []\n",
        "            full_attention_mask = []\n",
        "            full_token_type_ids = []\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                prompt_len = inputs[\"attention_mask\"][i].sum().item()\n",
        "                suffix_len = suffix_encodings[\"attention_mask\"][i].sum().item()\n",
        "\n",
        "                # Combine input_ids\n",
        "                combined_input_ids = torch.cat([\n",
        "                    inputs[\"input_ids\"][i, :prompt_len],\n",
        "                    suffix_encodings[\"input_ids\"][i, 1:suffix_len]  # Skip BOS token\n",
        "                ])\n",
        "\n",
        "                # Create attention mask\n",
        "                combined_attention_mask = torch.ones(combined_input_ids.shape[0])\n",
        "\n",
        "                # Create token_type_ids (0 for prompt, 1 for suffix)\n",
        "                combined_token_type_ids = torch.cat([\n",
        "                    torch.zeros(prompt_len),\n",
        "                    torch.ones(suffix_len - 1)  # Skip BOS token\n",
        "                ])\n",
        "\n",
        "                # Create labels (only for suffix part)\n",
        "                combined_labels = torch.cat([\n",
        "                    torch.full((prompt_len,), fill_value=-100),\n",
        "                    suffix_encodings[\"input_ids\"][i, 1:suffix_len]  # Skip BOS token\n",
        "                ])\n",
        "\n",
        "                # Pad to max length if needed\n",
        "                max_len = max(seq_len, prompt_len + suffix_len - 1)\n",
        "\n",
        "                padded_input_ids = torch.full((max_len,), fill_value=self.tokenizer.pad_token_id)\n",
        "                padded_attention_mask = torch.zeros(max_len)\n",
        "                padded_token_type_ids = torch.zeros(max_len)\n",
        "                padded_labels = torch.full((max_len,), fill_value=-100)\n",
        "\n",
        "                combined_len = combined_input_ids.shape[0]\n",
        "                padded_input_ids[:combined_len] = combined_input_ids\n",
        "                padded_attention_mask[:combined_len] = combined_attention_mask\n",
        "                padded_token_type_ids[:combined_len] = combined_token_type_ids\n",
        "                padded_labels[:combined_len] = combined_labels\n",
        "\n",
        "                full_input_ids.append(padded_input_ids)\n",
        "                full_attention_mask.append(padded_attention_mask)\n",
        "                full_token_type_ids.append(padded_token_type_ids)\n",
        "\n",
        "            # Stack tensors\n",
        "            inputs[\"input_ids\"] = torch.stack(full_input_ids)\n",
        "            inputs[\"attention_mask\"] = torch.stack(full_attention_mask)\n",
        "            inputs[\"token_type_ids\"] = torch.stack(full_token_type_ids)\n",
        "            inputs[\"labels\"] = torch.stack(full_input_ids)  # Use input_ids as labels\n",
        "\n",
        "            # Replace padding token ids with -100 in labels\n",
        "            inputs[\"labels\"][inputs[\"labels\"] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        else:\n",
        "            # Standard processing without suffix\n",
        "            inputs = self.tokenizer(\n",
        "                input_strings,\n",
        "                return_tensors=return_tensors,\n",
        "                padding=padding,\n",
        "                truncation=truncation,\n",
        "            )\n",
        "\n",
        "        return_data = {\"pixel_values\": pixel_values, **inputs}\n",
        "        return return_data\n",
        "\n",
        "\n",
        "\n",
        "# === inference.py ===\n",
        "from PIL import Image\n",
        "import torch\n",
        "import fire\n",
        "\n",
        "def move_inputs_to_device(model_inputs: dict, device: str):\n",
        "    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def get_model_inputs(\n",
        "    processor: PaliGemmaProcessor, prompt: str, image_file_path: str, device: str\n",
        "):\n",
        "    image = Image.open(image_file_path)\n",
        "    images = [image]\n",
        "    prompts = [prompt]\n",
        "    model_inputs = processor(text=prompts, images=images)\n",
        "    model_inputs = move_inputs_to_device(model_inputs, device)\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "def test_inference(\n",
        "    model: PaliGemmaForConditionalGeneration,\n",
        "    processor: PaliGemmaProcessor,\n",
        "    device: str,\n",
        "    prompt: str,\n",
        "    image_file_path: str,\n",
        "    max_tokens_to_generate: int,\n",
        "    temperature: float,\n",
        "    top_p: float,\n",
        "    do_sample: bool,\n",
        "):\n",
        "    model_inputs = get_model_inputs(processor, prompt, image_file_path, device)\n",
        "    input_ids = model_inputs[\"input_ids\"]\n",
        "    attention_mask = model_inputs[\"attention_mask\"]\n",
        "    pixel_values = model_inputs[\"pixel_values\"]\n",
        "\n",
        "    kv_cache = KVCache()\n",
        "\n",
        "    # Generate tokens until you see the stop token\n",
        "    stop_token = processor.tokenizer.eos_token_id\n",
        "    generated_tokens = []\n",
        "\n",
        "    for _ in range(max_tokens_to_generate):\n",
        "        # Get the model outputs\n",
        "        # TODO: remove the labels\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            pixel_values=pixel_values,\n",
        "            attention_mask=attention_mask,\n",
        "            kv_cache=kv_cache,\n",
        "        )\n",
        "        kv_cache = outputs[\"kv_cache\"]\n",
        "        next_token_logits = outputs[\"logits\"][:, -1, :]\n",
        "        # Sample the next token\n",
        "        if do_sample:\n",
        "            # Apply temperature\n",
        "            next_token_logits = torch.softmax(next_token_logits / temperature, dim=-1)\n",
        "            next_token = _sample_top_p(next_token_logits, top_p)\n",
        "        else:\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "        assert next_token.size() == (1, 1)\n",
        "        next_token = next_token.squeeze(0)  # Remove batch dimension\n",
        "        generated_tokens.append(next_token)\n",
        "        # Stop if the stop token has been generated\n",
        "        if next_token.item() == stop_token:\n",
        "            break\n",
        "        # Append the next token to the input\n",
        "        input_ids = next_token.unsqueeze(-1)\n",
        "        attention_mask = torch.cat(\n",
        "            [attention_mask, torch.ones((1, 1), device=input_ids.device)], dim=-1\n",
        "        )\n",
        "\n",
        "    generated_tokens = torch.cat(generated_tokens, dim=-1)\n",
        "    # Decode the generated tokens\n",
        "    decoded = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    print(prompt + decoded)\n",
        "\n",
        "\n",
        "def _sample_top_p(probs: torch.Tensor, p: float):\n",
        "    # (B, vocab_size)\n",
        "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "    # (B, vocab_size)\n",
        "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "    # (B, vocab_size)\n",
        "    # (Substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n",
        "    mask = probs_sum - probs_sort > p\n",
        "    # Zero out all the probabilities of tokens that are not selected by the Top P\n",
        "    probs_sort[mask] = 0.0\n",
        "    # Redistribute the probabilities so that they sum up to 1.\n",
        "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "    # Sample a token (its index) from the top p distribution\n",
        "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "    # Get the token position in the vocabulary corresponding to the sampled index\n",
        "    next_token = torch.gather(probs_idx, -1, next_token)\n",
        "    return next_token\n",
        "\n",
        "def main(\n",
        "    model_path: str = \"/content/Files\",\n",
        "    prompts: list = None,  # Change this to accept a list of prompts\n",
        "    image_file_path: str = \"/content/2341.jpg\",\n",
        "    max_tokens_to_generate: int = 100,\n",
        "    temperature: float = 0.8,\n",
        "    top_p: float = 0.9,\n",
        "    do_sample: bool = False,\n",
        "    only_cpu: bool = False,\n",
        "):\n",
        "    device = \"cpu\"\n",
        "\n",
        "    if not only_cpu:\n",
        "        if torch.cuda.is_available():\n",
        "            device = \"cuda\"\n",
        "        elif torch.backends.mps.is_available():\n",
        "            device = \"mps\"\n",
        "\n",
        "    print(\"Device in use: \", device)\n",
        "\n",
        "    print(f\"Loading model\")\n",
        "    model, tokenizer = load_hf_model(model_path, device)\n",
        "    model = model.to(device).eval()\n",
        "\n",
        "    num_image_tokens = model.config.vision_config.num_image_tokens\n",
        "    image_size = model.config.vision_config.image_size\n",
        "    processor = PaliGemmaProcessor(tokenizer, num_image_tokens, image_size)\n",
        "\n",
        "    print(\"Running inference for multiple prompts:\")\n",
        "    with torch.no_grad():\n",
        "        # Iterate over all prompts and run inference\n",
        "        for prompt in prompts:\n",
        "            print(f\"Processing prompt: {prompt}\")\n",
        "            test_inference(\n",
        "                model,\n",
        "                processor,\n",
        "                device,\n",
        "                prompt,\n",
        "                image_file_path,\n",
        "                max_tokens_to_generate,\n",
        "                temperature,\n",
        "                top_p,\n",
        "                do_sample,\n",
        "            )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if we are in a Colab environment and bypass `fire` if it's a notebook\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "\n",
        "    if IN_COLAB:\n",
        "       # Example usage with multiple prompts\n",
        "        prompts = [\n",
        "          \"What is the subcategory of this fashion item?\",\n",
        "          \"What type of article is this fashion item?\",\n",
        "          \"What is the base color of this fashion item?\",\n",
        "          \"What season is this fashion item designed for?\",\n",
        "          \"What is the intended usage of this fashion item?\"\n",
        "        ]\n",
        "        model_path = \"/content/Files\"  # Or any path you use\n",
        "        image_file_path = \"/content/2341.jpg\"  # Update this with the correct path\n",
        "        max_tokens_to_generate = 100\n",
        "        temperature = 0.9\n",
        "        top_p = 0.4\n",
        "        do_sample = False\n",
        "        only_cpu = False\n",
        "\n",
        "        main(\n",
        "            model_path=model_path,\n",
        "            prompts=prompts,  # Pass the list of prompts here\n",
        "            image_file_path=image_file_path,\n",
        "            max_tokens_to_generate=max_tokens_to_generate,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=do_sample,\n",
        "            only_cpu=only_cpu,\n",
        "        )\n",
        "    else:\n",
        "        # For other environments (like terminal), use fire\n",
        "        import fire\n",
        "        fire.Fire(main)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DFDdVGvZQqx",
        "outputId": "e544ac46-3152-4434-ba4c-362c7d7a2d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device in use:  cpu\n",
            "Loading model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference for multiple prompts:\n",
            "Processing prompt: What is the subcategory of this fashion item?\n",
            "What is the subcategory of this fashion item?clothing\n",
            "Processing prompt: What type of article is this fashion item?\n",
            "What type of article is this fashion item?all\n",
            "Processing prompt: What is the base color of this fashion item?\n",
            "What is the base color of this fashion item?black\n",
            "Processing prompt: What season is this fashion item designed for?\n",
            "What season is this fashion item designed for?summer\n",
            "Processing prompt: What is the intended usage of this fashion item?\n",
            "What is the intended usage of this fashion item?main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# combined application"
      ],
      "metadata": {
        "id": "cNIem13uPtON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import tiktoken\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from google.colab import drive\n",
        "\n",
        "# Define paths\n",
        "MODEL_PATH = \"/content/Files\"  # Base VLM model path\n",
        "TRAINED_VLM_PATH = \"/content/drive/MyDrive/finetuned_paligemma/temp_train_model/model.pt\"  # Fine-tuned VLM path\n",
        "LLM_ZIP_PATH = '/content/drive/MyDrive/fashion_files/best_model.zip'  # LLM model zip path\n",
        "LLM_UNZIP_DESTINATION = '/content/extracted_model/'  # Where to extract LLM model\n",
        "USER_TEXT_PATH = '/content/drive/MyDrive/fashion_files/user.txt'  # User profile path\n",
        "CONFIG_PATH = '/content/drive/MyDrive/fashion_files/generation_config.json'  # Generation config path\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/finetuned_paligemma\"  # Output directory\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Default generation parameters (will be overridden by config file if present)\n",
        "DEFAULT_CONFIG = {\n",
        "    \"max_new_tokens\": 150,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 40,\n",
        "    \"instruction\": \"\",\n",
        "    \"input_text\": \"\",\n",
        "    \"consider_wardrobe\": \"no\",  # For personalization\n",
        "    \"consider_user_details\": \"no\"  # For personalization\n",
        "}\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(LLM_UNZIP_DESTINATION, exist_ok=True)\n",
        "os.makedirs(os.path.dirname(USER_TEXT_PATH), exist_ok=True)\n",
        "\n",
        "# PART 1: VLM FUNCTIONS\n",
        "# -------------------------------------\n",
        "\n",
        "# Function to capture images from webcam in Colab\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "          const div = document.createElement('div');\n",
        "          const capture = document.createElement('button');\n",
        "          capture.textContent = 'Capture';\n",
        "          div.appendChild(capture);\n",
        "\n",
        "          const video = document.createElement('video');\n",
        "          video.style.display = 'block';\n",
        "          const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "          document.body.appendChild(div);\n",
        "          div.appendChild(video);\n",
        "          video.srcObject = stream;\n",
        "          await video.play();\n",
        "\n",
        "          // Resize the output to fit the video element.\n",
        "          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "          // Wait for Capture to be clicked.\n",
        "          await new Promise((resolve) => {\n",
        "            capture.onclick = resolve;\n",
        "          });\n",
        "\n",
        "          const canvas = document.createElement('canvas');\n",
        "          canvas.width = video.videoWidth;\n",
        "          canvas.height = video.videoHeight;\n",
        "          canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "          stream.getVideoTracks()[0].stop();\n",
        "          div.remove();\n",
        "          return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "        ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "def load_finetuned_vlm(base_model_path, trained_model_path, device):\n",
        "    print(f\"Loading base VLM model from {base_model_path}\")\n",
        "\n",
        "    # Import model architecture and processor\n",
        "    # Note: These would need to be properly imported in a real implementation\n",
        "    # from paligemma import load_hf_model, PaliGemmaProcessor, KVCache\n",
        "\n",
        "    model, tokenizer = load_hf_model(base_model_path, device)\n",
        "    if os.path.exists(trained_model_path):\n",
        "        print(f\"Loading fine-tuned VLM weights from {trained_model_path}\")\n",
        "        state_dict = torch.load(trained_model_path, map_location=device)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "        print(\"Successfully loaded fine-tuned VLM weights\")\n",
        "    else:\n",
        "        print(f\"Warning: Fine-tuned VLM model not found at {trained_model_path}. Using base model.\")\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "def move_inputs_to_device(model_inputs: dict, device: str):\n",
        "    return {k: v.to(device) for k, v in model_inputs.items()}\n",
        "\n",
        "def get_model_inputs(processor, prompt, image, device):\n",
        "    \"\"\"Process image (as numpy array or PIL Image) with the processor\"\"\"\n",
        "    if isinstance(image, np.ndarray):\n",
        "        # Convert OpenCV BGR to RGB\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        pil_image = Image.fromarray(image_rgb)\n",
        "    else:\n",
        "        pil_image = image\n",
        "\n",
        "    model_inputs = processor(text=[prompt], images=[pil_image])\n",
        "    model_inputs = move_inputs_to_device(model_inputs, device)\n",
        "    return model_inputs\n",
        "\n",
        "def generate_vlm_prediction(model, processor, image, prompt, max_tokens=50):\n",
        "    \"\"\"Generate prediction for an image (can be PIL Image or numpy array)\"\"\"\n",
        "    model_inputs = get_model_inputs(processor, prompt, image, device)\n",
        "    input_ids = model_inputs[\"input_ids\"]\n",
        "    attention_mask = model_inputs[\"attention_mask\"]\n",
        "    pixel_values = model_inputs[\"pixel_values\"]\n",
        "    kv_cache = KVCache()\n",
        "    stop_token = processor.tokenizer.eos_token_id\n",
        "    generated_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_tokens):\n",
        "            # Always pass pixel_values for this architecture\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                pixel_values=pixel_values,\n",
        "                attention_mask=attention_mask,\n",
        "                kv_cache=kv_cache,\n",
        "            )\n",
        "            kv_cache = outputs[\"kv_cache\"]\n",
        "            next_token_logits = outputs[\"logits\"][:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "            token_id = next_token.item()\n",
        "            generated_tokens.append(token_id)\n",
        "            if token_id == stop_token:\n",
        "                break\n",
        "            input_ids = next_token\n",
        "            attention_mask = torch.cat([attention_mask, torch.ones((1, 1), device=device)], dim=-1)\n",
        "\n",
        "    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "def process_image_with_vlm(vlm_model, processor, image_path=None, use_webcam=False):\n",
        "    \"\"\"Process an image with the VLM and return formatted results\"\"\"\n",
        "    # Define the prompts for the VLM\n",
        "    prompts = [\n",
        "        \"What is the subcategory of this fashion item?\",\n",
        "        \"What type of article is this fashion item?\",\n",
        "        \"What is the base color of this fashion item?\",\n",
        "        \"What season is this fashion item designed for?\",\n",
        "        \"What is the intended usage of this fashion item?\"\n",
        "    ]\n",
        "\n",
        "    # Get image from webcam or file\n",
        "    if use_webcam:\n",
        "        print(\"Click 'Capture' to take a photo for fashion analysis\")\n",
        "        filename = take_photo()\n",
        "        print(f\"Photo saved as {filename}\")\n",
        "        img = Image.open(filename)\n",
        "    else:\n",
        "        # Use provided image path\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Image not found at {image_path}\")\n",
        "            return None\n",
        "        img = Image.open(image_path)\n",
        "\n",
        "    # Display the image\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Fashion Item\")\n",
        "    plt.show()\n",
        "\n",
        "    # Process the image with VLM\n",
        "    print(\"\\nRunning VLM analysis on image...\")\n",
        "    results = {}\n",
        "    for prompt in prompts:\n",
        "        prediction = generate_vlm_prediction(vlm_model, processor, img, prompt)\n",
        "        # Store the result\n",
        "        key = prompt.replace(\"What is the \", \"\").replace(\"?\", \"\").strip()\n",
        "        results[key] = prediction\n",
        "\n",
        "    # Create a concise description\n",
        "    concise_desc = f\"given image: {results.get('intended usage', '').lower()} \" \\\n",
        "                  f\"{results.get('base color', '').lower()} \" \\\n",
        "                  f\"{results.get('type of article', '').lower()} \" \\\n",
        "                  f\"{results.get('subcategory', '').lower()} \" \\\n",
        "                  f\"for {results.get('season', '').lower()}\"\n",
        "\n",
        "    # Display detailed results\n",
        "    print(\"\\nVLM Analysis Results:\")\n",
        "    for prompt, result in zip(prompts, results.values()):\n",
        "        print(f\"{prompt} {result}\")\n",
        "\n",
        "    print(f\"\\nConcise description: {concise_desc}\")\n",
        "\n",
        "    return concise_desc\n",
        "\n",
        "\n",
        "# PART 2: LLM FUNCTIONS\n",
        "# -------------------------------------\n",
        "\n",
        "# Class for parsing and handling user profile data\n",
        "class UserProfileManager:\n",
        "    def __init__(self, user_text_path):\n",
        "        self.user_text_path = user_text_path\n",
        "        self.user_data = {}\n",
        "        self.user_wardrobe = []\n",
        "        self.load_user_data()\n",
        "\n",
        "    def load_user_data(self):\n",
        "        \"\"\"Load and parse user data from user.txt\"\"\"\n",
        "        if not os.path.exists(self.user_text_path):\n",
        "            print(f\"User profile not found at {self.user_text_path}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            with open(self.user_text_path, 'r') as f:\n",
        "                content = f.read().strip()\n",
        "                # Parse the user data\n",
        "                self._parse_user_data(content)\n",
        "                print(f\"Loaded user profile from {self.user_text_path}\")\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading user profile: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _parse_user_data(self, content):\n",
        "        \"\"\"Parse user data content into structured data\"\"\"\n",
        "        lines = content.split('\\n')\n",
        "        for line in lines:\n",
        "            # Skip empty lines and lines without a proper key:value format\n",
        "            if not line.strip() or ':' not in line:\n",
        "                continue\n",
        "\n",
        "            parts = line.split(':', 1)\n",
        "            key = parts[0].strip().lower()\n",
        "            value = parts[1].strip()\n",
        "\n",
        "            # Special handling for wardrobe items\n",
        "            if key == 'wardrobe':\n",
        "                self.user_wardrobe = [item.strip() for item in value.split(',')]\n",
        "                self.user_data[key] = self.user_wardrobe\n",
        "            else:\n",
        "                self.user_data[key] = value\n",
        "\n",
        "    def get_user_profile_summary(self):\n",
        "        \"\"\"Get a formatted summary of the user profile for the model\"\"\"\n",
        "        if not self.user_data:\n",
        "            return \"No user profile data available.\"\n",
        "\n",
        "        summary = []\n",
        "        # Add basic user information\n",
        "        for key, value in self.user_data.items():\n",
        "            if key != 'wardrobe':  # Handle wardrobe separately\n",
        "                summary.append(f\"{key}: {value}\")\n",
        "\n",
        "        return \"\\n\".join(summary)\n",
        "\n",
        "    def get_wardrobe_summary(self):\n",
        "        \"\"\"Get a formatted summary of the user's wardrobe\"\"\"\n",
        "        if not self.user_wardrobe:\n",
        "            return \"No wardrobe information available.\"\n",
        "\n",
        "        return \"Wardrobe items: \" + \", \".join(self.user_wardrobe)\n",
        "\n",
        "# Function to extract zip with progress tracking\n",
        "def extract_with_progress(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # Get file info\n",
        "        file_info_list = zip_ref.infolist()\n",
        "\n",
        "        # Set up tqdm progress bar\n",
        "        with tqdm(total=len(file_info_list), desc=\"Extracting files\") as pbar:\n",
        "            for file_info in file_info_list:\n",
        "                zip_ref.extract(file_info, extract_to)\n",
        "                pbar.update(1)\n",
        "\n",
        "    print(f\"Extraction complete to {extract_to}\")\n",
        "    return os.path.join(extract_to, os.path.basename(zip_path).replace('.zip', '.pt'))\n",
        "\n",
        "def format_input(instruction, input_text=\"\", consider_wardrobe=\"no\", consider_user_details=\"no\", user_profile=None):\n",
        "    \"\"\"Format the instruction and input according to the training format, including personalization options.\"\"\"\n",
        "    # Base instruction\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request. \"\n",
        "        f\"Do not generate additional inputs or profile information.\"\n",
        "        f\"\\n\\n### Instruction:\\n{instruction}\"\n",
        "    )\n",
        "\n",
        "    # Add personalization details if requested\n",
        "    if user_profile and consider_user_details.lower() == \"yes\":\n",
        "        instruction_text += f\"\\n\\nConsider the following user details for personalization:\\n{user_profile.get_user_profile_summary()}\"\n",
        "\n",
        "    # Add wardrobe information if requested\n",
        "    if user_profile and consider_wardrobe.lower() == \"yes\":\n",
        "        instruction_text += f\"\\n\\nConsider these items from the user's wardrobe for your suggestions:\\n{user_profile.get_wardrobe_summary()}\"\n",
        "\n",
        "    # Add input text if provided\n",
        "    input_part = f\"\\n\\n### Input:\\n{input_text}\" if input_text else \"\"\n",
        "\n",
        "    return instruction_text + input_part + \"\\n\\n### Response:\"\n",
        "\n",
        "def load_finetuned_llm(checkpoint_path, device):\n",
        "    \"\"\"Load the fine-tuned LLM from a checkpoint.\"\"\"\n",
        "    print(f\"Loading LLM model from {checkpoint_path}\")\n",
        "\n",
        "    # Import your model definitions here\n",
        "    # from gpt_model import GPTModel, GPT_CONFIG_774M\n",
        "\n",
        "    # Initialize model with configuration\n",
        "    model = GPTModel(GPT_CONFIG_774M)\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # Move model to device\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"LLM model loaded successfully! Validation loss: {checkpoint.get('val_loss', 'N/A')}\")\n",
        "    return model\n",
        "\n",
        "def generate_llm_text(model, tokenizer, prompt, device, max_new_tokens=150, temperature=0.7, top_k=40):\n",
        "    \"\"\"Generate text based on a prompt using the fine-tuned LLM.\"\"\"\n",
        "    # Tokenize the prompt\n",
        "    encoded = torch.tensor(tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})).unsqueeze(0).to(device)\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "\n",
        "    # Generate response\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Generation parameters: max_tokens={max_new_tokens}, temp={temperature}, top_k={top_k}\")\n",
        "    generated_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in tqdm(range(max_new_tokens), desc=\"Generating tokens\"):\n",
        "            # Get context for the current step\n",
        "            idx_cond = encoded[:, -context_size:]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "            # Get logits for the last token\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Apply temperature and top-k filtering\n",
        "            if temperature > 0:\n",
        "                if top_k is not None:\n",
        "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                    logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "                probs = torch.softmax(logits / temperature, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                # Greedy sampling\n",
        "                idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "            # Append new token\n",
        "            encoded = torch.cat((encoded, idx_next), dim=1)\n",
        "            generated_tokens.append(idx_next.item())\n",
        "\n",
        "            # Stop if EOS token is generated (50256 is <|endoftext|>)\n",
        "            if idx_next.item() == 50256:\n",
        "                break\n",
        "\n",
        "            # Also stop if we see \"### Input:\" - this is a sign the model is hallucinating additional input\n",
        "            current_text = tokenizer.decode(encoded.squeeze().tolist()[len(tokenizer.encode(prompt)):])\n",
        "            if \"### Input:\" in current_text:\n",
        "                break\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(encoded.squeeze().tolist())\n",
        "\n",
        "    # Return only the response part (after the prompt)\n",
        "    response = generated_text[len(prompt):]\n",
        "\n",
        "    # Remove any hallucinated input sections\n",
        "    if \"### Input:\" in response:\n",
        "        response = response.split(\"### Input:\")[0].strip()\n",
        "\n",
        "    # Remove any hallucinated response markers\n",
        "    if \"### Response:\" in response:\n",
        "        response = response.replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    return response, generated_tokens\n",
        "\n",
        "def load_user_config():\n",
        "    \"\"\"Load user configuration from config file if it exists, otherwise use defaults.\"\"\"\n",
        "    config = DEFAULT_CONFIG.copy()\n",
        "\n",
        "    # Check if config file exists\n",
        "    if os.path.exists(CONFIG_PATH):\n",
        "        try:\n",
        "            with open(CONFIG_PATH, 'r') as f:\n",
        "                user_config = json.load(f)\n",
        "                config.update(user_config)\n",
        "                print(f\"Loaded configuration from {CONFIG_PATH}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading config file: {e}\")\n",
        "            print(\"Using default configuration\")\n",
        "    else:\n",
        "        print(f\"Config file not found at {CONFIG_PATH}. Using default configuration.\")\n",
        "        # Create a template config file for future use\n",
        "        try:\n",
        "            with open(CONFIG_PATH, 'w') as f:\n",
        "                json.dump(DEFAULT_CONFIG, f, indent=4)\n",
        "                print(f\"Created template config file at {CONFIG_PATH}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create template config file: {e}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "def update_config_interactive(config):\n",
        "    \"\"\"Allow user to interactively update configuration parameters.\"\"\"\n",
        "    print(\"\\n===== Generation Configuration =====\")\n",
        "    print(\"Current settings:\")\n",
        "    for key, value in config.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    print(\"\\nEnter new values (press Enter to keep current value):\")\n",
        "\n",
        "    # Update numerical parameters\n",
        "    for param in [\"max_new_tokens\", \"temperature\", \"top_k\"]:\n",
        "        new_value = input(f\"{param} [{config[param]}]: \")\n",
        "        if new_value.strip():\n",
        "            try:\n",
        "                if param == \"max_new_tokens\" or param == \"top_k\":\n",
        "                    config[param] = int(new_value)\n",
        "                else:\n",
        "                    config[param] = float(new_value)\n",
        "            except ValueError:\n",
        "                print(f\"Invalid value for {param}, keeping current value.\")\n",
        "\n",
        "    # Update personalization parameters\n",
        "    for param in [\"consider_wardrobe\", \"consider_user_details\"]:\n",
        "        new_value = input(f\"{param} [{config[param]}] (yes/no): \")\n",
        "        if new_value.strip().lower() in ['yes', 'no']:\n",
        "            config[param] = new_value.lower()\n",
        "\n",
        "    # Save updated config\n",
        "    try:\n",
        "        with open(CONFIG_PATH, 'w') as f:\n",
        "            json.dump(config, f, indent=4)\n",
        "            print(f\"Configuration saved to {CONFIG_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save configuration: {e}\")\n",
        "\n",
        "    return config\n",
        "\n",
        "# PART 3: USER PROFILE MANAGEMENT\n",
        "# -------------------------------------\n",
        "\n",
        "def update_user_profile(user_profile):\n",
        "    \"\"\"Interactive function to update user profile data\"\"\"\n",
        "    print(\"\\n===== Update User Profile =====\")\n",
        "\n",
        "    if user_profile.user_data:\n",
        "        print(\"Current profile:\")\n",
        "        for key, value in user_profile.user_data.items():\n",
        "            if key != 'wardrobe':  # Display wardrobe separately\n",
        "                print(f\"  {key}: {value}\")\n",
        "\n",
        "        # Display wardrobe items separately\n",
        "        if user_profile.user_wardrobe:\n",
        "            print(\"Wardrobe items:\")\n",
        "            for i, item in enumerate(user_profile.user_wardrobe, 1):\n",
        "                print(f\"  {i}. {item}\")\n",
        "    else:\n",
        "        print(\"No profile data found. Creating new profile.\")\n",
        "\n",
        "    print(\"\\nOptions:\")\n",
        "    print(\"1. Edit basic profile information\")\n",
        "    print(\"2. Edit wardrobe items\")\n",
        "    print(\"3. Cancel\")\n",
        "\n",
        "    choice = input(\"Choose an option (1-3): \")\n",
        "\n",
        "    if choice == '1':\n",
        "        # Edit basic profile info\n",
        "        print(\"\\nEnter profile information (press Enter to keep current values, 'delete' to remove):\")\n",
        "        fields = ['name', 'age', 'gender', 'location', 'style', 'occupation', 'preferences', 'favorite_colors', 'size']\n",
        "\n",
        "        updated_data = {}\n",
        "        for field in fields:\n",
        "            current = user_profile.user_data.get(field, '')\n",
        "            new_value = input(f\"{field} [{current}]: \")\n",
        "\n",
        "            if new_value.lower() == 'delete':\n",
        "                # Don't include this field\n",
        "                pass\n",
        "            elif new_value.strip():\n",
        "                # Update with new value\n",
        "                updated_data[field] = new_value\n",
        "            elif field in user_profile.user_data:\n",
        "                # Keep current value\n",
        "                updated_data[field] = current\n",
        "\n",
        "        # Preserve wardrobe data\n",
        "        if 'wardrobe' in user_profile.user_data:\n",
        "            updated_data['wardrobe'] = user_profile.user_data['wardrobe']\n",
        "\n",
        "        # Update user data\n",
        "        user_profile.user_data = updated_data\n",
        "\n",
        "        # Save to file\n",
        "        save_updated_profile(user_profile)\n",
        "\n",
        "    elif choice == '2':\n",
        "        # Edit wardrobe items\n",
        "        print(\"\\nOptions for wardrobe:\")\n",
        "        print(\"1. Add items\")\n",
        "        print(\"2. Remove items\")\n",
        "        print(\"3. Replace all items\")\n",
        "        print(\"4. Cancel\")\n",
        "\n",
        "        wardrobe_choice = input(\"Choose an option (1-4): \")\n",
        "\n",
        "        if wardrobe_choice == '1':\n",
        "            # Add items\n",
        "            new_items = input(\"Enter items to add (comma separated): \")\n",
        "            items_to_add = [item.strip() for item in new_items.split(',') if item.strip()]\n",
        "\n",
        "            if 'wardrobe' not in user_profile.user_data:\n",
        "                user_profile.user_data['wardrobe'] = []\n",
        "                user_profile.user_wardrobe = []\n",
        "\n",
        "            user_profile.user_wardrobe.extend(items_to_add)\n",
        "            user_profile.user_data['wardrobe'] = user_profile.user_wardrobe\n",
        "\n",
        "            save_updated_profile(user_profile)\n",
        "\n",
        "        elif wardrobe_choice == '2':\n",
        "            # Remove items\n",
        "            if not user_profile.user_wardrobe:\n",
        "                print(\"No wardrobe items to remove.\")\n",
        "                return\n",
        "\n",
        "            print(\"Current items:\")\n",
        "            for i, item in enumerate(user_profile.user_wardrobe, 1):\n",
        "                print(f\"  {i}. {item}\")\n",
        "\n",
        "            to_remove = input(\"Enter numbers of items to remove (comma separated): \")\n",
        "            try:\n",
        "                indices = [int(idx.strip()) for idx in to_remove.split(',') if idx.strip()]\n",
        "                # Sort in reverse to avoid index shifting during removal\n",
        "                indices.sort(reverse=True)\n",
        "\n",
        "                for idx in indices:\n",
        "                    if 1 <= idx <= len(user_profile.user_wardrobe):\n",
        "                        del user_profile.user_wardrobe[idx-1]\n",
        "\n",
        "                user_profile.user_data['wardrobe'] = user_profile.user_wardrobe\n",
        "                save_updated_profile(user_profile)\n",
        "\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please enter numbers separated by commas.\")\n",
        "\n",
        "        elif wardrobe_choice == '3':\n",
        "            # Replace all\n",
        "            new_wardrobe = input(\"Enter new wardrobe items (comma separated): \")\n",
        "            new_items = [item.strip() for item in new_wardrobe.split(',') if item.strip()]\n",
        "\n",
        "            user_profile.user_wardrobe = new_items\n",
        "            user_profile.user_data['wardrobe'] = new_items\n",
        "\n",
        "            save_updated_profile(user_profile)\n",
        "\n",
        "def save_updated_profile(user_profile):\n",
        "    \"\"\"Save updated profile to user.txt file\"\"\"\n",
        "    try:\n",
        "        # Format profile data for saving\n",
        "        lines = []\n",
        "\n",
        "        # Add basic profile fields first\n",
        "        for key, value in user_profile.user_data.items():\n",
        "            if key != 'wardrobe':  # Handle wardrobe separately\n",
        "                lines.append(f\"{key}: {value}\")\n",
        "\n",
        "        # Add wardrobe at the end\n",
        "        if user_profile.user_wardrobe:\n",
        "            lines.append(f\"wardrobe: {', '.join(user_profile.user_wardrobe)}\")\n",
        "\n",
        "        # Write to file\n",
        "        with open(user_profile.user_text_path, 'w') as f:\n",
        "            f.write('\\n'.join(lines))\n",
        "\n",
        "        print(f\"Profile updated and saved to {user_profile.user_text_path}\")\n",
        "        # Reload the profile to ensure everything is up to date\n",
        "        user_profile.load_user_data()\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving profile: {e}\")\n",
        "        return False\n",
        "\n",
        "# PART 4: INTEGRATED SYSTEM\n",
        "# -------------------------------------\n",
        "\n",
        "def run_integrated_fashion_system():\n",
        "    \"\"\"Main function to run the integrated fashion recommendation system\"\"\"\n",
        "    # Mount Google Drive if needed\n",
        "    if not os.path.exists(\"/content/drive\"):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # Step 1: Extract LLM model\n",
        "    print(\"\\n===== Loading Models =====\")\n",
        "    llm_model_path = extract_with_progress(LLM_ZIP_PATH, LLM_UNZIP_DESTINATION)\n",
        "\n",
        "    # Step 2: Load tokenizer for LLM\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Step 3: Load user profile\n",
        "    user_profile = UserProfileManager(USER_TEXT_PATH)\n",
        "\n",
        "    # Check if user profile needs to be created or updated\n",
        "    if not user_profile.user_data:\n",
        "        print(\"No user profile found. Would you like to create one? (y/n)\")\n",
        "        if input().lower() == 'y':\n",
        "            update_user_profile(user_profile)\n",
        "    else:\n",
        "        print(\"User profile loaded. Would you like to update it? (y/n)\")\n",
        "        if input().lower() == 'y':\n",
        "            update_user_profile(user_profile)\n",
        "\n",
        "    # Step 4: Load models\n",
        "    llm_model = load_finetuned_llm(llm_model_path, device)\n",
        "\n",
        "    # Main loop\n",
        "    while True:\n",
        "        print(\"\\n===== Integrated Fashion Recommendation System =====\")\n",
        "        print(\"Options:\")\n",
        "        print(\"1. Use fashion image for recommendation\")\n",
        "        print(\"2. Get fashion advice without image\")\n",
        "        print(\"3. Update user profile\")\n",
        "        print(\"4. Exit\")\n",
        "\n",
        "        option = input(\"Choose an option (1-4): \")\n",
        "\n",
        "        if option == '1':\n",
        "            # Use fashion image\n",
        "            image_input_type = input(\"Do you want to (1) use webcam or (2) upload an image file? \")\n",
        "\n",
        "            image_description = None\n",
        "            if image_input_type == '1':\n",
        "                # Load VLM only when needed (since it might be large)\n",
        "                vlm_model, vlm_tokenizer = load_finetuned_vlm(MODEL_PATH, TRAINED_VLM_PATH, device)\n",
        "                num_image_tokens = vlm_model.config.vision_config.num_image_tokens\n",
        "                image_size = vlm_model.config.vision_config.image_size\n",
        "                processor = PaliGemmaProcessor(vlm_tokenizer, num_image_tokens, image_size)\n",
        "\n",
        "                # Use webcam\n",
        "                image_description = process_image_with_vlm(vlm_model, processor, use_webcam=True)\n",
        "            elif image_input_type == '2':\n",
        "                # Upload file\n",
        "                image_path = input(\"Enter the path to the image file: \")\n",
        "\n",
        "                # Load VLM only when needed\n",
        "                vlm_model, vlm_tokenizer = load_finetuned_vlm(MODEL_PATH, TRAINED_VLM_PATH, device)\n",
        "                num_image_tokens = vlm_model.config.vision_config.num_image_tokens\n",
        "                image_size = vlm_model.config.vision_config.image_size\n",
        "                processor = PaliGemmaProcessor(vlm_tokenizer, num_image_tokens, image_size)\n",
        "\n",
        "                image_description = process_image_with_vlm(vlm_model, processor, image_path=image_path)\n",
        "            else:\n",
        "                print(\"Invalid option. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            if not image_description:\n",
        "                print(\"Failed to analyze image. Please try again.\")\n",
        "                continue\n",
        "\n",
        "            # Now get instruction from user\n",
        "            instruction = input(\"\\nWhat would you like to know about this fashion item? \")\n",
        "\n",
        "            # Load config for LLM generation\n",
        "            config = load_user_config()\n",
        "\n",
        "            # Allow user to adjust generation parameters\n",
        "            print(\"\\nWould you like to adjust generation parameters? (y/n)\")\n",
        "            if input().lower() == 'y':\n",
        "                config = update_config_interactive(config)\n",
        "\n",
        "            # Format the instruction with the image description\n",
        "            combined_input = f\"{image_description}\\n\\n{instruction}\"\n",
        "\n",
        "            # Format the prompt with personalization options\n",
        "            prompt = format_input(\n",
        "                instruction,\n",
        "                combined_input,\n",
        "                config[\"consider_wardrobe\"],\n",
        "                config[\"consider_user_details\"],\n",
        "                user_profile\n",
        "            )\n",
        "\n",
        "            # Generate response\n",
        "            print(\"\\nGenerating fashion advice...\")\n",
        "            response, tokens = generate_llm_text(\n",
        "                llm_model, tokenizer, prompt, device,\n",
        "                max_new_tokens=config[\"max_new_tokens\"],\n",
        "                temperature=config[\"temperature\"],\n",
        "                top_k=config[\"top_k\"]\n",
        "            )\n",
        "\n",
        "            # Display results\n",
        "            print(\"\\n=== Fashion Advice ===\")\n",
        "            print(response)\n",
        "            print(\"\\n=== Generation Stats ===\")\n",
        "            print(f\"Generated {len(tokens)} tokens\")\n",
        "            print(f\"Personalization: Wardrobe = {config['consider_wardrobe']}, User Details = {config['consider_user_details']}\")\n",
        "\n",
        "        elif option == '2':\n",
        "            # Get fashion advice without image\n",
        "            # Get fashion advice without image\n",
        "            instruction = input(\"\\nWhat fashion advice would you like? \")\n",
        "\n",
        "            # Load config\n",
        "            config = load_user_config()\n",
        "\n",
        "            # Allow user to adjust generation parameters\n",
        "            print(\"\\nWould you like to adjust generation parameters? (y/n)\")\n",
        "            if input().lower() == 'y':\n",
        "                config = update_config_interactive(config)\n",
        "\n",
        "            # Optional input text\n",
        "            input_text = \"\"  # No additional input for text-only mode\n",
        "\n",
        "            # Format the prompt with personalization options\n",
        "            prompt = format_input(\n",
        "                instruction,\n",
        "                input_text,\n",
        "                config[\"consider_wardrobe\"],\n",
        "                config[\"consider_user_details\"],\n",
        "                user_profile\n",
        "            )\n",
        "\n",
        "            # Generate response\n",
        "            print(\"\\nGenerating fashion advice...\")\n",
        "            response, tokens = generate_llm_text(\n",
        "                llm_model, tokenizer, prompt, device,\n",
        "                max_new_tokens=config[\"max_new_tokens\"],\n",
        "                temperature=config[\"temperature\"],\n",
        "                top_k=config[\"top_k\"]\n",
        "            )\n",
        "\n",
        "            # Display results\n",
        "            print(\"\\n=== Fashion Advice ===\")\n",
        "            print(response)\n",
        "            print(\"\\n=== Generation Stats ===\")\n",
        "            print(f\"Generated {len(tokens)} tokens\")\n",
        "            print(f\"Personalization: Wardrobe = {config['consider_wardrobe']}, User Details = {config['consider_user_details']}\")\n",
        "\n",
        "        elif option == '3':\n",
        "            # Update user profile\n",
        "            update_user_profile(user_profile)\n",
        "\n",
        "        elif option == '4':\n",
        "            # Exit\n",
        "            print(\"Thank you for using the Fashion Recommendation System. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid option. Please try again.\")\n",
        "\n",
        "        # Ask if user wants to continue\n",
        "        print(\"\\nPress Enter to continue...\")\n",
        "        input()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_integrated_fashion_system()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zPXKeNYqZa62",
        "outputId": "690b8931-7c57-40a2-f54d-dfc3c234bdcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "\n",
            "===== Loading Models =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting files: 100%|██████████| 1/1 [02:51<00:00, 171.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete to /content/extracted_model/\n",
            "Loaded user profile from /content/drive/MyDrive/fashion_files/user.txt\n",
            "User profile loaded. Would you like to update it? (y/n)\n",
            "y\n",
            "\n",
            "===== Update User Profile =====\n",
            "Current profile:\n",
            "  name: sriram\n",
            "  age: 21\n",
            "  gender: male\n",
            "  location: india\n",
            "  style: minimalist\n",
            "  occupation: student\n",
            "  preferences: minimalistic, stylish, classy\n",
            "  favorite_colors: navy, beige, white, black\n",
            "  size: m\n",
            "Wardrobe items:\n",
            "  1. white button-up shirt\n",
            "  2. navy trousers\n",
            "  3. black coat\n",
            "  4. beige cashmere sweater\n",
            "  5. white sneakers\n",
            "  6. black ankle boots\n",
            "  7. denim jeans\n",
            "  8. white t-shirts\n",
            "  9. pink polo tshirt\n",
            "  10. little black dress\n",
            "  11. brown watch\n",
            "  12. cotton black tshirt\n",
            "  13. brown trousers\n",
            "\n",
            "Options:\n",
            "1. Edit basic profile information\n",
            "2. Edit wardrobe items\n",
            "3. Cancel\n",
            "Choose an option (1-3): 3\n",
            "Loading LLM model from /content/extracted_model/best_model.pt\n",
            "LLM model loaded successfully! Validation loss: 1.2289036760727565\n",
            "\n",
            "===== Integrated Fashion Recommendation System =====\n",
            "Options:\n",
            "1. Use fashion image for recommendation\n",
            "2. Get fashion advice without image\n",
            "3. Update user profile\n",
            "4. Exit\n",
            "Choose an option (1-4): 1\n",
            "Do you want to (1) use webcam or (2) upload an image file? 1\n",
            "Loading base VLM model from /content/Files\n",
            "Loading fine-tuned VLM weights from /content/drive/MyDrive/finetuned_paligemma/temp_train_model/model.pt\n",
            "Successfully loaded fine-tuned VLM weights\n",
            "Click 'Capture' to take a photo for fashion analysis\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        async function takePhoto(quality) {\n",
              "          const div = document.createElement('div');\n",
              "          const capture = document.createElement('button');\n",
              "          capture.textContent = 'Capture';\n",
              "          div.appendChild(capture);\n",
              "\n",
              "          const video = document.createElement('video');\n",
              "          video.style.display = 'block';\n",
              "          const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "          document.body.appendChild(div);\n",
              "          div.appendChild(video);\n",
              "          video.srcObject = stream;\n",
              "          await video.play();\n",
              "\n",
              "          // Resize the output to fit the video element.\n",
              "          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "          // Wait for Capture to be clicked.\n",
              "          await new Promise((resolve) => {\n",
              "            capture.onclick = resolve;\n",
              "          });\n",
              "\n",
              "          const canvas = document.createElement('canvas');\n",
              "          canvas.width = video.videoWidth;\n",
              "          canvas.height = video.videoHeight;\n",
              "          canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "          stream.getVideoTracks()[0].stop();\n",
              "          div.remove();\n",
              "          return canvas.toDataURL('image/jpeg', quality);\n",
              "        }\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "NotAllowedError: Permission denied",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b19821494473>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m     \u001b[0mrun_integrated_fashion_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-b19821494473>\u001b[0m in \u001b[0;36mrun_integrated_fashion_system\u001b[0;34m()\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0;31m# Use webcam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m                 \u001b[0mimage_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_image_with_vlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvlm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_webcam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mimage_input_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0;31m# Upload file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b19821494473>\u001b[0m in \u001b[0;36mprocess_image_with_vlm\u001b[0;34m(vlm_model, processor, image_path, use_webcam)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_webcam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Click 'Capture' to take a photo for fashion analysis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_photo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Photo saved as {filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b19821494473>\u001b[0m in \u001b[0;36mtake_photo\u001b[0;34m(filename, quality)\u001b[0m\n\u001b[1;32m     82\u001b[0m         ''')\n\u001b[1;32m     83\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'takePhoto({})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquality\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: NotAllowedError: Permission denied"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3yjJP7zbc0ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "with tkinter"
      ],
      "metadata": {
        "id": "7JtaDjkfPhDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import threading\n",
        "import tkinter as tk\n",
        "from tkinter import ttk, filedialog, scrolledtext, messagebox\n",
        "from PIL import Image, ImageTk\n",
        "import torch\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Define paths\n",
        "MODEL_PATH = \"/content/Files\"  # Base VLM model path\n",
        "TRAINED_VLM_PATH = \"/content/drive/MyDrive/finetuned_paligemma/temp_train_model/model.pt\"  # Fine-tuned VLM path\n",
        "LLM_ZIP_PATH = '/content/drive/MyDrive/fashion_files/best_model.zip'  # LLM model zip path\n",
        "LLM_EXTRACT_DIR = '/content/extracted_model/'  # Where to extract LLM model\n",
        "USER_PROFILE_PATH = '/content/drive/MyDrive/fashion_files/user.txt'  # User profile path\n",
        "CONFIG_PATH = '/content/drive/MyDrive/fashion_files/generation_config.json'  # Generation config path\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/finetuned_paligemma\"  # Output directory\n",
        "\n",
        "# Default generation parameters\n",
        "DEFAULT_CONFIG = {\n",
        "    \"max_new_tokens\": 150,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 40,\n",
        "    \"consider_wardrobe\": \"no\",  # For personalization\n",
        "    \"consider_user_details\": \"no\"  # For personalization\n",
        "}\n",
        "\n",
        "# Global variables for models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "llm_model = None\n",
        "vlm_model = None\n",
        "vlm_processor = None\n",
        "tokenizer = None\n",
        "user_profile = None\n",
        "webcam = None\n",
        "\n",
        "\n",
        "# ---------- MODEL MANAGER ----------\n",
        "class ModelManager:\n",
        "    \"\"\"Class to manage model loading and initialization\"\"\"\n",
        "\n",
        "    def __init__(self, status_callback=None):\n",
        "        self.llm_model = None\n",
        "        self.vlm_model = None\n",
        "        self.vlm_processor = None\n",
        "        self.tokenizer = None\n",
        "        self.status_callback = status_callback\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def update_status(self, message):\n",
        "        \"\"\"Update status message if callback is provided\"\"\"\n",
        "        if self.status_callback:\n",
        "            self.status_callback(message)\n",
        "        else:\n",
        "            print(message)\n",
        "\n",
        "    def initialize_models(self):\n",
        "        \"\"\"Initialize all required models\"\"\"\n",
        "        try:\n",
        "            # Initialize tokenizer first (lightweight)\n",
        "            self.update_status(\"Initializing tokenizer...\")\n",
        "            self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "            # Load models in separate threads to improve startup time\n",
        "            llm_thread = threading.Thread(target=self._load_llm)\n",
        "            vlm_thread = threading.Thread(target=self._load_vlm)\n",
        "\n",
        "            self.update_status(\"Loading models in background...\")\n",
        "            llm_thread.start()\n",
        "            vlm_thread.start()\n",
        "\n",
        "            # Wait for models to load\n",
        "            llm_thread.join()\n",
        "            vlm_thread.join()\n",
        "\n",
        "            self.update_status(\"All models loaded successfully!\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            self.update_status(f\"Failed to initialize models: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def _load_llm(self):\n",
        "        \"\"\"Load LLM model\"\"\"\n",
        "        try:\n",
        "            self.update_status(\"Loading LLM model...\")\n",
        "\n",
        "            # Extract LLM if needed\n",
        "            llm_model_path = self._extract_model_if_needed(LLM_ZIP_PATH, LLM_EXTRACT_DIR)\n",
        "\n",
        "            # Import required modules here to avoid global imports\n",
        "            from gpt_model import GPTModel, GPT_CONFIG_774M\n",
        "\n",
        "            # Initialize model with configuration\n",
        "            model = GPTModel(GPT_CONFIG_774M)\n",
        "\n",
        "            # Load the checkpoint\n",
        "            checkpoint = torch.load(llm_model_path, map_location=self.device)\n",
        "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "            # Move model to device\n",
        "            model.to(self.device)\n",
        "            model.eval()\n",
        "\n",
        "            self.llm_model = model\n",
        "            self.update_status(\"LLM model loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            self.update_status(f\"Error loading LLM: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _load_vlm(self):\n",
        "        \"\"\"Load VLM model\"\"\"\n",
        "        try:\n",
        "            self.update_status(\"Loading VLM model...\")\n",
        "\n",
        "            # Import model architecture and processor\n",
        "            from paligemma import load_hf_model, PaliGemmaProcessor\n",
        "\n",
        "            # Load base model\n",
        "            model, tokenizer = load_hf_model(MODEL_PATH, self.device)\n",
        "\n",
        "            # Load fine-tuned weights if available\n",
        "            if os.path.exists(TRAINED_VLM_PATH):\n",
        "                self.update_status(\"Loading fine-tuned VLM weights...\")\n",
        "                state_dict = torch.load(TRAINED_VLM_PATH, map_location=self.device)\n",
        "                model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            # Initialize processor\n",
        "            num_image_tokens = model.config.vision_config.num_image_tokens\n",
        "            image_size = model.config.vision_config.image_size\n",
        "            processor = PaliGemmaProcessor(tokenizer, num_image_tokens, image_size)\n",
        "\n",
        "            self.vlm_model = model\n",
        "            self.vlm_processor = processor\n",
        "            self.update_status(\"VLM model loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            self.update_status(f\"Error loading VLM: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _extract_model_if_needed(self, zip_path, extract_dir):\n",
        "        \"\"\"Extract model zip if needed\"\"\"\n",
        "        # Expected path for the extracted model\n",
        "        model_path = os.path.join(extract_dir, os.path.basename(zip_path).replace('.zip', '.pt'))\n",
        "\n",
        "        # Check if model is already extracted\n",
        "        if os.path.exists(model_path):\n",
        "            self.update_status(f\"Using existing model at {model_path}\")\n",
        "            return model_path\n",
        "\n",
        "        # Create extraction directory if it doesn't exist\n",
        "        os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "        # Extract with progress tracking\n",
        "        self.update_status(f\"Extracting model from {zip_path}...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            # Get file info\n",
        "            file_info_list = zip_ref.infolist()\n",
        "\n",
        "            # Extract all files\n",
        "            for file_info in file_info_list:\n",
        "                zip_ref.extract(file_info, extract_dir)\n",
        "\n",
        "        self.update_status(f\"Extraction complete to {extract_dir}\")\n",
        "        return model_path\n",
        "\n",
        "    def get_models(self):\n",
        "        \"\"\"Get loaded models\"\"\"\n",
        "        return {\n",
        "            \"llm\": self.llm_model,\n",
        "            \"vlm\": self.vlm_model,\n",
        "            \"processor\": self.vlm_processor,\n",
        "            \"tokenizer\": self.tokenizer,\n",
        "            \"device\": self.device\n",
        "        }\n",
        "\n",
        "\n",
        "# ---------- USER PROFILE MANAGER ----------\n",
        "class UserProfileManager:\n",
        "    \"\"\"Manager for user profile data\"\"\"\n",
        "\n",
        "    def __init__(self, user_text_path):\n",
        "        self.user_text_path = user_text_path\n",
        "        self.user_data = {}\n",
        "        self.user_wardrobe = []\n",
        "        self.load_user_data()\n",
        "\n",
        "    def load_user_data(self):\n",
        "        \"\"\"Load and parse user data from user.txt\"\"\"\n",
        "        if not os.path.exists(self.user_text_path):\n",
        "            print(f\"User profile not found at {self.user_text_path}\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            with open(self.user_text_path, 'r') as f:\n",
        "                content = f.read().strip()\n",
        "                # Parse the user data\n",
        "                self._parse_user_data(content)\n",
        "                print(f\"Loaded user profile from {self.user_text_path}\")\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading user profile: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _parse_user_data(self, content):\n",
        "        \"\"\"Parse user data content into structured data\"\"\"\n",
        "        lines = content.split('\\n')\n",
        "        for line in lines:\n",
        "            # Skip empty lines and lines without a proper key:value format\n",
        "            if not line.strip() or ':' not in line:\n",
        "                continue\n",
        "\n",
        "            parts = line.split(':', 1)\n",
        "            key = parts[0].strip().lower()\n",
        "            value = parts[1].strip()\n",
        "\n",
        "            # Special handling for wardrobe items\n",
        "            if key == 'wardrobe':\n",
        "                self.user_wardrobe = [item.strip() for item in value.split(',')]\n",
        "                self.user_data[key] = self.user_wardrobe\n",
        "            else:\n",
        "                self.user_data[key] = value\n",
        "\n",
        "    def get_user_profile_summary(self):\n",
        "        \"\"\"Get a formatted summary of the user profile for the model\"\"\"\n",
        "        if not self.user_data:\n",
        "            return \"No user profile data available.\"\n",
        "\n",
        "        summary = []\n",
        "        # Add basic user information\n",
        "        for key, value in self.user_data.items():\n",
        "            if key != 'wardrobe':  # Handle wardrobe separately\n",
        "                summary.append(f\"{key}: {value}\")\n",
        "\n",
        "        return \"\\n\".join(summary)\n",
        "\n",
        "    def get_wardrobe_summary(self):\n",
        "        \"\"\"Get a formatted summary of the user's wardrobe\"\"\"\n",
        "        if not self.user_wardrobe:\n",
        "            return \"No wardrobe information available.\"\n",
        "\n",
        "        return \"Wardrobe items: \" + \", \".join(self.user_wardrobe)\n",
        "\n",
        "    def save_profile(self, user_data):\n",
        "        \"\"\"Save updated profile to file\"\"\"\n",
        "        try:\n",
        "            self.user_data = user_data\n",
        "            if 'wardrobe' in user_data:\n",
        "                self.user_wardrobe = user_data['wardrobe'] if isinstance(user_data['wardrobe'], list) else []\n",
        "\n",
        "            # Format profile data for saving\n",
        "            lines = []\n",
        "\n",
        "            # Add basic profile fields first\n",
        "            for key, value in self.user_data.items():\n",
        "                if key != 'wardrobe':  # Handle wardrobe separately\n",
        "                    lines.append(f\"{key}: {value}\")\n",
        "\n",
        "            # Add wardrobe at the end\n",
        "            if self.user_wardrobe:\n",
        "                lines.append(f\"wardrobe: {', '.join(self.user_wardrobe)}\")\n",
        "\n",
        "            # Write to file\n",
        "            with open(self.user_text_path, 'w') as f:\n",
        "                f.write('\\n'.join(lines))\n",
        "\n",
        "            print(f\"Profile saved to {self.user_text_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving profile: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "# ---------- CONFIG MANAGER ----------\n",
        "class ConfigManager:\n",
        "    \"\"\"Manager for generation configuration\"\"\"\n",
        "\n",
        "    def __init__(self, config_path):\n",
        "        self.config_path = config_path\n",
        "        self.config = DEFAULT_CONFIG.copy()\n",
        "        self.load_config()\n",
        "\n",
        "    def load_config(self):\n",
        "        \"\"\"Load configuration from file or create default\"\"\"\n",
        "        # Check if config file exists\n",
        "        if os.path.exists(self.config_path):\n",
        "            try:\n",
        "                with open(self.config_path, 'r') as f:\n",
        "                    user_config = json.load(f)\n",
        "                    self.config.update(user_config)\n",
        "                    print(f\"Loaded configuration from {self.config_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading config file: {e}\")\n",
        "                print(\"Using default configuration\")\n",
        "        else:\n",
        "            print(f\"Config file not found at {self.config_path}. Creating default.\")\n",
        "            self.save_config()\n",
        "\n",
        "    def save_config(self):\n",
        "        \"\"\"Save current configuration to file\"\"\"\n",
        "        try:\n",
        "            with open(self.config_path, 'w') as f:\n",
        "                json.dump(self.config, f, indent=4)\n",
        "                print(f\"Configuration saved to {self.config_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save configuration: {e}\")\n",
        "            return False\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Get current configuration\"\"\"\n",
        "        return self.config\n",
        "\n",
        "    def update_config(self, new_config):\n",
        "        \"\"\"Update configuration with new values\"\"\"\n",
        "        self.config.update(new_config)\n",
        "        return self.save_config()\n",
        "\n",
        "\n",
        "# ---------- ML FUNCTIONS ----------\n",
        "def process_image_with_vlm(vlm_model, processor, image, device):\n",
        "    \"\"\"Process an image with the VLM and return formatted results\"\"\"\n",
        "    # Define the prompts for the VLM\n",
        "    prompts = [\n",
        "        \"What is the subcategory of this fashion item?\",\n",
        "        \"What type of article is this fashion item?\",\n",
        "        \"What is the base color of this fashion item?\",\n",
        "        \"What season is this fashion item designed for?\",\n",
        "        \"What is the intended usage of this fashion item?\"\n",
        "    ]\n",
        "\n",
        "    # Process the image with VLM\n",
        "    results = {}\n",
        "    for prompt in prompts:\n",
        "        prediction = generate_vlm_prediction(vlm_model, processor, image, prompt, device)\n",
        "        # Store the result\n",
        "        key = prompt.replace(\"What is the \", \"\").replace(\"?\", \"\").strip()\n",
        "        results[key] = prediction\n",
        "\n",
        "    # Create a concise description\n",
        "    concise_desc = f\"given image: {results.get('intended usage', '').lower()} \" \\\n",
        "                  f\"{results.get('base color', '').lower()} \" \\\n",
        "                  f\"{results.get('type of article', '').lower()} \" \\\n",
        "                  f\"{results.get('subcategory', '').lower()} \" \\\n",
        "                  f\"for {results.get('season', '').lower()}\"\n",
        "\n",
        "    return concise_desc, results\n",
        "\n",
        "def generate_vlm_prediction(model, processor, image, prompt, device):\n",
        "    \"\"\"Generate prediction for an image using VLM\"\"\"\n",
        "    # Process image and text with the processor\n",
        "    if isinstance(image, np.ndarray):\n",
        "        # Convert OpenCV BGR to RGB\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        pil_image = Image.fromarray(image_rgb)\n",
        "    else:\n",
        "        pil_image = image\n",
        "\n",
        "    # Process inputs\n",
        "    model_inputs = processor(text=[prompt], images=[pil_image])\n",
        "    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
        "\n",
        "    # Get model inputs\n",
        "    input_ids = model_inputs[\"input_ids\"]\n",
        "    attention_mask = model_inputs[\"attention_mask\"]\n",
        "    pixel_values = model_inputs[\"pixel_values\"]\n",
        "\n",
        "    # Import KVCache here to avoid circular imports\n",
        "    from paligemma import KVCache\n",
        "    kv_cache = KVCache()\n",
        "    stop_token = processor.tokenizer.eos_token_id\n",
        "    generated_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(50):  # Max 50 tokens for VLM output\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                pixel_values=pixel_values,\n",
        "                attention_mask=attention_mask,\n",
        "                kv_cache=kv_cache,\n",
        "            )\n",
        "            kv_cache = outputs[\"kv_cache\"]\n",
        "            next_token_logits = outputs[\"logits\"][:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "            token_id = next_token.item()\n",
        "            generated_tokens.append(token_id)\n",
        "            if token_id == stop_token:\n",
        "                break\n",
        "            input_ids = next_token\n",
        "            attention_mask = torch.cat([attention_mask, torch.ones((1, 1), device=device)], dim=-1)\n",
        "\n",
        "    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "def format_input(instruction, input_text=\"\", consider_wardrobe=\"no\", consider_user_details=\"no\", user_profile=None):\n",
        "    \"\"\"Format the instruction and input according to the training format, including personalization options.\"\"\"\n",
        "    # Base instruction\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request. \"\n",
        "        f\"Do not generate additional inputs or profile information.\"\n",
        "        f\"\\n\\n### Instruction:\\n{instruction}\"\n",
        "    )\n",
        "\n",
        "    # Add personalization details if requested\n",
        "    if user_profile and consider_user_details.lower() == \"yes\":\n",
        "        instruction_text += f\"\\n\\nConsider the following user details for personalization:\\n{user_profile.get_user_profile_summary()}\"\n",
        "\n",
        "    # Add wardrobe information if requested\n",
        "    if user_profile and consider_wardrobe.lower() == \"yes\":\n",
        "        instruction_text += f\"\\n\\nConsider these items from the user's wardrobe for your suggestions:\\n{user_profile.get_wardrobe_summary()}\"\n",
        "\n",
        "    # Add input text if provided\n",
        "    input_part = f\"\\n\\n### Input:\\n{input_text}\" if input_text else \"\"\n",
        "\n",
        "    return instruction_text + input_part + \"\\n\\n### Response:\"\n",
        "\n",
        "def generate_llm_text(model, tokenizer, prompt, device, max_new_tokens=150, temperature=0.7, top_k=40):\n",
        "    \"\"\"Generate text based on a prompt using the fine-tuned LLM.\"\"\"\n",
        "    # Tokenize the prompt\n",
        "    encoded = torch.tensor(tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})).unsqueeze(0).to(device)\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "\n",
        "    # Generate response\n",
        "    model.eval()\n",
        "    generated_tokens = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Get context for the current step\n",
        "            idx_cond = encoded[:, -context_size:]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "            # Get logits for the last token\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Apply temperature and top-k filtering\n",
        "            if temperature > 0:\n",
        "                if top_k is not None:\n",
        "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                    logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "                probs = torch.softmax(logits / temperature, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                # Greedy sampling\n",
        "                idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "            # Append new token\n",
        "            encoded = torch.cat((encoded, idx_next), dim=1)\n",
        "            generated_tokens.append(idx_next.item())\n",
        "\n",
        "            # Stop if EOS token is generated (50256 is <|endoftext|>)\n",
        "            if idx_next.item() == 50256:\n",
        "                break\n",
        "\n",
        "            # Also stop if we see \"### Input:\" - this is a sign the model is hallucinating additional input\n",
        "            current_text = tokenizer.decode(encoded.squeeze().tolist()[len(tokenizer.encode(prompt)):])\n",
        "            if \"### Input:\" in current_text:\n",
        "                break\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(encoded.squeeze().tolist())\n",
        "\n",
        "    # Return only the response part (after the prompt)\n",
        "    response = generated_text[len(prompt):]\n",
        "\n",
        "    # Remove any hallucinated input sections\n",
        "    if \"### Input:\" in response:\n",
        "        response = response.split(\"### Input:\")[0].strip()\n",
        "\n",
        "    # Remove any hallucinated response markers\n",
        "    if \"### Response:\" in response:\n",
        "        response = response.replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    return response, generated_tokens\n",
        "\n",
        "\n",
        "# ---------- WEBCAM CLASS ----------\n",
        "class WebcamCapture:\n",
        "    \"\"\"Class to handle webcam capture functionality\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.cap = None\n",
        "        self.is_running = False\n",
        "        self.frame = None\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start webcam capture\"\"\"\n",
        "        if self.cap is None:\n",
        "            try:\n",
        "                self.cap = cv2.VideoCapture(0)\n",
        "                if not self.cap.isOpened():\n",
        "                    print(\"Failed to open webcam\")\n",
        "                    return False\n",
        "\n",
        "                self.is_running = True\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"Error starting webcam: {e}\")\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop webcam capture\"\"\"\n",
        "        if self.cap is not None:\n",
        "            self.is_running = False\n",
        "            self.cap.release()\n",
        "            self.cap = None\n",
        "\n",
        "    def get_frame(self):\n",
        "        \"\"\"Get current frame from webcam\"\"\"\n",
        "        if self.cap is not None and self.cap.isOpened():\n",
        "            ret, frame = self.cap.read()\n",
        "            if ret:\n",
        "                self.frame = frame\n",
        "                return frame\n",
        "        return None\n",
        "\n",
        "    def capture_image(self):\n",
        "        \"\"\"Capture single image from webcam\"\"\"\n",
        "        if self.frame is not None:\n",
        "            return self.frame.copy()\n",
        "        return self.get_frame()\n",
        "\n",
        "\n",
        "# ---------- UI CLASSES ----------\n",
        "class FashionSystemApp:\n",
        "    \"\"\"Main application class\"\"\"\n",
        "\n",
        "    def __init__(self, root):\n",
        "        self.root = root\n",
        "        self.root.title(\"Fashion Recommendation System\")\n",
        "        self.root.geometry(\"1024x768\")\n",
        "\n",
        "        # Set theme\n",
        "        style = ttk.Style()\n",
        "        style.theme_use('clam')  # Use a modern-looking theme\n",
        "\n",
        "        # Define colors\n",
        "        self.bg_color = \"#f0f0f0\"\n",
        "        self.accent_color = \"#6c5ce7\"\n",
        "        self.text_color = \"#2d3436\"\n",
        "\n",
        "        # Configure root window\n",
        "        self.root.configure(bg=self.bg_color)\n",
        "\n",
        "        # Initialize managers\n",
        "        self.config_manager = ConfigManager(CONFIG_PATH)\n",
        "        self.user_profile = UserProfileManager(USER_PROFILE_PATH)\n",
        "\n",
        "        # Initialize webcam\n",
        "        self.webcam = WebcamCapture()\n",
        "\n",
        "        # Initialize model loading status\n",
        "        self.models_loaded = False\n",
        "        self.model_manager = None\n",
        "        self.models = {}\n",
        "\n",
        "        # Create UI elements\n",
        "        self.create_widgets()\n",
        "\n",
        "        # Start model loading in background thread\n",
        "        self.start_model_loading()\n",
        "\n",
        "    def create_widgets(self):\n",
        "        \"\"\"Create and configure all UI widgets\"\"\"\n",
        "        # Create main container\n",
        "        self.main_container = ttk.PanedWindow(self.root, orient=tk.HORIZONTAL)\n",
        "        self.main_container.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
        "\n",
        "        # Create sidebar frame\n",
        "        self.sidebar = ttk.Frame(self.main_container, width=200)\n",
        "        self.main_container.add(self.sidebar, weight=1)\n",
        "\n",
        "        # Create content frame\n",
        "        self.content = ttk.Frame(self.main_container)\n",
        "        self.main_container.add(self.content, weight=3)\n",
        "\n",
        "        # Create status bar\n",
        "        self.status_bar = ttk.Label(self.root, text=\"Ready\", anchor=tk.W, relief=tk.SUNKEN)\n",
        "        self.status_bar.pack(side=tk.BOTTOM, fill=tk.X)\n",
        "\n",
        "        # Setup sidebar buttons\n",
        "        self.create_sidebar()\n",
        "\n",
        "        # Setup content area\n",
        "        self.show_home_screen()\n",
        "\n",
        "    def create_sidebar(self):\n",
        "        \"\"\"Create sidebar with menu buttons\"\"\"\n",
        "        # App title\n",
        "        ttk.Label(self.sidebar, text=\"Fashion AI\", font=(\"Arial\", 16, \"bold\")).pack(pady=10)\n",
        "\n",
        "        # Menu buttons\n",
        "        btn_style = {'width': 20, 'padding': 5}\n",
        "\n",
        "        ttk.Button(self.sidebar, text=\"Home\", command=self.show_home_screen, **btn_style).pack(pady=5)\n",
        "        ttk.Button(self.sidebar, text=\"Image Analysis\", command=self.show_image_analysis, **btn_style).pack(pady=5)\n",
        "        ttk.Button(self.sidebar, text=\"Text Advice\", command=self.show_text_advice, **btn_style).pack(pady=5)\n",
        "        ttk.Button(self.sidebar, text=\"User Profile\", command=self.show_profile_editor, **btn_style).pack(pady=5)\n",
        "        ttk.Button(self.sidebar, text=\"Settings\", command=self.show_settings, **btn_style).pack(pady=5)\n",
        "        ttk.Button(self.sidebar, text=\"Exit\", command=self.on_exit, **btn_style).pack(pady=5)\n",
        "\n",
        "        # Models status\n",
        "        ttk.Separator(self.sidebar, orient=tk.HORIZONTAL).pack(fill=tk.X, pady=10)\n",
        "        self.model_status_label = ttk.Label(self.sidebar, text=\"Models: Loading...\", wraplength=180)\n",
        "        self.model_status_label.pack(pady=5)\n",
        "\n",
        "    def clear_content(self):\n",
        "        \"\"\"Clear all widgets from content frame\"\"\"\n",
        "        for widget in self.content.winfo_children():\n",
        "            widget.destroy()\n",
        "\n",
        "    def show_home_screen(self):\n",
        "        \"\"\"Show home screen content\"\"\"\n",
        "        self.clear_content()\n",
        "\n",
        "        # Welcome message\n",
        "        ttk.Label(self.content, text=\"Welcome to Fashion AI\", font=(\"Arial\", 24, \"bold\")).pack(pady=20)\n",
        "        ttk.Label(self.content, text=\"Personalized fashion recommendations using AI\", font=(\"Arial\", 12)).pack()\n",
        "\n",
        "        # Options frame\n",
        "        options_frame = ttk.Frame(self.content)\n",
        "        options_frame.pack(pady=40, expand=True)\n",
        "\n",
        "        # Option buttons\n",
        "        ttk.Button(options_frame, text=\"Analyze Fashion Image\",\n",
        "                  command=self.show_image_analysis, width=25, padding=10).pack(pady=10)\n",
        "        ttk.Button(options_frame, text=\"Get Fashion Advice\",\n",
        "                  command=self.show_text_advice, width=25, padding=10).pack(pady=10)\n",
        "        ttk.Button(options_frame, text=\"Edit User Profile\",\n",
        "                  command=self.show_profile_editor, width=25, padding=10).pack(pady=10)\n",
        "\n",
        "    def show_image_analysis(self):\n",
        "        \"\"\"Show image analysis screen\"\"\"\n",
        "        self.clear_content()\n",
        "\n",
        "        if not self.models_loaded:\n",
        "            ttk.Label(self.content, text=\"Models are still loading. Please wait...\",\n",
        "                    font=(\"Arial\", 14)).pack(pady=20)\n",
        "            return\n",
        "\n",
        "        # Title\n",
        "        ttk.Label(self.content, text=\"Fashion Image Analysis\", font=(\"Arial\", 18, \"bold\")).pack(pady=10)\n",
        "\n",
        "        # Image selection frame\n",
        "        img_frame = ttk.Frame(self.content)\n",
        "        img_frame.pack(pady=10, fill=tk.X)\n",
        "\n",
        "        # Image source buttons\n",
        "        ttk.Button(img_frame, text=\"Use Webcam\", command=self.use_webcam).pack(side=tk.LEFT, padx=5)\n",
        "        ttk.Button(img_frame, text=\"Upload Image\", command=self.upload_image).pack(side=tk.LEFT, padx=5)\n",
        "\n",
        "        # Image display area\n",
        "        self.image_display = ttk.Label(self.content)\n",
        "        self.image_display.pack(pady=10)\n",
        "        ttk.Label(self.image_display, text=\"No image selected\").pack()\n",
        "\n",
        "        # Image analysis results\n",
        "        results_frame = ttk.LabelFrame(self.content, text=\"Analysis Results\")\n",
        "        results_frame.pack(pady=10, fill=tk.BOTH, expand=True)\n",
        "\n",
        "        # Results text area\n",
        "        self.results_text = scrolledtext.ScrolledText(results_frame, height=10)\n",
        "        self.results_text.pack(pady=5, fill=tk.BOTH, expand=True)\n",
        "\n",
        "        # Query frame\n",
        "        query_frame = ttk.Frame(self.content)\n",
        "        query_frame.pack(pady=10, fill=tk.X)\n",
        "\n",
        "        ttk.Label(query_frame, text=\"Ask about this fashion item:\").pack(side=tk.LEFT, padx=5)\n",
        "        self.query_entry = ttk.Entry(query_frame, width=50)\n",
        "        self.query_entry.pack(side=tk.LEFT, padx=5, fill=tk.X, expand=True)\n",
        "        self.query_entry.insert(0, \"What outfits can I create with this?\")\n",
        "\n",
        "        # Personalization options\n",
        "        options_frame = ttk.Frame(self.content)\n",
        "        options_frame.pack(pady=5, fill=tk.X)\n",
        "\n",
        "        # Checkboxes for personalization\n",
        "        self.use_wardrobe_var = tk.StringVar(value=\"no\")\n",
        "        ttk.Checkbutton(options_frame, text=\"Consider my wardrobe\",\n",
        "                       variable=self.use_wardrobe_var,\n",
        "                       onvalue=\"yes\", offvalue=\"no\").pack(side=tk.LEFT, padx=5)\n",
        "\n",
        "        self.use_profile_var = tk.StringVar(value=\"no\")\n",
        "        ttk.Checkbutton(options_frame, text=\"Consider my profile details\",\n",
        "                       variable=self.use_profile_var,\n",
        "                       onvalue=\"yes\", offvalue=\"no\").pack(side=tk.LEFT, padx=5)\n",
        "\n",
        "        # Generate button\n",
        "        ttk.Button(self.content, text=\"Generate Recommendation\",\n",
        "                  command=self.generate_image_recommendation).pack(pady=10)\n",
        "\n",
        "        # Response area\n",
        "        response_frame = ttk.LabelFrame(self.content, text=\"Fashion Recommendation\")\n",
        "        response_frame.pack(pady=10, fill=tk.BOTH, expand=True)\n",
        "\n",
        "        self.response_text = scrolledtext.ScrolledText(response_frame, height=10)\n",
        "        self.response_text.pack(pady=5, fill=tk.BOTH, expand=True)\n",
        "\n",
        "    def show_text_advice(self):\n",
        "        \"\"\"Show text advice screen\"\"\"\n",
        "        self.clear_content()\n",
        "\n",
        "        if not self.models_loaded:\n",
        "            ttk.Label(self.content, text=\"Models are still loading. Please wait...\",\n",
        "                    font=(\"Arial\", 14)).pack(pady=20)\n",
        "            return\n",
        "\n",
        "        # Title\n",
        "        ttk.Label(self.content, text=\"Fashion Advice\", font=(\"Arial\", 18, \"bold\")).pack(pady=10)\n",
        "\n",
        "        # Instruction frame\n",
        "        instruction_frame = ttk.Frame(self.content)\n",
        "        instruction_frame.pack(pady=10, fill=tk.X)\n",
        "\n",
        "        ttk.Label(instruction_frame, text=\"What fashion advice do you need?\").pack(anchor=tk.W)\n",
        "\n",
        "        # Query text area\n",
        "        self.advice_query = scrolledtext.ScrolledText(instruction_frame, height=5)\n",
        "        self.advice_query.pack(pady=5, fill=tk.BOTH)\n",
        "        self.advice_query.insert(tk.END, \"Suggest a professional outfit for a job interview\")\n",
        "\n",
        "        # Personalization options\n",
        "        options_frame = ttk.Frame(self.content)\n",
        "        options_frame.pack(pady=5, fill=tk.X)\n",
        "\n",
        "        # Checkboxes for personalization\n",
        "        self.text_use_wardrobe_var = tk.StringVar(value=\"no\")\n",
        "        ttk.Checkbutton(options_frame, text=\"Consider my wardrobe\",\n",
        "                       variable=self.text_use_wardrobe_var,\n",
        "                       onvalue=\"yes\", offvalue=\"no\").pack(side=tk.LEFT, padx=5)\n",
        "\n",
        "        self.text_use_profile_var = tk.StringVar(value=\"no\")\n",
        "        ttk.Checkbutton(options_frame, text=\"Consider my profile details\",\n",
        "                       variable=self.text_use_profile_var,\n",
        "                       onvalue=\"yes\", offvalue=\"no\").pack(side=tk.LEFT, padx=5)\n",
        "\n",
        "        # Generate button\n",
        "        ttk.Button(self.content, text=\"Generate Fashion Advice\",\n",
        "                  command=self.generate_text_advice).pack(pady=10)\n",
        "\n",
        "        # Response area\n",
        "        response_frame = ttk.LabelFrame(self.content, text=\"Fashion Recommendation\")\n",
        "        response_frame.pack(pady=10, fill=tk.BOTH, expand=True)\n",
        "\n",
        "        self.text_response = scrolledtext.ScrolledText(response_frame, height=15)\n",
        "        self.text_response.pack(pady=5, fill=tk.BOTH, expand=True)\n",
        "\n",
        "    def show_profile_editor(self):\n",
        "        \"\"\"Show user profile editor screen\"\"\"\n",
        "        self.clear_content()\n",
        "\n",
        "        # Title\n",
        "        ttk.Label(self.content, text=\"User Profile Editor\", font=(\"Arial\", 18, \"bold\")).pack(pady=10)\n",
        "\n",
        "        # Profile data frame\n",
        "        profile_frame = ttk.Frame(self.content)\n",
        "        profile_frame.pack(pady=10, fill=tk.BOTH, expand=True)\n",
        "\n",
        "        # Create entry fields for user profile data\n",
        "        row = 0\n",
        "        self.profile_entries = {}\n",
        "\n",
        "        # Basic profile fields\n",
        "        basic_fields = [\"name\", \"age\", \"gender\", \"height\", \"weight\", \"style_preference\", \"favorite_color\"]\n",
        "\n",
        "        for field in basic_fields:\n",
        "            ttk.Label(profile_frame, text=field.replace('_', ' ').title() + \":\").grid(row=row, column=0, sticky=tk.W, padx=5, pady=5)\n",
        "\n",
        "            # Get value from user profile if available\n",
        "            value = self.user_profile.user_data.get(field, \"\")\n",
        "\n",
        "            entry = ttk.Entry(profile_frame, width=40)\n",
        "            entry.insert(0, value)\n",
        "            entry.grid(row=row, column=1, sticky=tk.W, padx=5, pady=5)\n",
        "\n",
        "            self.profile_entries[field] = entry\n",
        "            row += 1\n",
        "\n",
        "        # Wardrobe section\n",
        "        ttk.Label(profile_frame, text=\"Wardrobe Items:\").grid(row=row, column=0, sticky=tk.W, padx=5, pady=5)\n",
        "\n",
        "        # Join wardrobe items with commas\n",
        "        wardrobe_text = \", \".join(self.user_profile.user_wardrobe) if self.user_profile.user_wardrobe else \"\"\n",
        "\n",
        "        self.wardrobe_entry = scrolledtext.ScrolledText(profile_frame, width=40, height=5)\n",
        "        self.wardrobe_entry.insert(tk.END, wardrobe_text)\n",
        "        self.wardrobe_entry.grid(row=row, column=1, sticky=tk.W, padx=5, pady=5)\n",
        "\n",
        "        # Help text\n",
        "        ttk.Label(profile_frame, text=\"Enter wardrobe items separated by commas\").grid(row=row+1, column=1, sticky=tk.W, padx=5)\n",
        "\n",
        "        # Save button\n",
        "        ttk.Button(self.content, text=\"Save Profile\", command=self.save_user_profile).pack(pady=20)\n",
        "\n",
        "    def show_settings(self):\n",
        "        \"\"\"Show settings screen\"\"\"\n",
        "        self.clear_content()\n",
        "\n",
        "        # Title\n",
        "        ttk.Label(self.content, text=\"Settings\", font=(\"Arial\", 18, \"bold\")).pack(pady=10)\n",
        "\n",
        "        # Settings frame\n",
        "        settings_frame = ttk.Frame(self.content)\n",
        "        settings_frame.pack(pady=10, fill=tk.BOTH)\n",
        "\n",
        "        # Get current config\n",
        "        config = self.config_manager.get_config()\n",
        "\n",
        "        # Create entries for numeric settings\n",
        "        row = 0\n",
        "        self.setting_entries = {}\n",
        "\n",
        "        # Numeric settings\n",
        "        ttk.Label(settings_frame, text=\"Max Tokens:\").grid(row=row, column=0, sticky=tk.W, padx=5, pady=5)\n",
        "        max_tokens_entry = ttk.Entry(settings_frame, width=10)\n",
        "        max_tokens_entry.insert(0, str(config.get(\"max_new_tokens\", 150)))\n",
        "        max_tokens_entry.grid(row=row, column=1, sticky=tk.W, padx=5, pady=5)\n",
        "        self.setting_entries[\"max_new_tokens\"] = max_tokens_entry\n",
        "        ttk.Label(settings_frame, text=\"Max number of tokens to generate\").grid(row=row, column=2, sticky=tk.W, padx=5)\n",
        "        row += 1\n",
        "\n",
        "        ttk.Label(settings_frame, text=\"Temperature:\").grid(row=row, column=0, sticky=tk.W, padx=5, pady=5)\n",
        "        temp_entry = ttk.Entry(settings_frame, width=10)\n",
        "        temp_entry.insert(0, str(config.get(\"temperature\", 0.7)))\n",
        "        temp_entry.grid(row=row, column=1, sticky=tk.W, padx=5, pady=5)\n",
        "        self.setting_entries[\"temperature\"] = temp_entry\n",
        "        ttk.Label(settings_frame, text=\"Controls randomness (0.0-1.0)\").grid(row=row, column=2, sticky=tk.W, padx=5)\n",
        "        row += 1\n",
        "\n",
        "        ttk.Label(settings_frame, text=\"Top K:\").grid(row=row, column=0, sticky=tk.W, padx=5, pady=5)\n",
        "        top_k_entry = ttk.Entry(settings_frame, width=10)\n",
        "        top_k_entry.insert(0, str(config.get(\"top_k\", 40)))\n",
        "        top_k_entry.grid(row=row, column=1, sticky=tk.W, padx=5, pady=5)\n",
        "        self.setting_entries[\"top_k\"] = top_k_entry\n",
        "        ttk.Label(settings_frame, text=\"Number of highest probability tokens to consider\").grid(row=row, column=2, sticky=tk.W, padx=5)\n",
        "        row += 1\n",
        "\n",
        "        # Save button\n",
        "        ttk.Button(self.content, text=\"Save Settings\", command=self.save_settings).pack(pady=20)\n",
        "\n",
        "        # Model information\n",
        "        info_frame = ttk.LabelFrame(self.content, text=\"System Information\")\n",
        "        info_frame.pack(pady=10, fill=tk.BOTH, expand=True)\n",
        "\n",
        "        # System info text\n",
        "        info_text = f\"\"\"\n",
        "        System Information:\n",
        "        - Device: {torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")}\n",
        "        - CUDA Available: {\"Yes\" if torch.cuda.is_available() else \"No\"}\n",
        "        - Models Loaded: {\"Yes\" if self.models_loaded else \"No\"}\n",
        "        \"\"\"\n",
        "\n",
        "        info_label = ttk.Label(info_frame, text=info_text, justify=tk.LEFT)\n",
        "        info_label.pack(padx=10, pady=10, anchor=tk.W)\n",
        "\n",
        "    def start_model_loading(self):\n",
        "        \"\"\"Start loading models in background thread\"\"\"\n",
        "        self.update_status(\"Initializing model manager...\")\n",
        "\n",
        "        # Create model manager with status callback\n",
        "        self.model_manager = ModelManager(status_callback=self.update_status)\n",
        "\n",
        "        # Start loading in background thread\n",
        "        loading_thread = threading.Thread(target=self.load_models_thread)\n",
        "        loading_thread.daemon = True\n",
        "        loading_thread.start()\n",
        "\n",
        "    def load_models_thread(self):\n",
        "        \"\"\"Thread function to load models\"\"\"\n",
        "        try:\n",
        "            success = self.model_manager.initialize_models()\n",
        "            if success:\n",
        "                self.models = self.model_manager.get_models()\n",
        "                self.models_loaded = True\n",
        "                self.update_status(\"All models loaded successfully!\")\n",
        "                self.update_model_status(\"Models: Ready\")\n",
        "            else:\n",
        "                self.update_status(\"Failed to load models.\")\n",
        "                self.update_model_status(\"Models: Error loading\")\n",
        "        except Exception as e:\n",
        "            self.update_status(f\"Error loading models: {str(e)}\")\n",
        "            self.update_model_status(\"Models: Error loading\")\n",
        "\n",
        "    def update_status(self, message):\n",
        "        \"\"\"Update status bar with message\"\"\"\n",
        "        self.root.after(0, lambda: self.status_bar.config(text=message))\n",
        "\n",
        "    def update_model_status(self, message):\n",
        "        \"\"\"Update model status label\"\"\"\n",
        "        self.root.after(0, lambda: self.model_status_label.config(text=message))\n",
        "\n",
        "    def upload_image(self):\n",
        "        \"\"\"Open file dialog to select an image\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            messagebox.showinfo(\"Models Loading\", \"Please wait for models to finish loading\")\n",
        "            return\n",
        "\n",
        "        file_path = filedialog.askopenfilename(\n",
        "            title=\"Select Image\",\n",
        "            filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png\")]\n",
        "        )\n",
        "\n",
        "        if file_path:\n",
        "            try:\n",
        "                # Load image\n",
        "                img = cv2.imread(file_path)\n",
        "                if img is None:\n",
        "                    messagebox.showerror(\"Error\", \"Could not read image file\")\n",
        "                    return\n",
        "\n",
        "                # Process and display image\n",
        "                self.display_and_analyze_image(img)\n",
        "            except Exception as e:\n",
        "                messagebox.showerror(\"Error\", f\"Error loading image: {str(e)}\")\n",
        "\n",
        "    def use_webcam(self):\n",
        "        \"\"\"Use webcam to capture image\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            messagebox.showinfo(\"Models Loading\", \"Please wait for models to finish loading\")\n",
        "            return\n",
        "\n",
        "        # Initialize webcam if needed\n",
        "        if not self.webcam.start():\n",
        "            messagebox.showerror(\"Error\", \"Could not initialize webcam\")\n",
        "            return\n",
        "\n",
        "        # Create webcam capture window\n",
        "        webcam_window = tk.Toplevel(self.root)\n",
        "        webcam_window.title(\"Webcam Capture\")\n",
        "        webcam_window.geometry(\"640x520\")\n",
        "\n",
        "        # Create frame for video\n",
        "        video_frame = ttk.Frame(webcam_window)\n",
        "        video_frame.pack(pady=10)\n",
        "\n",
        "        # Create label for video\n",
        "        video_label = ttk.Label(video_frame)\n",
        "        video_label.pack()\n",
        "\n",
        "        # Create capture button\n",
        "        ttk.Button(webcam_window, text=\"Capture\", command=lambda: self.capture_webcam_image(webcam_window)).pack(pady=10)\n",
        "\n",
        "        # Update webcam frame\n",
        "        self.update_webcam_frame(video_label, webcam_window)\n",
        "\n",
        "    def update_webcam_frame(self, label, window):\n",
        "        \"\"\"Update webcam frame in UI\"\"\"\n",
        "        if not hasattr(window, 'closed') or not window.closed:\n",
        "            frame = self.webcam.get_frame()\n",
        "            if frame is not None:\n",
        "                # Convert to RGB for PIL\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                img = Image.fromarray(frame_rgb)\n",
        "                img = ImageTk.PhotoImage(image=img)\n",
        "\n",
        "                # Update label\n",
        "                label.configure(image=img)\n",
        "                label.image = img\n",
        "\n",
        "            # Schedule next update\n",
        "            window.after(30, lambda: self.update_webcam_frame(label, window))\n",
        "\n",
        "    def capture_webcam_image(self, window):\n",
        "        \"\"\"Capture image from webcam and close window\"\"\"\n",
        "        img = self.webcam.capture_image()\n",
        "        if img is not None:\n",
        "            # Stop webcam\n",
        "            self.webcam.stop()\n",
        "\n",
        "            # Close window\n",
        "            window.closed = True\n",
        "            window.destroy()\n",
        "\n",
        "            # Process and display image\n",
        "            self.display_and_analyze_image(img)\n",
        "        else:\n",
        "            messagebox.showerror(\"Error\", \"Could not capture image from webcam\")\n",
        "\n",
        "    def display_and_analyze_image(self, img):\n",
        "        \"\"\"Display image and analyze with VLM\"\"\"\n",
        "        try:\n",
        "            # Resize for display\n",
        "            display_img = cv2.resize(img, (300, 300))\n",
        "\n",
        "            # Convert to RGB for PIL\n",
        "            display_img_rgb = cv2.cvtColor(display_img, cv2.COLOR_BGR2RGB)\n",
        "            pil_img = Image.fromarray(display_img_rgb)\n",
        "            photo_img = ImageTk.PhotoImage(image=pil_img)\n",
        "\n",
        "            # Update image display\n",
        "            self.clear_widget(self.image_display)\n",
        "            self.image_display.configure(image=photo_img)\n",
        "            self.image_display.image = photo_img\n",
        "\n",
        "            # Process with VLM\n",
        "            self.update_status(\"Analyzing image with VLM...\")\n",
        "\n",
        "            # Get models\n",
        "            vlm_model = self.models.get(\"vlm\")\n",
        "            processor = self.models.get(\"processor\")\n",
        "            device = self.models.get(\"device\")\n",
        "\n",
        "            # Process image\n",
        "            concise_desc, results = process_image_with_vlm(vlm_model, processor, img, device)\n",
        "\n",
        "            # Display results\n",
        "            self.results_text.delete(1.0, tk.END)\n",
        "            self.results_text.insert(tk.END, f\"Image Description: {concise_desc}\\n\\n\")\n",
        "\n",
        "            for key, value in results.items():\n",
        "                self.results_text.insert(tk.END, f\"{key.title()}: {value}\\n\")\n",
        "\n",
        "            self.update_status(\"Image analysis complete!\")\n",
        "        except Exception as e:\n",
        "            self.update_status(f\"Error processing image: {str(e)}\")\n",
        "            messagebox.showerror(\"Error\", f\"Error processing image: {str(e)}\")\n",
        "\n",
        "    def generate_image_recommendation(self):\n",
        "        \"\"\"Generate fashion recommendation based on image and query\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            messagebox.showinfo(\"Models Loading\", \"Please wait for models to finish loading\")\n",
        "            return\n",
        "\n",
        "        # Check if image is loaded\n",
        "        if not hasattr(self.image_display, 'image') or self.image_display.image is None:\n",
        "            messagebox.showinfo(\"No Image\", \"Please upload or capture an image first\")\n",
        "            return\n",
        "\n",
        "        # Get query\n",
        "        query = self.query_entry.get().strip()\n",
        "        if not query:\n",
        "            messagebox.showinfo(\"Empty Query\", \"Please enter a question about the fashion item\")\n",
        "            return\n",
        "\n",
        "        # Get personalization options\n",
        "        consider_wardrobe = self.use_wardrobe_var.get()\n",
        "        consider_user_details = self.use_profile_var.get()\n",
        "\n",
        "        # Get image description from results\n",
        "        image_desc = self.results_text.get(1.0, tk.END).strip()\n",
        "\n",
        "        # Prepare input for LLM\n",
        "        instruction = f\"Provide fashion advice about the following clothing item: {query}\"\n",
        "        input_text = f\"The image shows: {image_desc}\"\n",
        "\n",
        "        # Format input with personalization options\n",
        "        formatted_input = format_input(\n",
        "            instruction,\n",
        "            input_text,\n",
        "            consider_wardrobe,\n",
        "            consider_user_details,\n",
        "            self.user_profile\n",
        "        )\n",
        "\n",
        "        # Generate response\n",
        "        self.update_status(\"Generating recommendation...\")\n",
        "        try:\n",
        "            # Get models\n",
        "            llm_model = self.models.get(\"llm\")\n",
        "            tokenizer = self.models.get(\"tokenizer\")\n",
        "            device = self.models.get(\"device\")\n",
        "\n",
        "            # Get generation parameters\n",
        "            config = self.config_manager.get_config()\n",
        "            max_tokens = int(config.get(\"max_new_tokens\", 150))\n",
        "            temperature = float(config.get(\"temperature\", 0.7))\n",
        "            top_k = int(config.get(\"top_k\", 40))\n",
        "\n",
        "            # Generate text\n",
        "            response, _ = generate_llm_text(\n",
        "                llm_model,\n",
        "                tokenizer,\n",
        "                formatted_input,\n",
        "                device,\n",
        "                max_tokens,\n",
        "                temperature,\n",
        "                top_k\n",
        "            )\n",
        "\n",
        "            # Display response\n",
        "            self.response_text.delete(1.0, tk.END)\n",
        "            self.response_text.insert(tk.END, response)\n",
        "\n",
        "            self.update_status(\"Recommendation generated!\")\n",
        "        except Exception as e:\n",
        "            self.update_status(f\"Error generating recommendation: {str(e)}\")\n",
        "            messagebox.showerror(\"Error\", f\"Error generating recommendation: {str(e)}\")\n",
        "\n",
        "    def generate_text_advice(self):\n",
        "        \"\"\"Generate fashion advice based on text query\"\"\"\n",
        "        if not self.models_loaded:\n",
        "            messagebox.showinfo(\"Models Loading\", \"Please wait for models to finish loading\")\n",
        "            return\n",
        "\n",
        "        # Get query\n",
        "        query = self.advice_query.get(1.0, tk.END).strip()\n",
        "        if not query:\n",
        "            messagebox.showinfo(\"Empty Query\", \"Please enter a fashion advice question\")\n",
        "            return\n",
        "\n",
        "        # Get personalization options\n",
        "        consider_wardrobe = self.text_use_wardrobe_var.get()\n",
        "        consider_user_details = self.text_use_profile_var.get()\n",
        "\n",
        "        # Prepare input for LLM\n",
        "        instruction = f\"Provide fashion advice: {query}\"\n",
        "\n",
        "        # Format input with personalization options\n",
        "        formatted_input = format_input(\n",
        "            instruction,\n",
        "            \"\",\n",
        "            consider_wardrobe,\n",
        "            consider_user_details,\n",
        "            self.user_profile\n",
        "        )\n",
        "\n",
        "        # Generate response\n",
        "        self.update_status(\"Generating fashion advice...\")\n",
        "        try:\n",
        "            # Get models\n",
        "            llm_model = self.models.get(\"llm\")\n",
        "            tokenizer = self.models.get(\"tokenizer\")\n",
        "            device = self.models.get(\"device\")\n",
        "\n",
        "            # Get generation parameters\n",
        "            config = self.config_manager.get_config()\n",
        "            max_tokens = int(config.get(\"max_new_tokens\", 150))\n",
        "            temperature = float(config.get(\"temperature\", 0.7))\n",
        "            top_k = int(config.get(\"top_k\", 40))\n",
        "\n",
        "            # Generate text\n",
        "            response, _ = generate_llm_text(\n",
        "                llm_model,\n",
        "                tokenizer,\n",
        "                formatted_input,\n",
        "                device,\n",
        "                max_tokens,\n",
        "                temperature,\n",
        "                top_k\n",
        "            )\n",
        "\n",
        "            # Display response\n",
        "            self.text_response.delete(1.0, tk.END)\n",
        "            self.text_response.insert(tk.END, response)\n",
        "\n",
        "            self.update_status(\"Fashion advice generated!\")\n",
        "        except Exception as e:\n",
        "            self.update_status(f\"Error generating advice: {str(e)}\")\n",
        "            messagebox.showerror(\"Error\", f\"Error generating advice: {str(e)}\")\n",
        "\n",
        "    def save_user_profile(self):\n",
        "        \"\"\"Save user profile data\"\"\"\n",
        "        try:\n",
        "            # Collect data from entry fields\n",
        "            profile_data = {}\n",
        "\n",
        "            # Get basic profile fields\n",
        "            for field, entry in self.profile_entries.items():\n",
        "                profile_data[field] = entry.get().strip()\n",
        "\n",
        "            # Get wardrobe items\n",
        "            wardrobe_text = self.wardrobe_entry.get(1.0, tk.END).strip()\n",
        "            wardrobe_items = [item.strip() for item in wardrobe_text.split(',') if item.strip()]\n",
        "            profile_data['wardrobe'] = wardrobe_items\n",
        "\n",
        "            # Save profile\n",
        "            if self.user_profile.save_profile(profile_data):\n",
        "                messagebox.showinfo(\"Success\", \"User profile saved successfully\")\n",
        "                self.update_status(\"User profile saved\")\n",
        "            else:\n",
        "                messagebox.showerror(\"Error\", \"Failed to save user profile\")\n",
        "        except Exception as e:\n",
        "            messagebox.showerror(\"Error\", f\"Error saving profile: {str(e)}\")\n",
        "\n",
        "    def save_settings(self):\n",
        "        \"\"\"Save application settings\"\"\"\n",
        "        try:\n",
        "            # Collect settings from entry fields\n",
        "            new_config = {}\n",
        "\n",
        "            # Get numeric settings\n",
        "            for setting, entry in self.setting_entries.items():\n",
        "                value = entry.get().strip()\n",
        "\n",
        "                # Convert to appropriate type\n",
        "                if setting in [\"max_new_tokens\", \"top_k\"]:\n",
        "                    value = int(value)\n",
        "                elif setting == \"temperature\":\n",
        "                    value = float(value)\n",
        "\n",
        "                new_config[setting] = value\n",
        "\n",
        "            # Update config\n",
        "            if self.config_manager.update_config(new_config):\n",
        "                messagebox.showinfo(\"Success\", \"Settings saved successfully\")\n",
        "                self.update_status(\"Settings saved\")\n",
        "            else:\n",
        "                messagebox.showerror(\"Error\", \"Failed to save settings\")\n",
        "        except ValueError:\n",
        "            messagebox.showerror(\"Error\", \"Please enter valid numeric values for settings\")\n",
        "        except Exception as e:\n",
        "            messagebox.showerror(\"Error\", f\"Error saving settings: {str(e)}\")\n",
        "\n",
        "    def clear_widget(self, widget):\n",
        "        \"\"\"Clear a widget's content\"\"\"\n",
        "        if isinstance(widget, ttk.Label):\n",
        "            widget.configure(image='')\n",
        "            if hasattr(widget, 'image'):\n",
        "                delattr(widget, 'image')\n",
        "        elif isinstance(widget, scrolledtext.ScrolledText):\n",
        "            widget.delete(1.0, tk.END)\n",
        "        elif isinstance(widget, ttk.Entry):\n",
        "            widget.delete(0, tk.END)\n",
        "\n",
        "    def on_exit(self):\n",
        "        \"\"\"Handle application exit\"\"\"\n",
        "        if messagebox.askokcancel(\"Exit\", \"Are you sure you want to exit?\"):\n",
        "            # Stop webcam if running\n",
        "            if self.webcam:\n",
        "                self.webcam.stop()\n",
        "\n",
        "            # Destroy root window\n",
        "            self.root.destroy()\n",
        "\n",
        "\n",
        "# ---------- MAIN ENTRY POINT ----------\n",
        "def main():\n",
        "    \"\"\"Main entry point for the application\"\"\"\n",
        "    # Create root window\n",
        "    # Install xvfb if needed (in Colab)\n",
        "    !apt-get install -y xvfb\n",
        "\n",
        "    # Set up virtual display\n",
        "    !Xvfb :1 -screen 0 1024x768x24 &\n",
        "    import os\n",
        "    os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "    root = tk.Tk()\n",
        "\n",
        "    # Create app instance\n",
        "    app = FashionSystemApp(root)\n",
        "\n",
        "    # Start main loop\n",
        "    root.mainloop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "-pnA5f_7PiLw",
        "outputId": "94df9ff0-803f-459a-9701-454b0e1df013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.14).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "couldn't connect to display \":1\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-953ba1340d52>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-953ba1340d52>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DISPLAY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m':1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     \u001b[0;31m# Create app instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: couldn't connect to display \":1\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zJCro0rWRh5J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}